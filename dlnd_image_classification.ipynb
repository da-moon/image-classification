{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 1:\n",
      "Image - Min Value: 15 Max Value: 249\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAGk5JREFUeJzt3cuuJed5HuC/1nGfunc3m2SLEg8WKcoylDiJTCGAfQNG\npoGRYe4mg9xHJp4EyMRTDzIPYMOwE8eiJFMiRbK7ye59WoeqykAGMv5fbJLAh+eZf/hq1eldNXqH\neZ4bAFDT4rs+AADgmyPoAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCF\nCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABS2+q4P4Jvyn//Tn83J3Ha97J7Z73fJqja3qX9oiFa1\n5ZBd6oeP3+qeee+HH0a7vve9/l1XL19Eu+5u7qK5m5vr7pnrq5fRrsO4756Z2zHaNQz9j8t6md2M\ny0X2fbEY+p/NRbhrWPTvauGu9HNrCPYtwhfIYuifS2Zaa22xDM59y671sAhfqMGuz373PFr1X/7r\nfwsP8v/zRQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4A\nChP0AFBY2fa6wzRGc8upvyhos9lEuw7HoJ1sikr52mKZXeoHFw+7Z954+k606+0f/bR7Zt+ypqsx\nvD9ePv+8e+Y3//dvo10vvvi0e2a/u412JaVm4a3Y2pyVcc1D/3dJeohJy1taMTbEk9+m5EyGvyu/\nsbonhrBhr83BrmzTvfBFDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAo\nTNADQGGCHgAKK1tqM4Z1FuM8dc+cbc+iXdPQf4zT2H98rbW2XGflL7vbr7tnPvv8t9Gu49nr3TOr\nN38U7ZovzqO58ell98zFJtv1+uHQPTPdZqU2+5vr7pnheBPtmo530dz+0P/b7nZX0a7drv+3jcdd\ntKvNWcFSCwq4WvDO+ZfBb3FXNheVxoSHmPy0+HTcA1/0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAo\nTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhZVtr5uGrK1tP/VXDO3HY7Rruej/nzVkHU1t\nPGYNWXd3/U1jr148j3Ztzz/tnnl80t8m11prZ9torF08edw98/iDd7Nl+3X3yO1X/W2DrbV29az/\n3K/32a51+HmxWvY/08tl9ry8fPmse+bLzz+Jdv3u04+juXEXtOXN2bsqqV6b4mq47JoNwb5FeozB\n2Dx/d/V1vugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNAD\nQGFlS20WYTHCNPWXv9ztDtGui/Oz7pnTzUm0a7vp39Vaa3/04b/rnvn+Dz6Idl0+frN7Ztj0F7+0\n1trFRdZqc7bpL1bZhn+np5Ope+blebZru+t/FSzGi2jXMGfPZmv9534YspP/9OnD7pmTbXY+2pSd\nj0/++e+7Z46HoAintbZaBYVCwyba1ebwgUlvq0BUUKPUBgD4Jgh6AChM0ANAYYIeAAoT9ABQmKAH\ngMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFBY4fa67D/MYexvGNpNx2jX+Xl/u1PaDPev\nfvpRNPf+H/yke+Z0+yDaNQXtTlPrbxtsrbXVaVbzdnLa/9vOTrLz0ab+VsTdMTsfd8+vumeeffVF\ntGsTNjBePOhvlHsQzLSW3Yun54+iXd9/98No7vb2VffM55/+Mtp1fdW/6/wiewcPQUtha61NQ9Bu\nGDbKDUE76nfXXeeLHgBKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAH\ngMIEPQAUJugBoDDtdfcwN0/9TUattXZ5+Xr3zLvv/GG068Mf/9tobrPtb9g7hA1qV1c33TPPvnwe\n7TrZZg1qr7/+RvfMw7OsKa8N/edxaP2Nd6219vzFs+6Zv/zL/x7t+vyLrPXu7Xfe7p75i7/4j9Gu\nx69dds+cnvQ/K621dvmo/z3QWmvvf/DH3TOL8JX/ya//oXvm6tXX0a7T09Nobhs901lTXpQvYVPe\nffBFDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKK1tq\nMx6O0dxy6D8lDx89iXb95MOfdc988P5Po11nZxfR3GHqL0n5zWefRbv+19/8bffMX//1/4x2ffDe\ne9Hcv/+o/5qdn2aP2fVVfynIy6tX0a7Ncuqeef+9d7Ndm+x8PHjYX3YyTrto13rZX3ayXm2jXeOU\nlUCdPuh/7zx52l8M1Fpr11dfdc/877/7dbTrcHEXzT287C8i2p6eRbvmRX9BzTj1P2P3xRc9ABQm\n6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYWXb666/\n7G9baq211998q3vmoz/5s2jX+z/6o+6ZB5evRbt2YZvfOPc3Lj17/iLa9auPP+6e+edf/iLa9Wi7\njuaeffq97plf/lN2jJ99+bvumVe3N9GucRi6Z/74o38d7fpo8yfR3NnZSffMo8tH0a4HD/vbHpfL\n7HW622cNe9uT/ra8h5ePo12PXnuje2Z78iDadR02MM5Tf6Pc65uscTAxBsd3X3zRA0Bhgh4AChP0\nAFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCypbaHO76y1haa+1k+7B7\n5u13fxjtOn/UX7gxLbP/ZrvDIZrbbPtLH3784R9Gu15/8qR75ucf/Sza9ebj/l2ttbZc9D8yH3/y\nabTrf/zVX3XP/OOvfxXt2p6ddc/8hz//82jXz3/2b6K5t3/QXyg0j1mZ0yJ4zOY52zW07F01z/0l\nKcMyK3M6Oe9/Lz558/vRrk9+8Q/R3Ncv+ovMLi4vo13rk/6CpbkptQEAvgGCHgAKE/QAUJigB4DC\nBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUVra9rq37W9daa+380eP+Vaen\n0a7letk9s9pk7VPr5SaaOz3tbzU7OzuPdl1c9O969Li/Vau11s6DNq7WWnv18qZ75uNPv4h2Pb/a\ndc+8eNk/01prl8uL/qGhv8GrtdaGIbsX16v+Z/rsIrsXb26vu2f2h320K201O479jZTDov+d01pr\n5xf9TZtvff/daNezzz6J5l48u+qeeflVf+Nda609DJo2k7bB++KLHgAKE/QAUJigB4DCBD0AFCbo\nAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoLCy7XXr86wha3HSf0rGsH3qcDj2\n7zqO0a7NaojmWpu6JxaLbNd2299Odjn0t2q11toQ/sU9Oeu/P374/jvRrj/90593zzx6dBnturrq\nb2sbb7Lmr+ef/zqau7zof84evPcH0a7lvv8eHg7Zs3k89LfQtdbamLTXDdm7anPS31T48LU3ol3p\n3NdfPe+e+eK3n0W7tqf952Puf5XeG1/0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAK\nE/QAUJigB4DCBD0AFCboAaCwsqU2i7BB4Ljfdc+Mx6yUYh/s2u/6Z1prbbXMLvVy2f9fcBH+fZzm\noBRkyK7z3e1VNPfq6xfdM9evvox2vfuD/sKeh+c/iXYdg4Klt55m5SOrZfa8fP3VF90zLx89jHYl\nvUzz2H8OW2vt7qa/UKi11sY5eNDCbqvVet09c3r2INr15tMfRHMvX/SX2vziH/8+2vXgVf81O87L\naNd98EUPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT\n9ABQWNn2upNl1hS0DtqdxqCFrrXW9rvb7pnbRXrJ5mhqteo/IUPYkHUc990z8xQ03rXWjvu7aG4+\n9h/jeshazX7w5mX3zPeeXES75qDtcRiye2oYsmu2bP3P2fXX/Y1mrbU2BDfxYd9/b7TW2t1N9v5Y\nbk67Z4awxXIY+ueW622069GTp9Hck9f72w1/8/Gvol2vnr/snjlO3913tS96AChM0ANAYYIeAAoT\n9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFBY2VKb65evorlXX33dPXN3cxPt\n2m7Pu2fmOWuMubm+iuYWQTfQZruJdk1Tf/nL7jY798vWX+LSWmubVf8jc7bNyj32+/7So3mR/Xdf\nBiVQc1hOM89ZGU4b+q/Z7VV/+UhrrQ1D/3nc7w/Rrtu7rGBpG3ynDetoVUsuWVqgsznpfy+21trl\n5ZPumadP34p2/fKf/k/3zIuvsky6D77oAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAU\nJugBoDBBDwCFCXoAKEzQA0Bhgh4ACivbXnd+8SCa2276m8auwqa8s7P+Y0ybvw7HrFnr1dVJ98zF\nkJ37xbK/me9u19/w1lpr62BXa62tgna48wfZ+Vjv+qvG1qugbrC1dnrWf9+Pc3/bYGutHY/Z3OHQ\nP3fcZbuSurZpyp6x9B5ebk+7ZxZhu+GcfBOG76pF0KTYWmsnDx52zzx9591o1+e//U33zLPPn0W7\n7oMvegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQWNlS\nmzfefD2au3zUX4wwDWO06zD3l2AMU1b4ME9TNHfc77tnDsFMa62t1/2349CycppxzAo3hkX/vvW6\nvxjo93Ob7pmTbX85TWutnZ+dd8+sggKo1lrb7XfR3O31VffM9VVWONXm/mf6OGbvgcWQ3cNT8kyH\nxzi3pOQn25V+f25P+kt+Hjx6HO169KQ/Xz7/4nm06z74ogeAwgQ9ABQm6AGgMEEPAIUJegAoTNAD\nQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6ACisbHvdo9feiOa+//Y73TPn5xfRrkXwP+t4\n6G+8a6214z6b293edc9swwa19bK/mW8TNLy11toy2NVaa+v1untmE7TytdbaEDSGrVbZrsWq/zwO\ni+zcn2yzY9yu+++rhw+yZ/Pq1cvumf0ha21cBfdUa1l7Xdpi2YKGvf0ue+eM4SG2Rf95XG3OolUX\nj17rnjm5eBDtug++6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6\nAChM0ANAYYIeAAor2173zg9/HM299qS/9e4wRqvaYXfMBgPjMdt1DObGY3ZCkmat1SK7hU9Pstaq\n9aa/IWueszqu/r6w1paLrJVvGJL//P3teq21Nk7ZvTiP/XPL8P5Yrvqv83KZtdCl12xKTn92ydoc\nDI5T+B4ImvJay67ZapM1bZ4EjaXrk9No133wRQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAU\nJugBoDBBDwCFCXoAKEzQA0Bhgh4ACitbavPk6ZvR3Gq16Z6ZwlabqJQilZadLL/F/4LB+ViEv+t4\nt4vm5n3/3GKVlXQMy+C3zdkjPQcVOuvw9XE4ZDf+MXjOlov0Ieu/7zfbrCDl9CwrWDpM/cc4zdnz\nfAxeVlPYoDMF5VattTaO/XNh704bp/7nJZm5L77oAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAH\ngMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4ACivbXneyzRqhFkFj2Djto13HoG1pDivvlkkTWmia\nw/apoEpqswwb1Pa30dzd2H+tw4K9tt72NykuV1mD2jJoDJuH7DthDO/hpGFvCO+PeQxqzYZ1tCu9\nZsdD/zUbgnP4+7n+8zHM2XUeD8dobn/b3yx5uL7Odl0Fc8ewKu8e+KIHgMIEPQAUJugBoDBBDwCF\nCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIWVLbU5P3sQzc1Df+nDfp+VFewPh/6h\nOSulSEttpqB45xCWUhzX/edxWmYFOnNYuJGUCo0tPMZ9//2xnML7I5lbpK+P7BhbUKIzhecjuYWP\n4a4pfA2PQcHSt3nfz0FJVWutHY5Z4dT++Kp7ZhyzUpvXXnvYPfP+B+9Hu+6DL3oAKEzQA0Bhgh4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DC6rbXnfe3C7XW2jj1\n11bd3GZtS8Purntmbln71GKR/aeL2q7msK1t6p87jllD1jxl52Oa+1sAx6zMr83H/t+2WGbL1pv+\n33UYb6JdQ9AQ+S+D3SPpl8x+398Md7fPzv3hmD0vu6DdcEyfl+CZDgrvWmutnT24jOZOtifdM3eL\nbbRrNfS3o94dzqNd98EXPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nMEEPAIUJegAoTNADQGFl2+sWi6wha279LV5Ja1JrrR2DdrIpbIZbrfp/V2utrZf9/wWXQ/j/MWi7\n2h+yxrC09W4c+8//OGY1XlHT2JDdH8u74DwOYT1ZWF6XNCmmbW3J3PEY3ovh3HLV//pOyihbay05\njednWYPo6elpNDce+xsHvzpmJ+Srl7vumV147u+DL3oAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZUpthmZW4LBdBict6He1aBKUUU1iAMaf/6Yb+Yxyn\nrLVkt+v/bdN8iHYdjtncNPWXxgyL7F4cgoKl1TK7F4ehf9c49Rd7/F7W7rEIns3Tk220K5EWxkxT\nNrgK3h93d3fRrt2u/1pvNpto1zIs4Epuq9XJWbRqWvcX7+wX2fm4D77oAaAwQQ8AhQl6AChM0ANA\nYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4ACivbXrcIG5DmpIJqkbW1DUEb\nV9qENoX/6XaHsXvmbp81ZCXtU4tFdguvN1nL2yqYW4ZNinPQapY0vLWWNYathnDXMp3rP8b0fByD\nlsj9fh/tSueurq66Z45ha2P0XgzNu2zXPmjY2+3DFsvkfAxZTtwHX/QAUJigB4DCBD0AFCboAaAw\nQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoLCypTbjNEVzQ1BQs95sol2bk/7ijMUh\n/F1DdqmH1n8+jsfsGOdg12p9Eu06Ob2I5jbRtc7Ox35/2z0zjllByjEoBFmGBUvrdVYolBQYrVbZ\nrv2hv+xkHPuf59Zam+Zwbuo/xnnuL6lqLStxudv137+ttXbYZ+djd9dfpnV3218M1Fpri7n/OVvM\n/c/YffFFDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAK\nE/QAUFjZ9rpj2F63WfW3k20222jXfh80ZB2zdrJpzFqrlkH71zo8H0PQhjYEjWattbYf+899a62N\n+/77ar3K/k/394VlDW+ttTb0Fwe2oNCstdba/pDdi+PU3/41LLNzn7S1TeH5mLLT0aap/7cdj1kz\n3OGQzSV2YevdLml7DBoAW2ttueq/2KtVlkn3wRc9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QA\nUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYWXb6xZDfxNaa63Nc3+N1/GYtRIdghavtEVqDI9x\nCPZtTs6iXdt1f+vdsMj+q+73WQvgfhdcs1X2mC2D5rXVMrvvF8F5PBzCBsAxu4f3h/57OG1Cm4P2\nujFsiBzDRrlk3/GYHWNyrac5e+fc7e6iuf0heKbDY1ysT7tnlsHMffFFDwCFCXoAKEzQA0Bhgh4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKK1tqs1ptorllUAoyLsNihKB4ZzFk\n/80W66zsZFj0z62W2W019PcJtWnKzn2bsnKPY1CccXt7E+3abPpLfjabb+++P+x30a5jWIYzTf3l\nL9OcXeekHGhIbuDWWgsKdFrL7v1jWKBzd9dfNHNzkxUK3e2y+yp5gZycZUUzq9VJ98z6JPxd98AX\nPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl\n2+uOx6y1qrX+BqRxzBrUkpampEWqtbxZa7Hov0XmOds1BS1e6XUej/0tdL/f19+8lt4fh+CajWN4\n32fVgdGqec7mpqBxMGkbbK2147L/fCzCZ2yesva6m5v+VsSrq6to1/V1fxPdIWzKW62zRrmT7Xn3\nzOnpw2hX0l53epadj/vgix4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QA\nUJigB4DCBD0AFFa21Ob6OitvmIKijlevXka7nj1/1j2TltqkhRtnZ/1FEecXF9Gu1vrLLI5hcUZa\n/jIH98ciO/VtPvaXHu3Ckp+kHGi57i/2aK21sWWlNuPYXyg03fWfw9ZaOwTX+Thm9+Jh3/+7Wmtt\nf0j2Zd92y3X/e2C9zXadnvbvaq21bVBqk5TTtNbasOj/bcP83X1X+6IHgMIEPQAUJugBoDBBDwCF\nCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAobJjn+bs+BgDgG+KLHgAKE/QA\nUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoA\nKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0A\nFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIX9PysMpFD9dQGPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f804466ca58>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 1\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : x: ({Number of images} , {Height} , {Width} , {Layers(colors)})\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \"\"\"\n",
    "    #Method 1 : Using simplest normalization\n",
    "    normalized = x/255\n",
    "   \n",
    "   \"\"\"\n",
    "    #method 2 :\n",
    "    a = 0.0\n",
    "    b = 1.0\n",
    "    normalized = a + ( ( (x - np.min(x))*(b - a) )/( np.max(x) - np.min(x) ) )\n",
    "\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#encoder = None\n",
    "def one_hot_encode(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return np.eye(10)[x]\n",
    "    #create the encoder - hard way\n",
    "    \"\"\"\n",
    "    global encoder\n",
    "    if encoder is not None:\n",
    "        #if the given lable has less than 10 values, we do not initiate encoder creation again, we would transform\n",
    "        #the array to it's one-hot encoded form based on lables\n",
    "        return encoder.transform(x)\n",
    "    else:\n",
    "        encoder = preprocessing.LabelBinarizer()\n",
    "        #Fuind one-hot vactor values\n",
    "        encoder.fit(x)\n",
    "        #transform and return the one-hot encoded lables\n",
    "        return encoder.transform(x)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,[None, image_shape[0], \n",
    "                                      image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Image Properties\n",
    "#     image_width = int(x_tensor.get_shape()[1])\n",
    "#     image_height = int(x_tensor.get_shape()[2])\n",
    "    color_channels =  int(x_tensor.get_shape()[3])\n",
    "    # Convolution filter\n",
    "    filter_size_width = int(conv_ksize[0])\n",
    "    filter_size_height = int(conv_ksize[1])\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=[filter_size_height, filter_size_width, \n",
    "                                              color_channels, conv_num_outputs]\n",
    "                                            ,\n",
    "                                             stddev=np.sqrt(2/x_tensor.shape[-1].value)\n",
    "                                            ))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight\n",
    "                              , strides=[1,int(conv_strides[0])\n",
    "                                         ,int(conv_strides[1]),1], padding='SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,\n",
    "                                ksize=[1,int(pool_ksize[0]),\n",
    "                                       int(pool_ksize[1]),1],\n",
    "                                strides=[1,int(pool_strides[0]),\n",
    "                                         int(pool_strides[1]),1], padding='SAME')\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, \n",
    "                                             num_outputs=num_outputs,\n",
    "                                             activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, \n",
    "                                                     num_outputs=num_outputs, \n",
    "                                                     activation_fn=None)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 32, (3,3), (1,1), (2,2), (2,2))\n",
    "    conv2 = conv2d_maxpool(conv1, 64, (3,3), (1,1), (2,2), (2,2))\n",
    "    conv3 = conv2d_maxpool(conv2,128,(3,3),(1,1),(2,2),(2,2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flat1 = flatten(conv2)\n",
    "    fc1 = fully_conn(flat1, 512)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1, 256)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    fc3 = fully_conn(fc2, 128)\n",
    "    fc3 = tf.nn.dropout(fc3, keep_prob)\n",
    "    out = output(fc3,10)\n",
    "    return out\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict = {x: feature_batch,y: label_batch, keep_prob: 1.0})\n",
    "    acc = session.run(accuracy,feed_dict = {x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print('Loss at {}'.format(loss), 'Validation Accuracy at {}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 256\n",
    "batch_size = 64\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.284106731414795 Validation Accuracy at 0.1629999876022339\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 2.2556684017181396 Validation Accuracy at 0.18639998137950897\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 2.1882479190826416 Validation Accuracy at 0.18679998815059662\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 2.1115329265594482 Validation Accuracy at 0.22579997777938843\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 2.0256733894348145 Validation Accuracy at 0.25620001554489136\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.987401008605957 Validation Accuracy at 0.2637999951839447\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.9805595874786377 Validation Accuracy at 0.284199982881546\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.959415078163147 Validation Accuracy at 0.29679998755455017\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 1.9179978370666504 Validation Accuracy at 0.3046000003814697\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 1.906767725944519 Validation Accuracy at 0.3100000023841858\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 1.9079656600952148 Validation Accuracy at 0.3279999792575836\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 1.794451117515564 Validation Accuracy at 0.3635999858379364\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 1.7804092168807983 Validation Accuracy at 0.3792000114917755\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 1.666040301322937 Validation Accuracy at 0.38019999861717224\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 1.549837589263916 Validation Accuracy at 0.3895999789237976\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 1.4586148262023926 Validation Accuracy at 0.4009999632835388\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 1.3588664531707764 Validation Accuracy at 0.4179999530315399\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 1.3491579294204712 Validation Accuracy at 0.41679996252059937\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 1.3551872968673706 Validation Accuracy at 0.4283999800682068\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 1.2674205303192139 Validation Accuracy at 0.42819997668266296\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 1.3481712341308594 Validation Accuracy at 0.4189999997615814\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 1.2271729707717896 Validation Accuracy at 0.44099995493888855\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 1.0637646913528442 Validation Accuracy at 0.44199997186660767\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 1.0447624921798706 Validation Accuracy at 0.43919995427131653\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 1.00895094871521 Validation Accuracy at 0.45499998331069946\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 0.9819846749305725 Validation Accuracy at 0.4471999406814575\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 0.9452138543128967 Validation Accuracy at 0.4647999703884125\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 0.8915759325027466 Validation Accuracy at 0.4723999500274658\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 0.906086802482605 Validation Accuracy at 0.46439996361732483\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 0.7864002585411072 Validation Accuracy at 0.46939992904663086\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 0.7345616817474365 Validation Accuracy at 0.4827999770641327\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 0.7211105823516846 Validation Accuracy at 0.48259997367858887\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 0.6891972422599792 Validation Accuracy at 0.4723999500274658\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 0.6328619718551636 Validation Accuracy at 0.4819999635219574\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 0.6283162832260132 Validation Accuracy at 0.4763999581336975\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 0.6935181021690369 Validation Accuracy at 0.4721999764442444\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 0.5809109807014465 Validation Accuracy at 0.4827999472618103\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 0.5780816674232483 Validation Accuracy at 0.4811999797821045\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 0.5808576345443726 Validation Accuracy at 0.48079997301101685\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 0.6284761428833008 Validation Accuracy at 0.4777999520301819\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 0.5408967137336731 Validation Accuracy at 0.4785999655723572\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 0.516562819480896 Validation Accuracy at 0.4899999499320984\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 0.500049352645874 Validation Accuracy at 0.4889999330043793\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 0.568316638469696 Validation Accuracy at 0.47939997911453247\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 0.4836703836917877 Validation Accuracy at 0.4843999743461609\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 0.5078376531600952 Validation Accuracy at 0.48739999532699585\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 0.49625203013420105 Validation Accuracy at 0.49039995670318604\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 0.47497645020484924 Validation Accuracy at 0.4927999675273895\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 0.47385284304618835 Validation Accuracy at 0.4941999316215515\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 0.47543203830718994 Validation Accuracy at 0.4843999445438385\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 0.5182753205299377 Validation Accuracy at 0.4907999634742737\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 0.4736618995666504 Validation Accuracy at 0.49539995193481445\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 0.4341576099395752 Validation Accuracy at 0.49299997091293335\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 0.3984805643558502 Validation Accuracy at 0.5025999546051025\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 0.4099825620651245 Validation Accuracy at 0.49219995737075806\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 0.39838719367980957 Validation Accuracy at 0.5029999613761902\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 0.39359456300735474 Validation Accuracy at 0.498399943113327\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 0.43846258521080017 Validation Accuracy at 0.4957999289035797\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 0.4063582420349121 Validation Accuracy at 0.49779993295669556\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 0.44336947798728943 Validation Accuracy at 0.4893999695777893\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 0.377504825592041 Validation Accuracy at 0.5069999694824219\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 0.38711419701576233 Validation Accuracy at 0.5025999546051025\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 0.37520965933799744 Validation Accuracy at 0.4963999390602112\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 0.3888828158378601 Validation Accuracy at 0.501599907875061\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 0.3944121301174164 Validation Accuracy at 0.5039999485015869\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 0.3870108127593994 Validation Accuracy at 0.4987999498844147\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 0.36970511078834534 Validation Accuracy at 0.5023999810218811\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 0.34163373708724976 Validation Accuracy at 0.49459996819496155\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 0.3315023183822632 Validation Accuracy at 0.5079999566078186\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 0.3312760591506958 Validation Accuracy at 0.5005999803543091\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 0.42277729511260986 Validation Accuracy at 0.5095999836921692\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 0.33405864238739014 Validation Accuracy at 0.5001999139785767\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 0.30668920278549194 Validation Accuracy at 0.4909999370574951\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 0.30873903632164 Validation Accuracy at 0.5033999085426331\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 0.33823883533477783 Validation Accuracy at 0.5053999423980713\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 0.307828426361084 Validation Accuracy at 0.5051999688148499\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 0.3238401412963867 Validation Accuracy at 0.49479997158050537\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 0.35726243257522583 Validation Accuracy at 0.4963999390602112\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 0.3159952461719513 Validation Accuracy at 0.4987999498844147\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 0.31922709941864014 Validation Accuracy at 0.501800000667572\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 0.3642241656780243 Validation Accuracy at 0.49599993228912354\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 0.309905469417572 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 0.3070079982280731 Validation Accuracy at 0.5063999891281128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 0.3380490839481354 Validation Accuracy at 0.5091999769210815\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 0.3152029514312744 Validation Accuracy at 0.5189999341964722\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 0.3510498106479645 Validation Accuracy at 0.5115998983383179\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 0.318862646818161 Validation Accuracy at 0.5007999539375305\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 0.3230147361755371 Validation Accuracy at 0.5123999714851379\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 0.29669129848480225 Validation Accuracy at 0.5239999890327454\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 0.3217201828956604 Validation Accuracy at 0.5173999667167664\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 0.3202893137931824 Validation Accuracy at 0.5071999430656433\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 0.2840297818183899 Validation Accuracy at 0.5191999673843384\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 0.31694549322128296 Validation Accuracy at 0.5243999361991882\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 0.26268091797828674 Validation Accuracy at 0.5271999835968018\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 0.3076781630516052 Validation Accuracy at 0.5273998975753784\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 0.2555488049983978 Validation Accuracy at 0.5374000072479248\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 0.2857428193092346 Validation Accuracy at 0.5387999415397644\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 0.2687668800354004 Validation Accuracy at 0.5361999273300171\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 0.24710893630981445 Validation Accuracy at 0.5475999116897583\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 0.22884060442447662 Validation Accuracy at 0.5459999442100525\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss at 0.2254967987537384 Validation Accuracy at 0.545799970626831\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss at 0.1846523880958557 Validation Accuracy at 0.5463999509811401\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss at 0.22662916779518127 Validation Accuracy at 0.549799919128418\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss at 0.20132765173912048 Validation Accuracy at 0.5529999732971191\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss at 0.28073984384536743 Validation Accuracy at 0.5411999225616455\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss at 0.19509494304656982 Validation Accuracy at 0.5549999475479126\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss at 0.1512116938829422 Validation Accuracy at 0.5433999300003052\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss at 0.17404234409332275 Validation Accuracy at 0.5573999285697937\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss at 0.16283446550369263 Validation Accuracy at 0.5575999021530151\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss at 0.14661169052124023 Validation Accuracy at 0.5573998689651489\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss at 0.17958204448223114 Validation Accuracy at 0.5537998676300049\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss at 0.22789837419986725 Validation Accuracy at 0.5635999441146851\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss at 0.15829086303710938 Validation Accuracy at 0.5587999224662781\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss at 0.12715889513492584 Validation Accuracy at 0.5511999130249023\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss at 0.13959020376205444 Validation Accuracy at 0.5547999143600464\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss at 0.10904616117477417 Validation Accuracy at 0.5513999462127686\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss at 0.11615929752588272 Validation Accuracy at 0.5715999007225037\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss at 0.10467857122421265 Validation Accuracy at 0.5657998919487\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss at 0.1009846180677414 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss at 0.1355375498533249 Validation Accuracy at 0.5654000043869019\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss at 0.1107470914721489 Validation Accuracy at 0.5641999244689941\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss at 0.11365804076194763 Validation Accuracy at 0.5673999190330505\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss at 0.11941342800855637 Validation Accuracy at 0.5627999305725098\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss at 0.11218959838151932 Validation Accuracy at 0.5601999163627625\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss at 0.11879640817642212 Validation Accuracy at 0.5623998641967773\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss at 0.08930548280477524 Validation Accuracy at 0.5601999163627625\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss at 0.0817687138915062 Validation Accuracy at 0.5623998641967773\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss at 0.09485915303230286 Validation Accuracy at 0.5751999020576477\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss at 0.07869324088096619 Validation Accuracy at 0.5685999393463135\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss at 0.08065436035394669 Validation Accuracy at 0.5655999779701233\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss at 0.08168291300535202 Validation Accuracy at 0.5703998804092407\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss at 0.07842496037483215 Validation Accuracy at 0.5723999738693237\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss at 0.11330077052116394 Validation Accuracy at 0.5565999150276184\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss at 0.08068051934242249 Validation Accuracy at 0.5661998987197876\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss at 0.06695778667926788 Validation Accuracy at 0.5649998784065247\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss at 0.06267894059419632 Validation Accuracy at 0.5645999312400818\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss at 0.07170087099075317 Validation Accuracy at 0.5771999359130859\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss at 0.05930016562342644 Validation Accuracy at 0.5673999786376953\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss at 0.06640400737524033 Validation Accuracy at 0.5639998912811279\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss at 0.08350849151611328 Validation Accuracy at 0.5671999454498291\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss at 0.07179266214370728 Validation Accuracy at 0.5643999576568604\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss at 0.058796774595975876 Validation Accuracy at 0.5665999054908752\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss at 0.0582147054374218 Validation Accuracy at 0.5641999244689941\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss at 0.06395364552736282 Validation Accuracy at 0.5725999474525452\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss at 0.11327721178531647 Validation Accuracy at 0.5691999197006226\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss at 0.08082001656293869 Validation Accuracy at 0.5653999447822571\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss at 0.05510757490992546 Validation Accuracy at 0.5583999156951904\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss at 0.06905236840248108 Validation Accuracy at 0.5691999197006226\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss at 0.06198643147945404 Validation Accuracy at 0.561199963092804\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss at 0.05857092887163162 Validation Accuracy at 0.5747999548912048\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss at 0.07057960331439972 Validation Accuracy at 0.5687999129295349\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss at 0.038274746388196945 Validation Accuracy at 0.5675999522209167\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss at 0.04990839213132858 Validation Accuracy at 0.5651999115943909\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss at 0.05338568985462189 Validation Accuracy at 0.5641999244689941\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss at 0.0376533567905426 Validation Accuracy at 0.568399965763092\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss at 0.05381887033581734 Validation Accuracy at 0.5651999115943909\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss at 0.04570643976330757 Validation Accuracy at 0.5653999447822571\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss at 0.04650881513953209 Validation Accuracy at 0.5711999535560608\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss at 0.03622780740261078 Validation Accuracy at 0.5787999033927917\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss at 0.052206218242645264 Validation Accuracy at 0.5669999122619629\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss at 0.041815999895334244 Validation Accuracy at 0.5687999725341797\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss at 0.044530756771564484 Validation Accuracy at 0.571199893951416\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss at 0.03235689550638199 Validation Accuracy at 0.5767999291419983\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss at 0.032257791608572006 Validation Accuracy at 0.5717998743057251\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss at 0.033155135810375214 Validation Accuracy at 0.5651999711990356\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss at 0.07464935630559921 Validation Accuracy at 0.5625998973846436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167, CIFAR-10 Batch 1:  Loss at 0.03490016236901283 Validation Accuracy at 0.5791999101638794\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss at 0.02812972292304039 Validation Accuracy at 0.5727999210357666\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss at 0.03699535131454468 Validation Accuracy at 0.564799964427948\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss at 0.033653613179922104 Validation Accuracy at 0.5757999420166016\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss at 0.034739669412374496 Validation Accuracy at 0.5701999068260193\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss at 0.018189745023846626 Validation Accuracy at 0.5691999197006226\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss at 0.02595197968184948 Validation Accuracy at 0.5659998655319214\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss at 0.02538513019680977 Validation Accuracy at 0.5729999542236328\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss at 0.025978557765483856 Validation Accuracy at 0.5677999258041382\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss at 0.02255541831254959 Validation Accuracy at 0.5649999380111694\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss at 0.02279035374522209 Validation Accuracy at 0.5585999488830566\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss at 0.016816219314932823 Validation Accuracy at 0.5755999088287354\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss at 0.02617887407541275 Validation Accuracy at 0.5667999386787415\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss at 0.01946345902979374 Validation Accuracy at 0.5745998620986938\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss at 0.013512518256902695 Validation Accuracy at 0.5635999441146851\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss at 0.02460295706987381 Validation Accuracy at 0.5727999806404114\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss at 0.014772385358810425 Validation Accuracy at 0.5693999528884888\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss at 0.017964601516723633 Validation Accuracy at 0.5755999088287354\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss at 0.01893763057887554 Validation Accuracy at 0.577799916267395\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss at 0.031521935015916824 Validation Accuracy at 0.5775999426841736\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss at 0.04029058292508125 Validation Accuracy at 0.5629999041557312\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss at 0.04793012887239456 Validation Accuracy at 0.5731998682022095\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss at 0.02576272562146187 Validation Accuracy at 0.5643999576568604\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss at 0.01643010601401329 Validation Accuracy at 0.5671999454498291\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss at 0.031906601041555405 Validation Accuracy at 0.574199914932251\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss at 0.02056227996945381 Validation Accuracy at 0.572399914264679\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss at 0.02041253075003624 Validation Accuracy at 0.5679999589920044\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss at 0.026067666709423065 Validation Accuracy at 0.564599871635437\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss at 0.032793257385492325 Validation Accuracy at 0.569399893283844\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss at 0.025036223232746124 Validation Accuracy at 0.5657999515533447\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss at 0.029187161475419998 Validation Accuracy at 0.575999915599823\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss at 0.021167054772377014 Validation Accuracy at 0.572399914264679\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss at 0.020641222596168518 Validation Accuracy at 0.5751999616622925\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss at 0.020749051123857498 Validation Accuracy at 0.5767999291419983\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss at 0.034513987600803375 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss at 0.012197917327284813 Validation Accuracy at 0.5731999278068542\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss at 0.016317764297127724 Validation Accuracy at 0.5769999027252197\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss at 0.015655942261219025 Validation Accuracy at 0.5659998655319214\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss at 0.014752203598618507 Validation Accuracy at 0.577799916267395\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss at 0.011991644278168678 Validation Accuracy at 0.5771999359130859\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss at 0.01823471114039421 Validation Accuracy at 0.5653999447822571\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss at 0.014204166829586029 Validation Accuracy at 0.5743999481201172\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss at 0.019445370882749557 Validation Accuracy at 0.5637999176979065\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss at 0.026587096974253654 Validation Accuracy at 0.5725998878479004\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss at 0.01408518198877573 Validation Accuracy at 0.5697999000549316\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss at 0.012850851751863956 Validation Accuracy at 0.5635999441146851\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss at 0.012214899063110352 Validation Accuracy at 0.5747998952865601\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss at 0.0116606829687953 Validation Accuracy at 0.5705999135971069\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss at 0.020579740405082703 Validation Accuracy at 0.5639999508857727\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss at 0.015746627002954483 Validation Accuracy at 0.572999894618988\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss at 0.02490418776869774 Validation Accuracy at 0.5715999007225037\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss at 0.01930932328104973 Validation Accuracy at 0.5693999528884888\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss at 0.0251222625374794 Validation Accuracy at 0.5713998675346375\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss at 0.014672383666038513 Validation Accuracy at 0.5657999515533447\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss at 0.028503233566880226 Validation Accuracy at 0.5757999420166016\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss at 0.03852514177560806 Validation Accuracy at 0.5639999508857727\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss at 0.1445947289466858 Validation Accuracy at 0.5663999319076538\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss at 0.05911514163017273 Validation Accuracy at 0.5605999231338501\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss at 0.03385280445218086 Validation Accuracy at 0.567599892616272\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss at 0.08014523983001709 Validation Accuracy at 0.554599940776825\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss at 0.025486066937446594 Validation Accuracy at 0.5667999386787415\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss at 0.014537164010107517 Validation Accuracy at 0.5715999603271484\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss at 0.019584819674491882 Validation Accuracy at 0.5749999284744263\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss at 0.01314067654311657 Validation Accuracy at 0.5705999135971069\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss at 0.016221389174461365 Validation Accuracy at 0.5659999251365662\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss at 0.014110558666288853 Validation Accuracy at 0.5747998952865601\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss at 0.024515317752957344 Validation Accuracy at 0.5639999508857727\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss at 0.013607783243060112 Validation Accuracy at 0.5765999555587769\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss at 0.011068815365433693 Validation Accuracy at 0.5733999609947205\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss at 0.010785037651658058 Validation Accuracy at 0.5685999393463135\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss at 0.01059071533381939 Validation Accuracy at 0.5757999420166016\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss at 0.016197744756937027 Validation Accuracy at 0.5757998824119568\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss at 0.008048870600759983 Validation Accuracy at 0.5707999467849731\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss at 0.013420681469142437 Validation Accuracy at 0.5669999718666077\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss at 0.014697756618261337 Validation Accuracy at 0.5679998993873596\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss at 0.008505776524543762 Validation Accuracy at 0.5733999013900757\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss at 0.010346471332013607 Validation Accuracy at 0.5751999020576477\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss at 0.007887793704867363 Validation Accuracy at 0.5685999989509583\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss at 0.00909136701375246 Validation Accuracy at 0.5711999535560608\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss at 0.007920985110104084 Validation Accuracy at 0.572399914264679\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss at 0.008103746920824051 Validation Accuracy at 0.5759999752044678\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss at 0.01000023540109396 Validation Accuracy at 0.5625998973846436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249, CIFAR-10 Batch 1:  Loss at 0.006038809195160866 Validation Accuracy at 0.5627999305725098\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss at 0.005502840969711542 Validation Accuracy at 0.5733999013900757\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss at 0.007956838235259056 Validation Accuracy at 0.572399914264679\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss at 0.010035739280283451 Validation Accuracy at 0.5619999170303345\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss at 0.006800767034292221 Validation Accuracy at 0.5799999237060547\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss at 0.012855232693254948 Validation Accuracy at 0.5735999345779419\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss at 0.014642536640167236 Validation Accuracy at 0.5765998959541321\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss at 0.01577456295490265 Validation Accuracy at 0.5679998993873596\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.2935359477996826 Validation Accuracy at 0.20180000364780426\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss at 2.0830776691436768 Validation Accuracy at 0.28839999437332153\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss at 1.8501228094100952 Validation Accuracy at 0.3229999840259552\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss at 1.8165558576583862 Validation Accuracy at 0.30000001192092896\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss at 1.789144515991211 Validation Accuracy at 0.32499998807907104\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 1.8556772470474243 Validation Accuracy at 0.3779999613761902\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss at 1.664907455444336 Validation Accuracy at 0.39079996943473816\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss at 1.4439043998718262 Validation Accuracy at 0.4211999773979187\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss at 1.5695215463638306 Validation Accuracy at 0.4189999997615814\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss at 1.5530238151550293 Validation Accuracy at 0.43140000104904175\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 1.6800669431686401 Validation Accuracy at 0.4521999955177307\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss at 1.436428189277649 Validation Accuracy at 0.4465999901294708\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss at 1.3884813785552979 Validation Accuracy at 0.4785999357700348\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss at 1.520108938217163 Validation Accuracy at 0.46459996700286865\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss at 1.513895034790039 Validation Accuracy at 0.46299999952316284\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.535494089126587 Validation Accuracy at 0.5131999850273132\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss at 1.3279318809509277 Validation Accuracy at 0.4697999656200409\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss at 1.1735014915466309 Validation Accuracy at 0.51419997215271\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss at 1.2473660707473755 Validation Accuracy at 0.5299999117851257\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss at 1.2958470582962036 Validation Accuracy at 0.5151999592781067\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.3233470916748047 Validation Accuracy at 0.5419999361038208\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss at 1.0823864936828613 Validation Accuracy at 0.5589998960494995\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss at 1.0418012142181396 Validation Accuracy at 0.5591999292373657\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss at 1.1307458877563477 Validation Accuracy at 0.5745999813079834\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss at 1.142560601234436 Validation Accuracy at 0.5745999217033386\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.1999245882034302 Validation Accuracy at 0.5811999440193176\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss at 1.0329697132110596 Validation Accuracy at 0.5657999515533447\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss at 1.0534087419509888 Validation Accuracy at 0.5949999094009399\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss at 1.0579556226730347 Validation Accuracy at 0.5931999683380127\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss at 0.9894750118255615 Validation Accuracy at 0.605199933052063\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.051709771156311 Validation Accuracy at 0.6167998909950256\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss at 0.8990068435668945 Validation Accuracy at 0.6009998917579651\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss at 0.8992847204208374 Validation Accuracy at 0.6059999465942383\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss at 0.9306228160858154 Validation Accuracy at 0.621199905872345\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss at 0.9741674661636353 Validation Accuracy at 0.6187998652458191\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.0060415267944336 Validation Accuracy at 0.6147999167442322\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss at 0.9056488275527954 Validation Accuracy at 0.6207998394966125\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss at 0.8332825899124146 Validation Accuracy at 0.6139999032020569\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss at 0.8920305967330933 Validation Accuracy at 0.6207998991012573\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss at 0.8906127214431763 Validation Accuracy at 0.6377999186515808\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 0.9514087438583374 Validation Accuracy at 0.6213999390602112\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss at 0.8957291841506958 Validation Accuracy at 0.6231999397277832\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss at 0.790717363357544 Validation Accuracy at 0.6301999092102051\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss at 0.7926250100135803 Validation Accuracy at 0.6171998977661133\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss at 0.7913827300071716 Validation Accuracy at 0.6351999044418335\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 0.8491947650909424 Validation Accuracy at 0.637199878692627\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss at 0.8284024000167847 Validation Accuracy at 0.6389999389648438\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss at 0.7181586027145386 Validation Accuracy at 0.6465998888015747\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss at 0.7870926856994629 Validation Accuracy at 0.6415998935699463\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss at 0.7511641383171082 Validation Accuracy at 0.64739990234375\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 0.7632623910903931 Validation Accuracy at 0.6331998705863953\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss at 0.7069500088691711 Validation Accuracy at 0.645599901676178\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss at 0.7497900128364563 Validation Accuracy at 0.6361998915672302\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss at 0.6235703229904175 Validation Accuracy at 0.6473998427391052\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss at 0.6576650738716125 Validation Accuracy at 0.6569998860359192\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 0.6984736919403076 Validation Accuracy at 0.6511998772621155\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss at 0.6955757141113281 Validation Accuracy at 0.6407999396324158\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss at 0.6905691027641296 Validation Accuracy at 0.643799901008606\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss at 0.633720338344574 Validation Accuracy at 0.6561998724937439\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss at 0.6299251317977905 Validation Accuracy at 0.6511999368667603\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 0.6536357402801514 Validation Accuracy at 0.6545998454093933\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss at 0.6765404939651489 Validation Accuracy at 0.6509998440742493\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss at 0.7417664527893066 Validation Accuracy at 0.6511998772621155\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss at 0.6005880832672119 Validation Accuracy at 0.6591998934745789\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss at 0.594488263130188 Validation Accuracy at 0.6649998426437378\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 0.6212880611419678 Validation Accuracy at 0.6627998948097229\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss at 0.6808751821517944 Validation Accuracy at 0.660399854183197\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss at 0.6432919502258301 Validation Accuracy at 0.6597999334335327\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss at 0.5600454807281494 Validation Accuracy at 0.6595999002456665\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss at 0.559226393699646 Validation Accuracy at 0.6703998446464539\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 0.6205611228942871 Validation Accuracy at 0.656799852848053\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss at 0.5694451928138733 Validation Accuracy at 0.6529998779296875\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss at 0.6692984700202942 Validation Accuracy at 0.6587998867034912\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss at 0.48032182455062866 Validation Accuracy at 0.6665999293327332\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss at 0.47835588455200195 Validation Accuracy at 0.668199896812439\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 0.5976195335388184 Validation Accuracy at 0.671799898147583\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss at 0.502303421497345 Validation Accuracy at 0.6679998636245728\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss at 0.5977116823196411 Validation Accuracy at 0.6599999070167542\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss at 0.46238040924072266 Validation Accuracy at 0.6663998365402222\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss at 0.4962034523487091 Validation Accuracy at 0.6679998636245728\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 0.5624520182609558 Validation Accuracy at 0.6611999273300171\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss at 0.5770619511604309 Validation Accuracy at 0.6653998494148254\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss at 0.5315988659858704 Validation Accuracy at 0.6639999151229858\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss at 0.3916124701499939 Validation Accuracy at 0.6639998555183411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, CIFAR-10 Batch 5:  Loss at 0.42794981598854065 Validation Accuracy at 0.6655998229980469\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 0.5564538240432739 Validation Accuracy at 0.668799877166748\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss at 0.4124820828437805 Validation Accuracy at 0.668799877166748\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss at 0.5162951946258545 Validation Accuracy at 0.6691998839378357\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss at 0.37798261642456055 Validation Accuracy at 0.6665998697280884\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss at 0.398987352848053 Validation Accuracy at 0.674799919128418\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 0.5269198417663574 Validation Accuracy at 0.6729998588562012\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss at 0.41078728437423706 Validation Accuracy at 0.6741998791694641\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss at 0.4933167099952698 Validation Accuracy at 0.6787998676300049\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss at 0.48230254650115967 Validation Accuracy at 0.6675999164581299\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss at 0.36156177520751953 Validation Accuracy at 0.6729998588562012\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 0.48691439628601074 Validation Accuracy at 0.6727998852729797\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss at 0.3946034610271454 Validation Accuracy at 0.6781998872756958\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss at 0.4301203489303589 Validation Accuracy at 0.675399899482727\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss at 0.333527147769928 Validation Accuracy at 0.6637998819351196\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss at 0.33698350191116333 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 0.4609668552875519 Validation Accuracy at 0.6771999001502991\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss at 0.29218778014183044 Validation Accuracy at 0.6825998425483704\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss at 0.42772215604782104 Validation Accuracy at 0.6809998154640198\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss at 0.36727264523506165 Validation Accuracy at 0.6747998595237732\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss at 0.3193782866001129 Validation Accuracy at 0.6843999028205872\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 0.41256609559059143 Validation Accuracy at 0.6795998811721802\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss at 0.4336889386177063 Validation Accuracy at 0.6737999320030212\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss at 0.4361678957939148 Validation Accuracy at 0.6777998805046082\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss at 0.3801570236682892 Validation Accuracy at 0.6765998601913452\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss at 0.3071291744709015 Validation Accuracy at 0.6765998601913452\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 0.4306698441505432 Validation Accuracy at 0.6803998351097107\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss at 0.34795457124710083 Validation Accuracy at 0.68479984998703\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss at 0.45852407813072205 Validation Accuracy at 0.6823998689651489\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss at 0.32116174697875977 Validation Accuracy at 0.6657998561859131\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss at 0.2997168004512787 Validation Accuracy at 0.6759998798370361\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 0.4179037809371948 Validation Accuracy at 0.6805998682975769\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss at 0.3065786361694336 Validation Accuracy at 0.675399899482727\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss at 0.40625283122062683 Validation Accuracy at 0.6753998398780823\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss at 0.2508586645126343 Validation Accuracy at 0.6781998872756958\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss at 0.281713604927063 Validation Accuracy at 0.6787998676300049\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 0.41142046451568604 Validation Accuracy at 0.6839998960494995\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss at 0.24247527122497559 Validation Accuracy at 0.6869999170303345\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss at 0.3268124759197235 Validation Accuracy at 0.6883997917175293\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss at 0.27557364106178284 Validation Accuracy at 0.6815998554229736\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss at 0.20823697745800018 Validation Accuracy at 0.6831998825073242\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 0.3981044292449951 Validation Accuracy at 0.6817998290061951\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss at 0.23526959121227264 Validation Accuracy at 0.680199921131134\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss at 0.34167325496673584 Validation Accuracy at 0.6779998540878296\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss at 0.20958229899406433 Validation Accuracy at 0.6757999062538147\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss at 0.22844745218753815 Validation Accuracy at 0.6867998838424683\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 0.36688750982284546 Validation Accuracy at 0.6789999008178711\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss at 0.30997323989868164 Validation Accuracy at 0.6857998371124268\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss at 0.29865479469299316 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss at 0.22545981407165527 Validation Accuracy at 0.6805998682975769\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss at 0.21263901889324188 Validation Accuracy at 0.6863998770713806\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 0.2777445316314697 Validation Accuracy at 0.6911998391151428\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss at 0.23672735691070557 Validation Accuracy at 0.6767998337745667\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss at 0.275865375995636 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss at 0.210129514336586 Validation Accuracy at 0.681999921798706\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss at 0.26280754804611206 Validation Accuracy at 0.6801998615264893\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 0.2816411256790161 Validation Accuracy at 0.6747998595237732\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss at 0.22013849020004272 Validation Accuracy at 0.6849998831748962\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss at 0.262966513633728 Validation Accuracy at 0.6881998777389526\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss at 0.23771418631076813 Validation Accuracy at 0.6711998581886292\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss at 0.21265870332717896 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 0.2336946278810501 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss at 0.22835615277290344 Validation Accuracy at 0.681199848651886\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss at 0.2813178598880768 Validation Accuracy at 0.6845998764038086\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss at 0.21714141964912415 Validation Accuracy at 0.6871998310089111\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss at 0.24781093001365662 Validation Accuracy at 0.6897999048233032\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 0.25833940505981445 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss at 0.19225946068763733 Validation Accuracy at 0.6871999502182007\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss at 0.24538198113441467 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss at 0.1762034147977829 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss at 0.2733563184738159 Validation Accuracy at 0.6885999441146851\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 0.21888472139835358 Validation Accuracy at 0.684199869632721\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss at 0.16811248660087585 Validation Accuracy at 0.697999894618988\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss at 0.236221045255661 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss at 0.15725314617156982 Validation Accuracy at 0.681199848651886\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss at 0.2581337094306946 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 0.24768617749214172 Validation Accuracy at 0.6827998757362366\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss at 0.21128728985786438 Validation Accuracy at 0.6863998770713806\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss at 0.19914178550243378 Validation Accuracy at 0.6799999475479126\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss at 0.17304709553718567 Validation Accuracy at 0.6873998641967773\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss at 0.23402488231658936 Validation Accuracy at 0.6861997842788696\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 0.17712721228599548 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss at 0.19159623980522156 Validation Accuracy at 0.6775999069213867\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss at 0.22556327283382416 Validation Accuracy at 0.6867998242378235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, CIFAR-10 Batch 4:  Loss at 0.15254847705364227 Validation Accuracy at 0.6891999244689941\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss at 0.16579347848892212 Validation Accuracy at 0.686599850654602\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 0.17783696949481964 Validation Accuracy at 0.682999849319458\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss at 0.1713283509016037 Validation Accuracy at 0.6861998438835144\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss at 0.22295981645584106 Validation Accuracy at 0.6873998045921326\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss at 0.16469888389110565 Validation Accuracy at 0.691399872303009\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss at 0.19571512937545776 Validation Accuracy at 0.6877998113632202\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 0.17773091793060303 Validation Accuracy at 0.6909998059272766\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss at 0.1686905324459076 Validation Accuracy at 0.6833999156951904\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss at 0.1971798837184906 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss at 0.14036713540554047 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss at 0.2264210432767868 Validation Accuracy at 0.6937999129295349\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 0.13736476004123688 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss at 0.14797048270702362 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss at 0.1981121301651001 Validation Accuracy at 0.6921999454498291\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss at 0.14094382524490356 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss at 0.201776921749115 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 0.32411351799964905 Validation Accuracy at 0.6847999095916748\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss at 0.13047419488430023 Validation Accuracy at 0.679999828338623\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss at 0.17098581790924072 Validation Accuracy at 0.6877998113632202\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss at 0.20361346006393433 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss at 0.20804628729820251 Validation Accuracy at 0.6899998784065247\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 0.22908368706703186 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss at 0.16856074333190918 Validation Accuracy at 0.6919998526573181\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss at 0.17462268471717834 Validation Accuracy at 0.6959999203681946\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss at 0.16985173523426056 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss at 0.19516471028327942 Validation Accuracy at 0.6827998757362366\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 0.20593680441379547 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss at 0.15040791034698486 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss at 0.20579583942890167 Validation Accuracy at 0.6977999210357666\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss at 0.1652253419160843 Validation Accuracy at 0.689599871635437\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss at 0.12873733043670654 Validation Accuracy at 0.6859999299049377\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 0.16457465291023254 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss at 0.15563425421714783 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss at 0.2097693234682083 Validation Accuracy at 0.6919997930526733\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss at 0.13194121420383453 Validation Accuracy at 0.6849998235702515\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss at 0.141616553068161 Validation Accuracy at 0.6933998465538025\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 0.14228413999080658 Validation Accuracy at 0.6807998418807983\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss at 0.12992416322231293 Validation Accuracy at 0.701999843120575\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss at 0.15741033852100372 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss at 0.13694342970848083 Validation Accuracy at 0.689599871635437\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss at 0.1937217116355896 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 0.2569670081138611 Validation Accuracy at 0.6843999028205872\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss at 0.11742779612541199 Validation Accuracy at 0.6883999109268188\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss at 0.15805552899837494 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss at 0.17044542729854584 Validation Accuracy at 0.679399847984314\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss at 0.18464289605617523 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 0.1449040174484253 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss at 0.16703367233276367 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss at 0.19322338700294495 Validation Accuracy at 0.6897998452186584\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss at 0.10481829941272736 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss at 0.20048069953918457 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 0.18484044075012207 Validation Accuracy at 0.6899998784065247\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss at 0.13978612422943115 Validation Accuracy at 0.6913998126983643\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss at 0.1440981924533844 Validation Accuracy at 0.6893998980522156\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss at 0.0839509442448616 Validation Accuracy at 0.6951999068260193\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss at 0.16628174483776093 Validation Accuracy at 0.6983997821807861\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 0.1768362820148468 Validation Accuracy at 0.696199893951416\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss at 0.13891708850860596 Validation Accuracy at 0.6915997862815857\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss at 0.16943010687828064 Validation Accuracy at 0.6831998229026794\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss at 0.14350295066833496 Validation Accuracy at 0.6871998906135559\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss at 0.13207517564296722 Validation Accuracy at 0.694199800491333\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 0.17252396047115326 Validation Accuracy at 0.6917998194694519\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss at 0.14001621305942535 Validation Accuracy at 0.6891999244689941\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss at 0.14715905487537384 Validation Accuracy at 0.6911998391151428\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss at 0.14913997054100037 Validation Accuracy at 0.6881998777389526\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss at 0.10583063960075378 Validation Accuracy at 0.6945999264717102\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 0.1415361762046814 Validation Accuracy at 0.6843998432159424\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss at 0.13248294591903687 Validation Accuracy at 0.6911998391151428\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss at 0.12082414329051971 Validation Accuracy at 0.6935999393463135\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss at 0.11822675168514252 Validation Accuracy at 0.6919997930526733\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss at 0.15349499881267548 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 0.13704925775527954 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss at 0.1200549528002739 Validation Accuracy at 0.6937999129295349\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss at 0.12458749860525131 Validation Accuracy at 0.6829999089241028\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss at 0.11882540583610535 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss at 0.16146422922611237 Validation Accuracy at 0.6949998140335083\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 0.15199264883995056 Validation Accuracy at 0.6941998600959778\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss at 0.09284836798906326 Validation Accuracy at 0.7005999088287354\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss at 0.12367196381092072 Validation Accuracy at 0.7001999020576477\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss at 0.14521318674087524 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss at 0.14650315046310425 Validation Accuracy at 0.6959999203681946\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 0.09916931390762329 Validation Accuracy at 0.6851998567581177\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss at 0.10361151397228241 Validation Accuracy at 0.7007998824119568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, CIFAR-10 Batch 3:  Loss at 0.11029694229364395 Validation Accuracy at 0.6915997862815857\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss at 0.0930713415145874 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss at 0.16948671638965607 Validation Accuracy at 0.6913998126983643\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 0.09760163724422455 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss at 0.0865095853805542 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss at 0.08840252459049225 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss at 0.09969795495271683 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss at 0.16935163736343384 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 0.100272536277771 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss at 0.10663960129022598 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss at 0.09129743278026581 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss at 0.09883233904838562 Validation Accuracy at 0.6927999258041382\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss at 0.13795419037342072 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 0.1253833919763565 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss at 0.09287755936384201 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss at 0.07984055578708649 Validation Accuracy at 0.6935998797416687\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss at 0.12804070115089417 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss at 0.16781307756900787 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 0.09526163339614868 Validation Accuracy at 0.697999894618988\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss at 0.11897659301757812 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss at 0.10493940114974976 Validation Accuracy at 0.6929999589920044\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss at 0.08485367149114609 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss at 0.09217074513435364 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 0.1095714271068573 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss at 0.0872664824128151 Validation Accuracy at 0.6863998770713806\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss at 0.08284799009561539 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss at 0.08532127737998962 Validation Accuracy at 0.6947999000549316\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss at 0.09978953003883362 Validation Accuracy at 0.693199872970581\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 0.09725313633680344 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss at 0.07271919399499893 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss at 0.11713086813688278 Validation Accuracy at 0.6849998831748962\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss at 0.08320204168558121 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss at 0.12406817823648453 Validation Accuracy at 0.697399914264679\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 0.09187860041856766 Validation Accuracy at 0.6973999738693237\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss at 0.08796114474534988 Validation Accuracy at 0.7035998106002808\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss at 0.13211874663829803 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss at 0.051436834037303925 Validation Accuracy at 0.6989999413490295\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss at 0.13200561702251434 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 0.1154838502407074 Validation Accuracy at 0.703799843788147\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss at 0.07370100170373917 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss at 0.07770724594593048 Validation Accuracy at 0.6917998194694519\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss at 0.05849852412939072 Validation Accuracy at 0.695399820804596\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss at 0.12703552842140198 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 0.10693849623203278 Validation Accuracy at 0.6983997821807861\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss at 0.08089922368526459 Validation Accuracy at 0.6941998600959778\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss at 0.08310194313526154 Validation Accuracy at 0.694399893283844\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss at 0.07100790739059448 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss at 0.09445600211620331 Validation Accuracy at 0.7045997977256775\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 0.10149005800485611 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss at 0.06948985159397125 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss at 0.09507659077644348 Validation Accuracy at 0.7021998763084412\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss at 0.0646037757396698 Validation Accuracy at 0.6997998356819153\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss at 0.09327111393213272 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 0.11516545712947845 Validation Accuracy at 0.6951999664306641\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss at 0.09386729449033737 Validation Accuracy at 0.6921999454498291\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss at 0.10749200731515884 Validation Accuracy at 0.6919999122619629\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss at 0.08173913508653641 Validation Accuracy at 0.6975998282432556\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss at 0.09687002003192902 Validation Accuracy at 0.6937999129295349\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 0.07071629166603088 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss at 0.0744047611951828 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss at 0.07757840305566788 Validation Accuracy at 0.6931999325752258\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss at 0.07614780217409134 Validation Accuracy at 0.6935998201370239\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss at 0.09465768188238144 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 0.05816049501299858 Validation Accuracy at 0.7001997828483582\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss at 0.08594343811273575 Validation Accuracy at 0.6875998377799988\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss at 0.06931691616773605 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss at 0.05755704641342163 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss at 0.1031404510140419 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 0.08918704092502594 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss at 0.05175996571779251 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss at 0.07996825128793716 Validation Accuracy at 0.7007998824119568\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss at 0.05419069528579712 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss at 0.06962186098098755 Validation Accuracy at 0.7029998302459717\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 0.09652037918567657 Validation Accuracy at 0.701999843120575\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss at 0.07995196431875229 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss at 0.07810897380113602 Validation Accuracy at 0.6905998587608337\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss at 0.052571311593055725 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss at 0.07820850610733032 Validation Accuracy at 0.6971998810768127\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 0.11527946591377258 Validation Accuracy at 0.6907998919487\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss at 0.06454925984144211 Validation Accuracy at 0.6907998919487\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss at 0.08339167386293411 Validation Accuracy at 0.6817998290061951\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss at 0.05304505676031113 Validation Accuracy at 0.6889998316764832\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss at 0.10841602087020874 Validation Accuracy at 0.6903999447822571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 0.08431489765644073 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss at 0.05869828164577484 Validation Accuracy at 0.7011998891830444\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss at 0.07544548809528351 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss at 0.07118644565343857 Validation Accuracy at 0.689599871635437\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss at 0.1375071406364441 Validation Accuracy at 0.6969999074935913\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 0.0829729288816452 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss at 0.06690753996372223 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss at 0.08578920364379883 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss at 0.05969016253948212 Validation Accuracy at 0.6907997727394104\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss at 0.1357734054327011 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 0.09424593299627304 Validation Accuracy at 0.6957998275756836\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss at 0.04667234048247337 Validation Accuracy at 0.7041998505592346\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss at 0.08532146364450455 Validation Accuracy at 0.7023998498916626\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss at 0.0826094001531601 Validation Accuracy at 0.6901998519897461\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss at 0.08747246861457825 Validation Accuracy at 0.6949999332427979\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 0.10337163507938385 Validation Accuracy at 0.6977998614311218\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss at 0.04836674779653549 Validation Accuracy at 0.697399914264679\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss at 0.07330529391765594 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss at 0.07927889376878738 Validation Accuracy at 0.7001999020576477\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss at 0.1141929104924202 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 0.07181717455387115 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss at 0.05731361359357834 Validation Accuracy at 0.6945999264717102\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss at 0.06882277131080627 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss at 0.055273305624723434 Validation Accuracy at 0.6983998417854309\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss at 0.08502206206321716 Validation Accuracy at 0.6933998465538025\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 0.09930012375116348 Validation Accuracy at 0.691199779510498\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss at 0.052570197731256485 Validation Accuracy at 0.700799822807312\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss at 0.07834896445274353 Validation Accuracy at 0.7013998031616211\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss at 0.06219427287578583 Validation Accuracy at 0.6847999095916748\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss at 0.06802158802747726 Validation Accuracy at 0.694399893283844\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 0.05879645049571991 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss at 0.0491924025118351 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss at 0.06616322696208954 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss at 0.09832301735877991 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss at 0.08704046905040741 Validation Accuracy at 0.6977998614311218\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 0.05223222076892853 Validation Accuracy at 0.7051998376846313\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss at 0.02536434680223465 Validation Accuracy at 0.7023999094963074\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss at 0.07589422166347504 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss at 0.0600108839571476 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss at 0.06340955942869186 Validation Accuracy at 0.6965999007225037\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 0.06464788317680359 Validation Accuracy at 0.6919999122619629\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss at 0.028829054906964302 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss at 0.07417912036180496 Validation Accuracy at 0.6987998485565186\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss at 0.08396746963262558 Validation Accuracy at 0.6871998310089111\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss at 0.06974373012781143 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 0.054128777235746384 Validation Accuracy at 0.6983999013900757\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss at 0.0245862677693367 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss at 0.07527816295623779 Validation Accuracy at 0.6913999319076538\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss at 0.06396476924419403 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss at 0.054184913635253906 Validation Accuracy at 0.6949999332427979\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 0.053998835384845734 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss at 0.016564320772886276 Validation Accuracy at 0.7015998959541321\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss at 0.09223832190036774 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss at 0.05803929269313812 Validation Accuracy at 0.7011998295783997\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss at 0.08862978219985962 Validation Accuracy at 0.700799822807312\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 0.06864266842603683 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss at 0.022442970424890518 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss at 0.07455813139677048 Validation Accuracy at 0.7053998708724976\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss at 0.07130345702171326 Validation Accuracy at 0.7015998959541321\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss at 0.06712579727172852 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 0.06337958574295044 Validation Accuracy at 0.6933998465538025\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss at 0.025140056386590004 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss at 0.07374175637960434 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss at 0.0834403708577156 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss at 0.05319099873304367 Validation Accuracy at 0.7099997997283936\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 0.0800785943865776 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss at 0.01744205132126808 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss at 0.07086505740880966 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss at 0.05474519357085228 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss at 0.06384095549583435 Validation Accuracy at 0.6979998350143433\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 0.07039893418550491 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss at 0.017120184376835823 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss at 0.0725775733590126 Validation Accuracy at 0.7033998966217041\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss at 0.07142028212547302 Validation Accuracy at 0.6987999081611633\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss at 0.08690474927425385 Validation Accuracy at 0.696199893951416\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 0.08393256366252899 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss at 0.011804670095443726 Validation Accuracy at 0.6933998465538025\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss at 0.0802726224064827 Validation Accuracy at 0.7079998254776001\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss at 0.05825219675898552 Validation Accuracy at 0.6993999481201172\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss at 0.1019243597984314 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 0.07746820151805878 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss at 0.015211126767098904 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss at 0.07759125530719757 Validation Accuracy at 0.7031998634338379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, CIFAR-10 Batch 4:  Loss at 0.05939226225018501 Validation Accuracy at 0.6975998282432556\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss at 0.0739523321390152 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 0.07494810968637466 Validation Accuracy at 0.696199893951416\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss at 0.022328654304146767 Validation Accuracy at 0.7067998051643372\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss at 0.06511562317609787 Validation Accuracy at 0.7011998295783997\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss at 0.03604859486222267 Validation Accuracy at 0.6957998275756836\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss at 0.0722203403711319 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 0.05518314987421036 Validation Accuracy at 0.704399824142456\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss at 0.013021627441048622 Validation Accuracy at 0.693199872970581\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss at 0.06422235816717148 Validation Accuracy at 0.7033998370170593\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss at 0.04443685710430145 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss at 0.0799323096871376 Validation Accuracy at 0.7047998309135437\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 0.05593874305486679 Validation Accuracy at 0.7021998763084412\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss at 0.011480317451059818 Validation Accuracy at 0.7027998566627502\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss at 0.06518162041902542 Validation Accuracy at 0.7009997963905334\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss at 0.04538809135556221 Validation Accuracy at 0.7035999298095703\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss at 0.09198486804962158 Validation Accuracy at 0.7081998586654663\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 0.05744355544447899 Validation Accuracy at 0.6927998661994934\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss at 0.01860465109348297 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss at 0.07550559192895889 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss at 0.0404336117208004 Validation Accuracy at 0.6941999197006226\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss at 0.10762415826320648 Validation Accuracy at 0.7083998322486877\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 0.06674940139055252 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss at 0.02272075228393078 Validation Accuracy at 0.696199893951416\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss at 0.07005983591079712 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss at 0.02908938005566597 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss at 0.09374816715717316 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 0.07133873552083969 Validation Accuracy at 0.7049998641014099\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss at 0.03686738386750221 Validation Accuracy at 0.7073997855186462\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss at 0.07613994926214218 Validation Accuracy at 0.7031998038291931\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss at 0.05736977607011795 Validation Accuracy at 0.6941999197006226\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss at 0.10240983963012695 Validation Accuracy at 0.702799916267395\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 0.057576216757297516 Validation Accuracy at 0.6983997821807861\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss at 0.024795446544885635 Validation Accuracy at 0.7061998248100281\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss at 0.07104728370904922 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss at 0.021957198157906532 Validation Accuracy at 0.6951999068260193\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss at 0.09792366623878479 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 0.0509076826274395 Validation Accuracy at 0.7039998173713684\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss at 0.020814619958400726 Validation Accuracy at 0.7051998972892761\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss at 0.06969430297613144 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss at 0.04986422508955002 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss at 0.10151784121990204 Validation Accuracy at 0.7041999101638794\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 0.09003140777349472 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss at 0.018051357939839363 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss at 0.07479873299598694 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss at 0.050535667687654495 Validation Accuracy at 0.6987998485565186\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss at 0.09710520505905151 Validation Accuracy at 0.6957998275756836\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 0.06394796073436737 Validation Accuracy at 0.7073999047279358\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss at 0.024228690192103386 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss at 0.07341592758893967 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss at 0.03295993432402611 Validation Accuracy at 0.7079998254776001\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss at 0.07863813638687134 Validation Accuracy at 0.7067998051643372\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 0.060481999069452286 Validation Accuracy at 0.6977999210357666\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss at 0.03690288960933685 Validation Accuracy at 0.7053998112678528\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss at 0.06809379160404205 Validation Accuracy at 0.7049999237060547\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss at 0.05416301637887955 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss at 0.08073507994413376 Validation Accuracy at 0.6845998764038086\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 0.052254561334848404 Validation Accuracy at 0.7067999243736267\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss at 0.03799403831362724 Validation Accuracy at 0.7049998641014099\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss at 0.07756704092025757 Validation Accuracy at 0.6963998675346375\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss at 0.03785889968276024 Validation Accuracy at 0.6969999074935913\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss at 0.08395031094551086 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 0.07022606581449509 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss at 0.009585961699485779 Validation Accuracy at 0.6981999278068542\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss at 0.06819047778844833 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss at 0.043518319725990295 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss at 0.07433903217315674 Validation Accuracy at 0.7005998492240906\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 0.03846964240074158 Validation Accuracy at 0.6881998777389526\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss at 0.005510163959115744 Validation Accuracy at 0.6977999210357666\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss at 0.06578212231397629 Validation Accuracy at 0.697999894618988\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss at 0.037796199321746826 Validation Accuracy at 0.6983997821807861\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss at 0.056752417236566544 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 0.04076192528009415 Validation Accuracy at 0.7033997774124146\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss at 0.012567403726279736 Validation Accuracy at 0.7001999020576477\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss at 0.0470346175134182 Validation Accuracy at 0.7051998376846313\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss at 0.032767944037914276 Validation Accuracy at 0.7011998295783997\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss at 0.055348850786685944 Validation Accuracy at 0.7047998309135437\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 0.04408550262451172 Validation Accuracy at 0.6921998858451843\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss at 0.0104948990046978 Validation Accuracy at 0.697999894618988\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss at 0.08427265286445618 Validation Accuracy at 0.693199872970581\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss at 0.025898003950715065 Validation Accuracy at 0.691399872303009\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss at 0.07081381976604462 Validation Accuracy at 0.6971999406814575\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss at 0.054379500448703766 Validation Accuracy at 0.6995998024940491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, CIFAR-10 Batch 2:  Loss at 0.007773148827254772 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss at 0.07013098895549774 Validation Accuracy at 0.7015998363494873\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss at 0.03339007869362831 Validation Accuracy at 0.689599871635437\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss at 0.061957716941833496 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss at 0.050175298005342484 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss at 0.02155192568898201 Validation Accuracy at 0.7053998112678528\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss at 0.05563206598162651 Validation Accuracy at 0.7059999108314514\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss at 0.03680388256907463 Validation Accuracy at 0.6983999013900757\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss at 0.07025480270385742 Validation Accuracy at 0.7069999575614929\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss at 0.059725675731897354 Validation Accuracy at 0.7005998492240906\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss at 0.033814381808042526 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss at 0.06022735685110092 Validation Accuracy at 0.700799822807312\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss at 0.03995640575885773 Validation Accuracy at 0.7015998363494873\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss at 0.06156943738460541 Validation Accuracy at 0.697199821472168\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss at 0.07217695564031601 Validation Accuracy at 0.6895998120307922\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss at 0.008785988204181194 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss at 0.06504116952419281 Validation Accuracy at 0.7001997828483582\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss at 0.03410518169403076 Validation Accuracy at 0.7051998972892761\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss at 0.06511172652244568 Validation Accuracy at 0.7027997970581055\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss at 0.024525359272956848 Validation Accuracy at 0.7029999494552612\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss at 0.005720944609493017 Validation Accuracy at 0.7053998112678528\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss at 0.053861938416957855 Validation Accuracy at 0.7013999223709106\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss at 0.03219819813966751 Validation Accuracy at 0.699199914932251\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss at 0.059935882687568665 Validation Accuracy at 0.7057998180389404\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss at 0.06267811357975006 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss at 0.008693182840943336 Validation Accuracy at 0.7077999114990234\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss at 0.061664558947086334 Validation Accuracy at 0.7033997774124146\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss at 0.028117673471570015 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss at 0.04814697802066803 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss at 0.06000448018312454 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss at 0.011701367795467377 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss at 0.05549381300806999 Validation Accuracy at 0.6983997821807861\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss at 0.03319806605577469 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss at 0.06129959225654602 Validation Accuracy at 0.7005998492240906\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss at 0.058890875428915024 Validation Accuracy at 0.704399824142456\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss at 0.003143240697681904 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss at 0.06264987587928772 Validation Accuracy at 0.7053998112678528\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss at 0.01706790365278721 Validation Accuracy at 0.696199893951416\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss at 0.03288450092077255 Validation Accuracy at 0.703799843788147\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss at 0.03766702115535736 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss at 0.005461900494992733 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss at 0.08752410113811493 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss at 0.036402203142642975 Validation Accuracy at 0.6937999129295349\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss at 0.04488509148359299 Validation Accuracy at 0.7051998972892761\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss at 0.03206123039126396 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss at 0.005840279161930084 Validation Accuracy at 0.6971999406814575\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss at 0.1596795618534088 Validation Accuracy at 0.7053998708724976\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss at 0.01550367847084999 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss at 0.05171400308609009 Validation Accuracy at 0.703799843788147\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss at 0.024825675413012505 Validation Accuracy at 0.7041999101638794\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss at 0.009421114809811115 Validation Accuracy at 0.7059998512268066\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss at 0.1588621735572815 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss at 0.0302308090031147 Validation Accuracy at 0.7023998498916626\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss at 0.05081010237336159 Validation Accuracy at 0.7049998641014099\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss at 0.04289361089468002 Validation Accuracy at 0.7063998579978943\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss at 0.0006780190742574632 Validation Accuracy at 0.7075998783111572\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss at 0.07475674152374268 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss at 0.040628496557474136 Validation Accuracy at 0.694399893283844\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss at 0.047423526644706726 Validation Accuracy at 0.6959999203681946\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss at 0.026763278990983963 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss at 0.011347297579050064 Validation Accuracy at 0.7095999121665955\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss at 0.06837192177772522 Validation Accuracy at 0.7021998763084412\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss at 0.03315732255578041 Validation Accuracy at 0.7049999237060547\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss at 0.04375016689300537 Validation Accuracy at 0.708599865436554\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss at 0.02528412453830242 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss at 0.023761432617902756 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss at 0.0698724091053009 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss at 0.057599738240242004 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss at 0.04219115898013115 Validation Accuracy at 0.7093998789787292\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss at 0.017613740637898445 Validation Accuracy at 0.7049998044967651\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss at 0.00526023842394352 Validation Accuracy at 0.7117998600006104\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss at 0.06738171726465225 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss at 0.03348098695278168 Validation Accuracy at 0.6957999467849731\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss at 0.03608284145593643 Validation Accuracy at 0.6987998485565186\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss at 0.03877676650881767 Validation Accuracy at 0.7029998898506165\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss at 0.01183338277041912 Validation Accuracy at 0.7105998396873474\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss at 0.07893920689821243 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss at 0.04160144552588463 Validation Accuracy at 0.7019999027252197\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss at 0.05291182920336723 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss at 0.03645428642630577 Validation Accuracy at 0.7083998918533325\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss at 0.006356533616781235 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss at 0.061394285410642624 Validation Accuracy at 0.7079998254776001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, CIFAR-10 Batch 4:  Loss at 0.020263759419322014 Validation Accuracy at 0.707399845123291\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss at 0.03182976320385933 Validation Accuracy at 0.708599865436554\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss at 0.04482952877879143 Validation Accuracy at 0.7107998132705688\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss at 0.012905898503959179 Validation Accuracy at 0.7091999053955078\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss at 0.06697255373001099 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss at 0.02411864697933197 Validation Accuracy at 0.7035999298095703\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss at 0.07839331030845642 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss at 0.018742386251688004 Validation Accuracy at 0.71399986743927\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss at 0.019335251301527023 Validation Accuracy at 0.7011998295783997\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss at 0.07019005715847015 Validation Accuracy at 0.7097998857498169\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss at 0.050881851464509964 Validation Accuracy at 0.7053998708724976\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss at 0.03399553894996643 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss at 0.04075828567147255 Validation Accuracy at 0.7073999047279358\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss at 0.006721686106175184 Validation Accuracy at 0.7123998403549194\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss at 0.07779859006404877 Validation Accuracy at 0.709199845790863\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss at 0.045018550008535385 Validation Accuracy at 0.68479984998703\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss at 0.023410022258758545 Validation Accuracy at 0.703799843788147\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss at 0.027775723487138748 Validation Accuracy at 0.707399845123291\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss at 0.004835219122469425 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss at 0.07897551357746124 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss at 0.03185203671455383 Validation Accuracy at 0.7085999250411987\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss at 0.049295999109745026 Validation Accuracy at 0.7037999033927917\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss at 0.04495491459965706 Validation Accuracy at 0.7089999318122864\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss at 0.013335513882339 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss at 0.058719322085380554 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss at 0.025128798559308052 Validation Accuracy at 0.7079998850822449\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss at 0.04724378138780594 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss at 0.045637279748916626 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss at 0.0035079619847238064 Validation Accuracy at 0.709199845790863\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss at 0.0638001412153244 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss at 0.03729206323623657 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss at 0.042656056582927704 Validation Accuracy at 0.6977998614311218\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss at 0.04062653332948685 Validation Accuracy at 0.7081998586654663\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss at 0.005700771696865559 Validation Accuracy at 0.7137998342514038\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss at 0.06752044707536697 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss at 0.028031406924128532 Validation Accuracy at 0.7061997652053833\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss at 0.03907939791679382 Validation Accuracy at 0.712199866771698\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss at 0.04539254307746887 Validation Accuracy at 0.708599865436554\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss at 0.010619284585118294 Validation Accuracy at 0.707399845123291\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss at 0.06442596018314362 Validation Accuracy at 0.7085998058319092\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss at 0.02669123187661171 Validation Accuracy at 0.7051998972892761\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss at 0.03409608453512192 Validation Accuracy at 0.709399938583374\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss at 0.039080750197172165 Validation Accuracy at 0.709199845790863\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss at 0.004845348186790943 Validation Accuracy at 0.7071998119354248\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss at 0.06655807048082352 Validation Accuracy at 0.7123998999595642\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss at 0.030437154695391655 Validation Accuracy at 0.7037999033927917\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss at 0.036406051367521286 Validation Accuracy at 0.7061999440193176\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss at 0.03600942716002464 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss at 0.03203561529517174 Validation Accuracy at 0.7043998837471008\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss at 0.0713006854057312 Validation Accuracy at 0.712199866771698\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss at 0.03685465455055237 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss at 0.026595665141940117 Validation Accuracy at 0.7127997875213623\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss at 0.006847726181149483 Validation Accuracy at 0.708999752998352\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss at 0.014962386339902878 Validation Accuracy at 0.7099999189376831\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss at 0.06525567173957825 Validation Accuracy at 0.7091999053955078\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss at 0.03103417530655861 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss at 0.03452789783477783 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss at 0.0074733407236635685 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss at 0.0063734957948327065 Validation Accuracy at 0.708599865436554\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss at 0.06024010851979256 Validation Accuracy at 0.7093998193740845\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss at 0.03395688906311989 Validation Accuracy at 0.7109999060630798\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss at 0.036000873893499374 Validation Accuracy at 0.7001999616622925\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss at 0.04697613790631294 Validation Accuracy at 0.6993998289108276\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss at 0.0076751988381147385 Validation Accuracy at 0.7105998992919922\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss at 0.06637178361415863 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss at 0.03158728778362274 Validation Accuracy at 0.7033998966217041\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss at 0.023695725947618484 Validation Accuracy at 0.715599775314331\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss at 0.03471536189317703 Validation Accuracy at 0.7081997990608215\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss at 0.00551537936553359 Validation Accuracy at 0.705599844455719\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss at 0.06536448746919632 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss at 0.01754986122250557 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss at 0.03599269688129425 Validation Accuracy at 0.7101998925209045\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss at 0.010196064598858356 Validation Accuracy at 0.7119998335838318\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss at 0.023608507588505745 Validation Accuracy at 0.7025999426841736\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss at 0.06545282155275345 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss at 0.013589653186500072 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss at 0.054547667503356934 Validation Accuracy at 0.7029998898506165\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss at 0.026178983971476555 Validation Accuracy at 0.7035998106002808\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss at 0.007307255174964666 Validation Accuracy at 0.7065998911857605\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss at 0.060852404683828354 Validation Accuracy at 0.7077999114990234\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss at 0.025575803592801094 Validation Accuracy at 0.705599844455719\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss at 0.03304875269532204 Validation Accuracy at 0.7025998830795288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134, CIFAR-10 Batch 1:  Loss at 0.01599748432636261 Validation Accuracy at 0.7041998505592346\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss at 0.01139167882502079 Validation Accuracy at 0.7105998396873474\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss at 0.06771765649318695 Validation Accuracy at 0.7015998959541321\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss at 0.017744405195116997 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss at 0.03145107999444008 Validation Accuracy at 0.7083998918533325\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss at 0.01939062401652336 Validation Accuracy at 0.7081999182701111\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss at 0.10124152898788452 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss at 0.07008498907089233 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss at 0.02741013653576374 Validation Accuracy at 0.7063998579978943\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss at 0.02658754400908947 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss at 0.03101888671517372 Validation Accuracy at 0.7061999440193176\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss at 0.005322934594005346 Validation Accuracy at 0.7093998193740845\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss at 0.06877771019935608 Validation Accuracy at 0.7071998119354248\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss at 0.03995828703045845 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss at 0.023469626903533936 Validation Accuracy at 0.704399824142456\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss at 0.006802992429584265 Validation Accuracy at 0.702799916267395\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss at 0.06679856777191162 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss at 0.06679410487413406 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss at 0.03880476951599121 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss at 0.02814944088459015 Validation Accuracy at 0.7007998824119568\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss at 0.023380275815725327 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss at 0.07301617413759232 Validation Accuracy at 0.7055997848510742\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss at 0.06353352218866348 Validation Accuracy at 0.702599823474884\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss at 0.03007039614021778 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss at 0.03054228238761425 Validation Accuracy at 0.7047998309135437\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss at 0.014633184298872948 Validation Accuracy at 0.7089999318122864\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss at 0.06432518362998962 Validation Accuracy at 0.7035998702049255\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss at 0.07022740691900253 Validation Accuracy at 0.7049998044967651\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss at 0.013759984634816647 Validation Accuracy at 0.701999843120575\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss at 0.0331631638109684 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss at 0.03717832639813423 Validation Accuracy at 0.7123998403549194\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss at 0.009381385520100594 Validation Accuracy at 0.707399845123291\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss at 0.06086549162864685 Validation Accuracy at 0.7063999176025391\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss at 0.03113052248954773 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss at 0.03404567390680313 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss at 0.00189999642316252 Validation Accuracy at 0.7071999311447144\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss at 0.00984498392790556 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss at 0.07197099179029465 Validation Accuracy at 0.7005999088287354\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss at 0.016410641372203827 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss at 0.04101581126451492 Validation Accuracy at 0.6987998485565186\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss at 0.019842395558953285 Validation Accuracy at 0.7059998512268066\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss at 0.06338396668434143 Validation Accuracy at 0.701999843120575\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss at 0.07319189608097076 Validation Accuracy at 0.7011999487876892\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss at 0.01702684722840786 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss at 0.025308052077889442 Validation Accuracy at 0.704599916934967\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss at 0.002560507971793413 Validation Accuracy at 0.7071998119354248\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss at 0.010024229995906353 Validation Accuracy at 0.7067998051643372\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss at 0.06238948553800583 Validation Accuracy at 0.7123998999595642\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss at 0.04656090959906578 Validation Accuracy at 0.6987997889518738\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss at 0.027950352057814598 Validation Accuracy at 0.7035999298095703\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss at 0.048311442136764526 Validation Accuracy at 0.7063997983932495\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss at 0.008847711607813835 Validation Accuracy at 0.7093998193740845\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss at 0.07033465802669525 Validation Accuracy at 0.7027997970581055\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss at 0.02324221096932888 Validation Accuracy at 0.7039998173713684\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss at 0.029538679867982864 Validation Accuracy at 0.7087998986244202\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss at 0.003347414545714855 Validation Accuracy at 0.709199845790863\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss at 0.01270323432981968 Validation Accuracy at 0.708599865436554\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss at 0.06781066954135895 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss at 0.025977225974202156 Validation Accuracy at 0.6933999061584473\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss at 0.02831355482339859 Validation Accuracy at 0.7005997896194458\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss at 0.049688950181007385 Validation Accuracy at 0.7017998099327087\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss at 0.0036581805907189846 Validation Accuracy at 0.7125998735427856\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss at 0.09272868186235428 Validation Accuracy at 0.7079998850822449\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss at 0.035476818680763245 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss at 0.030008327215909958 Validation Accuracy at 0.7027998566627502\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss at 0.03953781723976135 Validation Accuracy at 0.7015999555587769\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss at 0.017613500356674194 Validation Accuracy at 0.7005999088287354\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss at 0.07546831667423248 Validation Accuracy at 0.7081997990608215\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss at 0.0196104533970356 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss at 0.09936432540416718 Validation Accuracy at 0.7041998505592346\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss at 0.003128577256575227 Validation Accuracy at 0.707399845123291\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss at 0.012849942781031132 Validation Accuracy at 0.6969998478889465\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss at 0.06458059698343277 Validation Accuracy at 0.7085999250411987\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss at 0.017244629561901093 Validation Accuracy at 0.7061998248100281\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss at 0.030475053936243057 Validation Accuracy at 0.7093998193740845\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss at 0.01353217288851738 Validation Accuracy at 0.7059998512268066\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss at 0.008689760230481625 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss at 0.07261652499437332 Validation Accuracy at 0.7097998857498169\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss at 0.0750519186258316 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss at 0.02545243874192238 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss at 0.008271890692412853 Validation Accuracy at 0.7023998498916626\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss at 0.008404669351875782 Validation Accuracy at 0.7049998044967651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150, CIFAR-10 Batch 3:  Loss at 0.11644262820482254 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss at 0.026918569579720497 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss at 0.0287086833268404 Validation Accuracy at 0.7065998911857605\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss at 0.0074875797145068645 Validation Accuracy at 0.7059998512268066\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss at 0.048010535538196564 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss at 0.061582598835229874 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss at 0.04436756297945976 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss at 0.019738713279366493 Validation Accuracy at 0.7015998959541321\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss at 0.0017284448258578777 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss at 0.004513964056968689 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss at 0.07356426864862442 Validation Accuracy at 0.6963999271392822\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss at 0.02674991264939308 Validation Accuracy at 0.6999999284744263\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss at 0.01657620631158352 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss at 0.005511564668267965 Validation Accuracy at 0.7065998315811157\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss at 0.005843199789524078 Validation Accuracy at 0.7041999101638794\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss at 0.06702522188425064 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss at 0.01394505426287651 Validation Accuracy at 0.7077999114990234\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss at 0.030510390177369118 Validation Accuracy at 0.7091997861862183\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss at 0.009154894389212132 Validation Accuracy at 0.7057998180389404\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss at 0.04062272980809212 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss at 0.0748433768749237 Validation Accuracy at 0.7033998370170593\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss at 0.025988228619098663 Validation Accuracy at 0.703799843788147\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss at 0.014717049896717072 Validation Accuracy at 0.700799822807312\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss at 0.007399197667837143 Validation Accuracy at 0.7059997916221619\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss at 0.007882810197770596 Validation Accuracy at 0.7069998979568481\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss at 0.06922000646591187 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss at 0.009074650704860687 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss at 0.02664031647145748 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss at 0.04834818094968796 Validation Accuracy at 0.7049998641014099\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss at 0.009851692244410515 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss at 0.07207438349723816 Validation Accuracy at 0.7091997861862183\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss at 0.01439744234085083 Validation Accuracy at 0.707399845123291\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss at 0.036788493394851685 Validation Accuracy at 0.7063999176025391\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss at 0.005315927788615227 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss at 0.006971703376621008 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss at 0.07563300430774689 Validation Accuracy at 0.703799843788147\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss at 0.027833860367536545 Validation Accuracy at 0.7027998566627502\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss at 0.020782209932804108 Validation Accuracy at 0.7049998641014099\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss at 0.001309682847931981 Validation Accuracy at 0.7157999277114868\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss at 0.013098221272230148 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss at 0.06324812769889832 Validation Accuracy at 0.7107998728752136\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss at 0.012701436877250671 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss at 0.028737816959619522 Validation Accuracy at 0.7061998248100281\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss at 0.01561315543949604 Validation Accuracy at 0.707399845123291\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss at 0.005785378627479076 Validation Accuracy at 0.7065998911857605\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss at 0.059050850570201874 Validation Accuracy at 0.7075998783111572\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss at 0.04188583791255951 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss at 0.01737687923014164 Validation Accuracy at 0.7033998966217041\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss at 0.015590805560350418 Validation Accuracy at 0.7079998850822449\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss at 0.009377283044159412 Validation Accuracy at 0.7115998268127441\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss at 0.06425264477729797 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss at 0.02227991260588169 Validation Accuracy at 0.7085999250411987\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss at 0.024447787553071976 Validation Accuracy at 0.7091999053955078\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss at 0.014340073801577091 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss at 0.012363137677311897 Validation Accuracy at 0.7119998335838318\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss at 0.06397732347249985 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss at 0.026306241750717163 Validation Accuracy at 0.7005999088287354\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss at 0.0054490636102855206 Validation Accuracy at 0.7021998763084412\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss at 0.020123440772294998 Validation Accuracy at 0.702799916267395\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss at 0.005684234201908112 Validation Accuracy at 0.7049998641014099\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss at 0.06297587603330612 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss at 0.01806059293448925 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss at 0.007074381224811077 Validation Accuracy at 0.6999999284744263\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss at 0.004647089634090662 Validation Accuracy at 0.7067998051643372\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss at 0.0037844288162887096 Validation Accuracy at 0.7013999223709106\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss at 0.0708073154091835 Validation Accuracy at 0.7105997800827026\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss at 0.021498017013072968 Validation Accuracy at 0.708599865436554\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss at 0.005984166637063026 Validation Accuracy at 0.7111998796463013\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss at 0.01985873654484749 Validation Accuracy at 0.7117999196052551\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss at 0.0068045626394450665 Validation Accuracy at 0.7041998505592346\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss at 0.05757192522287369 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss at 0.013862495310604572 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss at 0.018801050260663033 Validation Accuracy at 0.6947997808456421\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss at 0.006190634332597256 Validation Accuracy at 0.7087998986244202\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss at 0.005650098901242018 Validation Accuracy at 0.708399772644043\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss at 0.061287298798561096 Validation Accuracy at 0.7041998505592346\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss at 0.017114214599132538 Validation Accuracy at 0.6911998391151428\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss at 0.01309751532971859 Validation Accuracy at 0.7041999101638794\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss at 0.006907281000167131 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss at 0.004738594871014357 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss at 0.06565064936876297 Validation Accuracy at 0.7061998248100281\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss at 0.010135848075151443 Validation Accuracy at 0.7065998315811157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166, CIFAR-10 Batch 5:  Loss at 0.01662004366517067 Validation Accuracy at 0.7105998992919922\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss at 0.022118477150797844 Validation Accuracy at 0.7051997780799866\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss at 0.009040987119078636 Validation Accuracy at 0.7093998789787292\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss at 0.07264664769172668 Validation Accuracy at 0.7125998735427856\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss at 0.014997062273323536 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss at 0.00802411139011383 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss at 0.027935629710555077 Validation Accuracy at 0.703799843788147\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss at 0.011290169321000576 Validation Accuracy at 0.7021998763084412\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss at 0.06175321340560913 Validation Accuracy at 0.7087997794151306\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss at 0.015173381194472313 Validation Accuracy at 0.7023997902870178\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss at 0.005222117993980646 Validation Accuracy at 0.708599865436554\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss at 0.0051451013423502445 Validation Accuracy at 0.7119998931884766\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss at 0.02114076539874077 Validation Accuracy at 0.7075998783111572\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss at 0.06460016965866089 Validation Accuracy at 0.7107998132705688\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss at 0.00949070230126381 Validation Accuracy at 0.7059997916221619\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss at 0.009589946828782558 Validation Accuracy at 0.6963998675346375\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss at 0.012770872563123703 Validation Accuracy at 0.7015998363494873\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss at 0.0056940047070384026 Validation Accuracy at 0.703799843788147\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss at 0.07119551301002502 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss at 0.01335006020963192 Validation Accuracy at 0.7069998383522034\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss at 0.00993212591856718 Validation Accuracy at 0.6975999474525452\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss at 0.03147732466459274 Validation Accuracy at 0.6937997937202454\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss at 0.004903928376734257 Validation Accuracy at 0.7091997861862183\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss at 0.06161940470337868 Validation Accuracy at 0.7143998742103577\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss at 0.008951777592301369 Validation Accuracy at 0.7053998708724976\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss at 0.021132953464984894 Validation Accuracy at 0.7071998119354248\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss at 0.02601969987154007 Validation Accuracy at 0.7023998498916626\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss at 0.007377542555332184 Validation Accuracy at 0.7023999094963074\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss at 0.05905550718307495 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss at 0.01857713796198368 Validation Accuracy at 0.7087997794151306\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss at 0.009372133761644363 Validation Accuracy at 0.7057998180389404\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss at 0.013839444145560265 Validation Accuracy at 0.7005999088287354\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss at 0.01167900487780571 Validation Accuracy at 0.702599823474884\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss at 0.05783781781792641 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss at 0.013591513969004154 Validation Accuracy at 0.7043998837471008\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss at 0.011383265256881714 Validation Accuracy at 0.7025997638702393\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss at 0.02027827873826027 Validation Accuracy at 0.7071998119354248\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss at 0.003181800711899996 Validation Accuracy at 0.702799916267395\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss at 0.072728231549263 Validation Accuracy at 0.7035998106002808\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss at 0.02137009985744953 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss at 0.01130884513258934 Validation Accuracy at 0.702799916267395\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss at 0.014251336455345154 Validation Accuracy at 0.7035998106002808\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss at 0.001731446827761829 Validation Accuracy at 0.7075998187065125\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss at 0.06615011394023895 Validation Accuracy at 0.7049998044967651\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss at 0.025499816983938217 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss at 0.006673078518360853 Validation Accuracy at 0.7083998918533325\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss at 0.006118081510066986 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss at 0.0035048217978328466 Validation Accuracy at 0.6961998343467712\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss at 0.07240340858697891 Validation Accuracy at 0.704399824142456\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss at 0.009180660359561443 Validation Accuracy at 0.6959999203681946\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss at 0.008019649423658848 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss at 0.011792958714067936 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss at 0.0036524953320622444 Validation Accuracy at 0.6977999210357666\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss at 0.0631997138261795 Validation Accuracy at 0.7061998248100281\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss at 0.022885190322995186 Validation Accuracy at 0.6963998675346375\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss at 0.007369923871010542 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss at 0.008095317520201206 Validation Accuracy at 0.7083998322486877\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss at 0.009331494569778442 Validation Accuracy at 0.7029998302459717\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss at 0.06814808398485184 Validation Accuracy at 0.7029998302459717\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss at 0.024287985637784004 Validation Accuracy at 0.7063998579978943\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss at 0.007408798206597567 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss at 0.004750423599034548 Validation Accuracy at 0.7011998891830444\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss at 0.002820197958499193 Validation Accuracy at 0.7051997780799866\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss at 0.07188296318054199 Validation Accuracy at 0.7105998396873474\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss at 0.025066303089261055 Validation Accuracy at 0.7051998376846313\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss at 0.004460406489670277 Validation Accuracy at 0.710399866104126\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss at 0.003927929792553186 Validation Accuracy at 0.707399845123291\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss at 0.003594032721593976 Validation Accuracy at 0.7149999141693115\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss at 0.061283208429813385 Validation Accuracy at 0.7079998254776001\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss at 0.0350315123796463 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss at 0.018643947318196297 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss at 0.002280360320582986 Validation Accuracy at 0.7109999060630798\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss at 0.0063194390386343 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss at 0.06418807804584503 Validation Accuracy at 0.7069997787475586\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss at 0.01143342163413763 Validation Accuracy at 0.6997998356819153\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss at 0.006198683753609657 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss at 0.0023504586424678564 Validation Accuracy at 0.7085998058319092\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss at 0.01035655103623867 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss at 0.06405235081911087 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss at 0.013425401411950588 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss at 0.00928910169750452 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss at 0.01358695887029171 Validation Accuracy at 0.7033997774124146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, CIFAR-10 Batch 2:  Loss at 0.0010838306043297052 Validation Accuracy at 0.7065998911857605\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss at 0.07142063230276108 Validation Accuracy at 0.7047998309135437\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss at 0.014314183034002781 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss at 0.007185044698417187 Validation Accuracy at 0.7033998966217041\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss at 0.00508061284199357 Validation Accuracy at 0.7039998173713684\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss at 0.00546461995691061 Validation Accuracy at 0.7079999446868896\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss at 0.0797484964132309 Validation Accuracy at 0.7035998702049255\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss at 0.01534067653119564 Validation Accuracy at 0.6963999271392822\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss at 0.018362771719694138 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss at 0.0028484133072197437 Validation Accuracy at 0.7123998999595642\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss at 0.007123147137463093 Validation Accuracy at 0.7007998824119568\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss at 0.07473015040159225 Validation Accuracy at 0.7011998891830444\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss at 0.010333319194614887 Validation Accuracy at 0.7011998891830444\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss at 0.008527522906661034 Validation Accuracy at 0.7015998959541321\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss at 0.0033876386005431414 Validation Accuracy at 0.7069998383522034\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss at 0.04329758137464523 Validation Accuracy at 0.6969998478889465\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss at 0.07286539673805237 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss at 0.018127644434571266 Validation Accuracy at 0.7069998979568481\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss at 0.0031867276411503553 Validation Accuracy at 0.712199866771698\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss at 0.0037105767987668514 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss at 0.006760737858712673 Validation Accuracy at 0.7109998464584351\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss at 0.06492405384778976 Validation Accuracy at 0.7047998309135437\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss at 0.02519923634827137 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss at 0.008618020452558994 Validation Accuracy at 0.69899982213974\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss at 0.006379968486726284 Validation Accuracy at 0.7029998302459717\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss at 0.0072384304367005825 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss at 0.07674704492092133 Validation Accuracy at 0.6999998092651367\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss at 0.00936969742178917 Validation Accuracy at 0.7051998376846313\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss at 0.0015617592725902796 Validation Accuracy at 0.7049999237060547\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss at 0.0008105985471047461 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss at 0.0035279239527881145 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss at 0.06723831593990326 Validation Accuracy at 0.6965999007225037\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss at 0.016553310677409172 Validation Accuracy at 0.7111998796463013\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss at 0.0029931890312582254 Validation Accuracy at 0.7083998918533325\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss at 0.0036380384117364883 Validation Accuracy at 0.7129998803138733\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss at 0.007560521364212036 Validation Accuracy at 0.7087999582290649\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss at 0.06376761943101883 Validation Accuracy at 0.7163998484611511\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss at 0.007213658187538385 Validation Accuracy at 0.7021999359130859\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss at 0.0016798193100839853 Validation Accuracy at 0.7057998180389404\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss at 0.0009413701482117176 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss at 0.0010635161306709051 Validation Accuracy at 0.7109998464584351\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss at 0.0615958645939827 Validation Accuracy at 0.7035998702049255\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss at 0.0071874032728374004 Validation Accuracy at 0.705599844455719\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss at 0.003533006412908435 Validation Accuracy at 0.7065998911857605\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss at 0.005434113554656506 Validation Accuracy at 0.708599865436554\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss at 0.003114873543381691 Validation Accuracy at 0.708599865436554\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss at 0.06232864782214165 Validation Accuracy at 0.7115998268127441\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss at 0.008297908119857311 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss at 0.0019942023791372776 Validation Accuracy at 0.7077997922897339\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss at 0.007889283820986748 Validation Accuracy at 0.7075998187065125\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss at 0.0022352717351168394 Validation Accuracy at 0.7057998180389404\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss at 0.07414594292640686 Validation Accuracy at 0.708599865436554\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss at 0.009529728442430496 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss at 0.0044797929003834724 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss at 0.006462809629738331 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss at 0.009078769013285637 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss at 0.06879302859306335 Validation Accuracy at 0.7081998586654663\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss at 0.01440453715622425 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss at 0.010789159685373306 Validation Accuracy at 0.6991998553276062\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss at 0.03657554090023041 Validation Accuracy at 0.7069998383522034\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss at 0.004995511844754219 Validation Accuracy at 0.710399866104126\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss at 0.07660485804080963 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss at 0.009028707630932331 Validation Accuracy at 0.7083998322486877\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss at 0.0017675112467259169 Validation Accuracy at 0.7129998207092285\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss at 0.011109176091849804 Validation Accuracy at 0.700799822807312\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss at 0.01146766822785139 Validation Accuracy at 0.710399866104126\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss at 0.06589071452617645 Validation Accuracy at 0.7043998837471008\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss at 0.014806456863880157 Validation Accuracy at 0.708599865436554\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss at 0.002805414143949747 Validation Accuracy at 0.708399772644043\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss at 0.0006274691550061107 Validation Accuracy at 0.7109998464584351\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss at 0.00219355640001595 Validation Accuracy at 0.7045997977256775\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss at 0.06423092633485794 Validation Accuracy at 0.7041999101638794\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss at 0.02471197582781315 Validation Accuracy at 0.7059998512268066\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss at 0.007350729312747717 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss at 0.008999957703053951 Validation Accuracy at 0.7203998565673828\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss at 0.0254881102591753 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss at 0.07227130234241486 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss at 0.0123363072052598 Validation Accuracy at 0.7083998322486877\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss at 0.0024698269553482533 Validation Accuracy at 0.7105998992919922\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss at 0.0011735879816114902 Validation Accuracy at 0.7135998606681824\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss at 0.019711069762706757 Validation Accuracy at 0.7081999182701111\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss at 0.06859159469604492 Validation Accuracy at 0.7089998126029968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199, CIFAR-10 Batch 4:  Loss at 0.05053682625293732 Validation Accuracy at 0.703799843788147\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss at 0.0021910285577178 Validation Accuracy at 0.7099997997283936\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss at 0.006702504586428404 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss at 0.008516661822795868 Validation Accuracy at 0.7135998606681824\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss at 0.06325278431177139 Validation Accuracy at 0.7131998538970947\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss at 0.027141978964209557 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss at 0.009124372154474258 Validation Accuracy at 0.7163999080657959\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss at 0.006574524566531181 Validation Accuracy at 0.703799843788147\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss at 0.004900882951915264 Validation Accuracy at 0.701999843120575\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss at 0.08461843430995941 Validation Accuracy at 0.7051997780799866\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss at 0.0454505980014801 Validation Accuracy at 0.7075998187065125\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss at 0.006456919014453888 Validation Accuracy at 0.7049999237060547\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss at 0.0164342038333416 Validation Accuracy at 0.7127999067306519\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss at 0.0037364948075264692 Validation Accuracy at 0.7117998003959656\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss at 0.06073157861828804 Validation Accuracy at 0.7115998268127441\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss at 0.015590989030897617 Validation Accuracy at 0.7015998959541321\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss at 0.011408703401684761 Validation Accuracy at 0.7111998796463013\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss at 0.0014559150440618396 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss at 0.006129765417426825 Validation Accuracy at 0.7015999555587769\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss at 0.07880592346191406 Validation Accuracy at 0.7121998071670532\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss at 0.013047866523265839 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss at 0.014871176332235336 Validation Accuracy at 0.7105998396873474\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss at 0.004738925956189632 Validation Accuracy at 0.7073999047279358\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss at 0.0017720878822728992 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss at 0.060655947774648666 Validation Accuracy at 0.7061998248100281\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss at 0.012528485618531704 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss at 0.0030629278626292944 Validation Accuracy at 0.715799868106842\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss at 0.03100489266216755 Validation Accuracy at 0.7007998824119568\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss at 0.004881174303591251 Validation Accuracy at 0.7031998038291931\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss at 0.06570551544427872 Validation Accuracy at 0.7165998816490173\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss at 0.026094036176800728 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss at 0.007186383940279484 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss at 0.02568603679537773 Validation Accuracy at 0.7045998573303223\n",
      "Epoch 206, CIFAR-10 Batch 2:  Loss at 0.004737844690680504 Validation Accuracy at 0.7151998281478882\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss at 0.06066295877099037 Validation Accuracy at 0.7125998735427856\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss at 0.012692264281213284 Validation Accuracy at 0.7037999033927917\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss at 0.00606226222589612 Validation Accuracy at 0.7131998538970947\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss at 0.005333920009434223 Validation Accuracy at 0.7113998532295227\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss at 0.0710524469614029 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss at 0.05922292172908783 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss at 0.037305865436792374 Validation Accuracy at 0.7153998613357544\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss at 0.007582266349345446 Validation Accuracy at 0.7065998315811157\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss at 0.013721051625907421 Validation Accuracy at 0.7051998376846313\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss at 0.006773735862225294 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss at 0.060529924929142 Validation Accuracy at 0.7085998058319092\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss at 0.03940832242369652 Validation Accuracy at 0.7055997848510742\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss at 0.007111747749149799 Validation Accuracy at 0.707399845123291\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss at 0.023461615666747093 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss at 0.0006490069790743291 Validation Accuracy at 0.708599865436554\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss at 0.06083937734365463 Validation Accuracy at 0.7131999135017395\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss at 0.028747683390975 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss at 0.004012266639620066 Validation Accuracy at 0.7079998254776001\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss at 0.001292713452130556 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss at 0.0010347656207159162 Validation Accuracy at 0.7063998579978943\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss at 0.05805376172065735 Validation Accuracy at 0.7053999304771423\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss at 0.04033130407333374 Validation Accuracy at 0.6991998553276062\n",
      "Epoch 210, CIFAR-10 Batch 5:  Loss at 0.0038475319743156433 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss at 0.018387986347079277 Validation Accuracy at 0.7007998824119568\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss at 0.009335466660559177 Validation Accuracy at 0.7021998763084412\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss at 0.07159677892923355 Validation Accuracy at 0.7087998986244202\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss at 0.026072276756167412 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss at 0.009437954053282738 Validation Accuracy at 0.7121998071670532\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss at 0.017120027914643288 Validation Accuracy at 0.7107998132705688\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss at 0.002897134982049465 Validation Accuracy at 0.71399986743927\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss at 0.061761513352394104 Validation Accuracy at 0.7065998911857605\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss at 0.009990904480218887 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss at 0.0024891356006264687 Validation Accuracy at 0.7117998600006104\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss at 0.0033772927708923817 Validation Accuracy at 0.7075998783111572\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss at 0.005725140683352947 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss at 0.06347568333148956 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss at 0.010297014378011227 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss at 0.00798060093075037 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss at 0.011074023321270943 Validation Accuracy at 0.7151998281478882\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss at 0.001279439078643918 Validation Accuracy at 0.7011999487876892\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss at 0.05762474238872528 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss at 0.012400640174746513 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss at 0.013450158759951591 Validation Accuracy at 0.7095999121665955\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss at 0.0018439056584611535 Validation Accuracy at 0.7127998471260071\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss at 0.0026470995508134365 Validation Accuracy at 0.7023998498916626\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss at 0.06569764018058777 Validation Accuracy at 0.7041998505592346\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss at 0.009748178534209728 Validation Accuracy at 0.7005997896194458\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss at 0.0093358363956213 Validation Accuracy at 0.702599823474884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216, CIFAR-10 Batch 1:  Loss at 0.003178287297487259 Validation Accuracy at 0.7035999298095703\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss at 0.0029336782172322273 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss at 0.06781167536973953 Validation Accuracy at 0.7033998370170593\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss at 0.05486954376101494 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss at 0.007104410324245691 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss at 0.000772485276684165 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss at 0.00220490712672472 Validation Accuracy at 0.7015998363494873\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss at 0.0626683458685875 Validation Accuracy at 0.7033998966217041\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss at 0.028663629665970802 Validation Accuracy at 0.7069998383522034\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss at 0.01503860205411911 Validation Accuracy at 0.7091997861862183\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss at 0.013580468483269215 Validation Accuracy at 0.6963998675346375\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss at 0.0062071895226836205 Validation Accuracy at 0.7137998938560486\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss at 0.07328751683235168 Validation Accuracy at 0.7047998309135437\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss at 0.011065738275647163 Validation Accuracy at 0.6963998079299927\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss at 0.0025719450786709785 Validation Accuracy at 0.7153999209403992\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss at 0.0050443848595023155 Validation Accuracy at 0.7119998931884766\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss at 0.0023485200945287943 Validation Accuracy at 0.7125998735427856\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss at 0.06836103647947311 Validation Accuracy at 0.707399845123291\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss at 0.010933865793049335 Validation Accuracy at 0.7119998335838318\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss at 0.005824747495353222 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss at 0.0058351499028503895 Validation Accuracy at 0.7125998735427856\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss at 0.006699193734675646 Validation Accuracy at 0.7105997800827026\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss at 0.058176759630441666 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss at 0.029291732236742973 Validation Accuracy at 0.7029999494552612\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss at 0.0044341059401631355 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss at 0.00390240759588778 Validation Accuracy at 0.7133998870849609\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss at 0.002815655432641506 Validation Accuracy at 0.7161998748779297\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss at 0.06509216129779816 Validation Accuracy at 0.7019997835159302\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss at 0.018304191529750824 Validation Accuracy at 0.7083998918533325\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss at 0.005676935892552137 Validation Accuracy at 0.7155998945236206\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss at 0.006664309184998274 Validation Accuracy at 0.7081999182701111\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss at 0.009334436617791653 Validation Accuracy at 0.708599865436554\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss at 0.07125627249479294 Validation Accuracy at 0.7033998370170593\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss at 0.016636168584227562 Validation Accuracy at 0.7081998586654663\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss at 0.003354378044605255 Validation Accuracy at 0.7105998396873474\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss at 0.028055859729647636 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss at 0.00545682804659009 Validation Accuracy at 0.71319979429245\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss at 0.05834278464317322 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss at 0.028127964586019516 Validation Accuracy at 0.7113998532295227\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss at 0.003920150455087423 Validation Accuracy at 0.7101998329162598\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss at 0.02252819389104843 Validation Accuracy at 0.7123998999595642\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss at 0.0010975159239023924 Validation Accuracy at 0.7161998748779297\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss at 0.0638207495212555 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss at 0.012712256982922554 Validation Accuracy at 0.7103997468948364\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss at 0.00534788565710187 Validation Accuracy at 0.7149998545646667\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss at 0.005647061392664909 Validation Accuracy at 0.7149999141693115\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss at 0.0008149471250362694 Validation Accuracy at 0.7163998484611511\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss at 0.07476843148469925 Validation Accuracy at 0.7107999324798584\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss at 0.05459238588809967 Validation Accuracy at 0.7161998748779297\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss at 0.006664433516561985 Validation Accuracy at 0.7107998728752136\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss at 0.0042611174285411835 Validation Accuracy at 0.71399986743927\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss at 0.0004520891234278679 Validation Accuracy at 0.7133998274803162\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss at 0.06877819448709488 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss at 0.02584299072623253 Validation Accuracy at 0.7105998992919922\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss at 0.009820174425840378 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss at 0.005832136608660221 Validation Accuracy at 0.7129998207092285\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss at 0.0025615293998271227 Validation Accuracy at 0.7091999053955078\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss at 0.06513576954603195 Validation Accuracy at 0.7045997977256775\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss at 0.026659198105335236 Validation Accuracy at 0.7047998905181885\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss at 0.01861487329006195 Validation Accuracy at 0.7033998966217041\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss at 0.02183585613965988 Validation Accuracy at 0.6995999217033386\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss at 0.003494820324704051 Validation Accuracy at 0.7045997977256775\n",
      "Epoch 228, CIFAR-10 Batch 3:  Loss at 0.05636660382151604 Validation Accuracy at 0.7053998112678528\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss at 0.03931163623929024 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss at 0.030280431732535362 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss at 0.013223908841609955 Validation Accuracy at 0.7069998979568481\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss at 0.0035132381599396467 Validation Accuracy at 0.7115998268127441\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss at 0.06716752052307129 Validation Accuracy at 0.7077999114990234\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss at 0.025581907480955124 Validation Accuracy at 0.7057998776435852\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss at 0.022586435079574585 Validation Accuracy at 0.7075998783111572\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss at 0.008966582827270031 Validation Accuracy at 0.7135998606681824\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss at 0.0022377045825123787 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss at 0.07664062827825546 Validation Accuracy at 0.7063999176025391\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss at 0.02571190521121025 Validation Accuracy at 0.7145998477935791\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss at 0.01370519120246172 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss at 0.007457071449607611 Validation Accuracy at 0.7111998796463013\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss at 0.0011662555625662208 Validation Accuracy at 0.711199939250946\n",
      "Epoch 231, CIFAR-10 Batch 3:  Loss at 0.05757485702633858 Validation Accuracy at 0.7093998789787292\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss at 0.025808798149228096 Validation Accuracy at 0.7071999311447144\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss at 0.007630309090018272 Validation Accuracy at 0.7027998566627502\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss at 0.00520126149058342 Validation Accuracy at 0.7001999020576477\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss at 0.02477247826755047 Validation Accuracy at 0.7033998966217041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, CIFAR-10 Batch 3:  Loss at 0.057403478771448135 Validation Accuracy at 0.712199866771698\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss at 0.02490764670073986 Validation Accuracy at 0.7087998986244202\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss at 0.003263761755079031 Validation Accuracy at 0.7027997970581055\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss at 0.024969972670078278 Validation Accuracy at 0.7083998322486877\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss at 0.005562662612646818 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss at 0.061127178370952606 Validation Accuracy at 0.7125998735427856\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss at 0.027918873354792595 Validation Accuracy at 0.704399824142456\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss at 0.007446056231856346 Validation Accuracy at 0.7061998248100281\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss at 0.007293411996215582 Validation Accuracy at 0.7181999087333679\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss at 0.00921582616865635 Validation Accuracy at 0.7055997848510742\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss at 0.05705717206001282 Validation Accuracy at 0.7127998471260071\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss at 0.025321798399090767 Validation Accuracy at 0.7077998518943787\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss at 0.004467911086976528 Validation Accuracy at 0.7113998532295227\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss at 0.0033811386674642563 Validation Accuracy at 0.7049998044967651\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss at 0.009468497708439827 Validation Accuracy at 0.7141999006271362\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss at 0.09004954993724823 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss at 0.027242645621299744 Validation Accuracy at 0.7153998613357544\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss at 0.0030790241435170174 Validation Accuracy at 0.7153998613357544\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss at 0.010202296078205109 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss at 0.0015191561542451382 Validation Accuracy at 0.7093998789787292\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss at 0.0594511404633522 Validation Accuracy at 0.7105998396873474\n",
      "Epoch 236, CIFAR-10 Batch 4:  Loss at 0.026839902624487877 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss at 0.007013902999460697 Validation Accuracy at 0.7193998694419861\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss at 0.0023185943718999624 Validation Accuracy at 0.7109998464584351\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss at 0.02721596509218216 Validation Accuracy at 0.7127997875213623\n",
      "Epoch 237, CIFAR-10 Batch 3:  Loss at 0.05850329250097275 Validation Accuracy at 0.7157999277114868\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss at 0.028403861448168755 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss at 0.005075851920992136 Validation Accuracy at 0.7115998268127441\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss at 0.003226272761821747 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss at 0.010326539166271687 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss at 0.0627761259675026 Validation Accuracy at 0.7081999182701111\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss at 0.04221712052822113 Validation Accuracy at 0.7163999080657959\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss at 0.008575276471674442 Validation Accuracy at 0.7051998376846313\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss at 0.00823366828262806 Validation Accuracy at 0.714999794960022\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss at 0.034852996468544006 Validation Accuracy at 0.7127997875213623\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss at 0.06146847456693649 Validation Accuracy at 0.7071998715400696\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss at 0.027752267196774483 Validation Accuracy at 0.7115998864173889\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss at 0.004316678270697594 Validation Accuracy at 0.71399986743927\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss at 0.009979559108614922 Validation Accuracy at 0.7051998376846313\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss at 0.008221094496548176 Validation Accuracy at 0.7097997665405273\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss at 0.07100765407085419 Validation Accuracy at 0.7157999277114868\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss at 0.025833118706941605 Validation Accuracy at 0.7097998857498169\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss at 0.006923841778188944 Validation Accuracy at 0.7131998538970947\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss at 0.05900338664650917 Validation Accuracy at 0.7007998824119568\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss at 0.004935731180012226 Validation Accuracy at 0.7135998010635376\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss at 0.07146139442920685 Validation Accuracy at 0.7079998254776001\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss at 0.027922824025154114 Validation Accuracy at 0.710399866104126\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss at 0.0038485368713736534 Validation Accuracy at 0.7063998579978943\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss at 0.0007838625460863113 Validation Accuracy at 0.7141998410224915\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss at 0.001568581908941269 Validation Accuracy at 0.7063997983932495\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss at 0.07504787296056747 Validation Accuracy at 0.7117999196052551\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss at 0.030987106263637543 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss at 0.002362088533118367 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss at 0.009654685854911804 Validation Accuracy at 0.6947998404502869\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss at 0.009897010400891304 Validation Accuracy at 0.7027998566627502\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss at 0.09629335254430771 Validation Accuracy at 0.715799868106842\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss at 0.044029559940099716 Validation Accuracy at 0.7051999568939209\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss at 0.004803507588803768 Validation Accuracy at 0.7115998268127441\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss at 0.008748487569391727 Validation Accuracy at 0.6987999081611633\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss at 0.0039032555650919676 Validation Accuracy at 0.7105998992919922\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss at 0.05967199057340622 Validation Accuracy at 0.7143999338150024\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss at 0.0255878958851099 Validation Accuracy at 0.7145998477935791\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss at 0.0021551907993853092 Validation Accuracy at 0.7109999060630798\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss at 0.004168201237916946 Validation Accuracy at 0.715799868106842\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss at 0.007599553093314171 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss at 0.05788467451930046 Validation Accuracy at 0.710399866104126\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss at 0.027447327971458435 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss at 0.003497785422950983 Validation Accuracy at 0.7125998139381409\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss at 0.016490695998072624 Validation Accuracy at 0.705599844455719\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss at 0.0037857452407479286 Validation Accuracy at 0.7151998281478882\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss at 0.058017488569021225 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss at 0.03222435712814331 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss at 0.008945375680923462 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss at 0.0017983352299779654 Validation Accuracy at 0.7061998844146729\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss at 0.005847355350852013 Validation Accuracy at 0.7145999073982239\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss at 0.06113692373037338 Validation Accuracy at 0.7143998742103577\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss at 0.02652459591627121 Validation Accuracy at 0.7141998410224915\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss at 0.0020889658480882645 Validation Accuracy at 0.7151998281478882\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss at 0.004199988208711147 Validation Accuracy at 0.7141998410224915\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss at 0.013562118634581566 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss at 0.0671672523021698 Validation Accuracy at 0.7075998187065125\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss at 0.02763715758919716 Validation Accuracy at 0.7001998424530029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248, CIFAR-10 Batch 5:  Loss at 0.011866763234138489 Validation Accuracy at 0.704399824142456\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss at 0.028644919395446777 Validation Accuracy at 0.700799822807312\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss at 0.010140751488506794 Validation Accuracy at 0.703799843788147\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss at 0.06186230108141899 Validation Accuracy at 0.7031998038291931\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss at 0.02489180490374565 Validation Accuracy at 0.719799816608429\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss at 0.004761208314448595 Validation Accuracy at 0.7095998525619507\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss at 0.0012448958586901426 Validation Accuracy at 0.7095999121665955\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss at 0.01730925776064396 Validation Accuracy at 0.7079998850822449\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss at 0.05864068120718002 Validation Accuracy at 0.7117998600006104\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss at 0.047098565846681595 Validation Accuracy at 0.7085998058319092\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss at 0.004329308867454529 Validation Accuracy at 0.7085999250411987\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss at 0.0037194103933870792 Validation Accuracy at 0.7133998870849609\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss at 0.01998252049088478 Validation Accuracy at 0.7107998728752136\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss at 0.07900763303041458 Validation Accuracy at 0.7039998769760132\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss at 0.012142357416450977 Validation Accuracy at 0.7063997983932495\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss at 0.005480694584548473 Validation Accuracy at 0.7031999230384827\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss at 0.018248623237013817 Validation Accuracy at 0.7037997841835022\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss at 0.02719559147953987 Validation Accuracy at 0.7103998064994812\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss at 0.05862948298454285 Validation Accuracy at 0.7135998010635376\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss at 0.01241069845855236 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss at 0.005147206597030163 Validation Accuracy at 0.7093998789787292\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss at 0.0091813700273633 Validation Accuracy at 0.710399866104126\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss at 0.00987326167523861 Validation Accuracy at 0.7107998728752136\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss at 0.06719619035720825 Validation Accuracy at 0.713799774646759\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss at 0.04799683019518852 Validation Accuracy at 0.7123998403549194\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss at 0.009443717077374458 Validation Accuracy at 0.7055999040603638\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss at 0.0006951619870960712 Validation Accuracy at 0.6997999548912048\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss at 0.002817436121404171 Validation Accuracy at 0.7113998532295227\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss at 0.056421224027872086 Validation Accuracy at 0.7067998647689819\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss at 0.016006870195269585 Validation Accuracy at 0.7181998491287231\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss at 0.012243587523698807 Validation Accuracy at 0.7023999094963074\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss at 0.0015860827406868339 Validation Accuracy at 0.712199866771698\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss at 0.0321933813393116 Validation Accuracy at 0.7111998200416565\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss at 0.060751475393772125 Validation Accuracy at 0.706599771976471\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss at 0.010012051090598106 Validation Accuracy at 0.7089998722076416\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss at 0.004827863536775112 Validation Accuracy at 0.7115998268127441\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss at 0.007579153869301081 Validation Accuracy at 0.7127998471260071\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss at 0.030244098976254463 Validation Accuracy at 0.6961998343467712\n",
      "Epoch 256, CIFAR-10 Batch 3:  Loss at 0.06231493875384331 Validation Accuracy at 0.7065998315811157\n",
      "Epoch 256, CIFAR-10 Batch 4:  Loss at 0.010958991944789886 Validation Accuracy at 0.7099998593330383\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss at 0.0043303039856255054 Validation Accuracy at 0.7141998410224915\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7125796178343949\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAJ/CAYAAAB4GhsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f80100e0630>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 349
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
