{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:19, 10.0MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 1:\n",
      "Image - Min Value: 15 Max Value: 249\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f040a1e6f98>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 3\n",
    "sample_id = 1\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.86666667  0.25490196  0.18039216]\n",
      "   [ 0.16470588  0.81176471  0.40784314]\n",
      "   [ 0.84313725  0.05882353  0.16078431]\n",
      "   ..., \n",
      "   [ 0.74901961  0.5254902   0.0745098 ]\n",
      "   [ 0.10196078  0.49803922  0.51372549]\n",
      "   [ 0.8         0.95294118  0.65882353]]\n",
      "\n",
      "  [[ 0.03137255  0.85098039  0.29803922]\n",
      "   [ 0.20392157  0.05490196  0.65098039]\n",
      "   [ 0.78823529  0.56078431  0.83921569]\n",
      "   ..., \n",
      "   [ 0.91372549  0.2745098   0.38039216]\n",
      "   [ 0.36470588  0.28235294  0.16470588]\n",
      "   [ 0.34509804  0.5372549   0.95294118]]\n",
      "\n",
      "  [[ 0.09411765  0.57254902  0.88627451]\n",
      "   [ 0.65882353  0.63137255  0.9372549 ]\n",
      "   [ 0.65098039  0.23529412  0.44705882]\n",
      "   ..., \n",
      "   [ 0.28627451  0.39607843  0.82745098]\n",
      "   [ 0.68235294  0.02352941  0.02352941]\n",
      "   [ 0.44705882  0.12941176  0.64705882]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.7254902   0.68235294  0.05490196]\n",
      "   [ 0.70980392  0.20392157  0.75686275]\n",
      "   [ 0.94117647  0.44313725  0.81568627]\n",
      "   ..., \n",
      "   [ 0.41176471  0.47058824  0.03137255]\n",
      "   [ 0.30196078  0.49411765  0.89411765]\n",
      "   [ 0.15294118  0.44313725  0.23137255]]\n",
      "\n",
      "  [[ 0.25490196  0.55686275  0.04705882]\n",
      "   [ 0.31372549  0.78823529  0.69019608]\n",
      "   [ 0.14117647  0.96078431  0.43529412]\n",
      "   ..., \n",
      "   [ 0.79215686  0.5372549   0.10196078]\n",
      "   [ 0.59215686  0.5254902   0.14117647]\n",
      "   [ 0.94117647  0.16078431  0.53333333]]\n",
      "\n",
      "  [[ 0.21960784  0.09803922  0.58039216]\n",
      "   [ 0.30588235  0.22352941  0.89411765]\n",
      "   [ 0.71764706  0.05098039  0.50980392]\n",
      "   ..., \n",
      "   [ 0.53333333  0.72941176  0.24705882]\n",
      "   [ 0.91764706  0.25490196  0.97647059]\n",
      "   [ 0.60784314  0.49019608  0.61568627]]]\n",
      "\n",
      "\n",
      " [[[ 0.65490196  0.71372549  0.64705882]\n",
      "   [ 0.15686275  0.05490196  0.01568627]\n",
      "   [ 0.41176471  0.8627451   0.72156863]\n",
      "   ..., \n",
      "   [ 0.10196078  0.04705882  0.29411765]\n",
      "   [ 0.09019608  0.91764706  0.20392157]\n",
      "   [ 0.5372549   0.18431373  0.05490196]]\n",
      "\n",
      "  [[ 0.4627451   0.40784314  0.85098039]\n",
      "   [ 0.92941176  0.58431373  0.23137255]\n",
      "   [ 0.29019608  0.15686275  0.94509804]\n",
      "   ..., \n",
      "   [ 0.30196078  0.98431373  0.91372549]\n",
      "   [ 0.30196078  0.15686275  0.56078431]\n",
      "   [ 0.06666667  0.34509804  0.02745098]]\n",
      "\n",
      "  [[ 0.8745098   0.14901961  0.41568627]\n",
      "   [ 0.07058824  0.50588235  0.12941176]\n",
      "   [ 0.1254902   0.0627451   0.32941176]\n",
      "   ..., \n",
      "   [ 0.05490196  0.2627451   0.08627451]\n",
      "   [ 0.7254902   0.50588235  0.74509804]\n",
      "   [ 0.90588235  0.24313725  0.50980392]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.65098039  0.14509804  0.84705882]\n",
      "   [ 0.9254902   0.31764706  0.08627451]\n",
      "   [ 0.51372549  0.44705882  0.48627451]\n",
      "   ..., \n",
      "   [ 0.26666667  0.90588235  0.37254902]\n",
      "   [ 0.99607843  0.14509804  0.08627451]\n",
      "   [ 0.12941176  0.50196078  0.92941176]]\n",
      "\n",
      "  [[ 0.90588235  0.15294118  0.27058824]\n",
      "   [ 0.37254902  0.95294118  0.34117647]\n",
      "   [ 0.6627451   0.10588235  0.06666667]\n",
      "   ..., \n",
      "   [ 0.12941176  0.34117647  0.48627451]\n",
      "   [ 0.06666667  0.89803922  0.38039216]\n",
      "   [ 0.18823529  0.10196078  0.23921569]]\n",
      "\n",
      "  [[ 0.45490196  0.98823529  0.03137255]\n",
      "   [ 0.65882353  0.14901961  0.07843137]\n",
      "   [ 0.61960784  0.09803922  0.20784314]\n",
      "   ..., \n",
      "   [ 0.23137255  0.31764706  0.37647059]\n",
      "   [ 0.60392157  0.03529412  0.84705882]\n",
      "   [ 0.05098039  0.15294118  0.64705882]]]\n",
      "\n",
      "\n",
      " [[[ 0.6627451   0.95294118  0.83137255]\n",
      "   [ 0.79215686  0.2         0.9254902 ]\n",
      "   [ 0.47843137  0.8         0.53333333]\n",
      "   ..., \n",
      "   [ 0.31764706  0.84705882  0.08627451]\n",
      "   [ 0.87843137  0.43921569  0.05098039]\n",
      "   [ 0.58039216  0.50980392  0.41960784]]\n",
      "\n",
      "  [[ 0.05882353  0.66666667  0.27058824]\n",
      "   [ 0.23529412  0.74509804  0.43137255]\n",
      "   [ 0.03529412  0.92941176  0.67058824]\n",
      "   ..., \n",
      "   [ 0.79215686  0.89803922  0.3254902 ]\n",
      "   [ 0.19215686  0.08235294  0.03921569]\n",
      "   [ 0.69411765  0.97647059  0.60784314]]\n",
      "\n",
      "  [[ 0.28235294  0.70196078  0.89019608]\n",
      "   [ 0.81176471  0.67843137  0.77254902]\n",
      "   [ 0.47058824  0.89019608  0.62352941]\n",
      "   ..., \n",
      "   [ 0.49411765  0.79215686  0.39607843]\n",
      "   [ 0.30196078  0.9372549   0.40392157]\n",
      "   [ 0.8745098   0.4         0.91764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.84705882  0.51372549  0.91764706]\n",
      "   [ 0.32156863  0.21176471  0.74509804]\n",
      "   [ 0.24313725  0.02352941  0.17647059]\n",
      "   ..., \n",
      "   [ 0.06666667  0.81176471  0.18039216]\n",
      "   [ 0.40784314  0.67843137  0.43137255]\n",
      "   [ 0.18039216  0.66666667  0.86666667]]\n",
      "\n",
      "  [[ 0.03529412  0.27058824  0.4745098 ]\n",
      "   [ 0.          0.45882353  0.85098039]\n",
      "   [ 0.6         0.82352941  0.46666667]\n",
      "   ..., \n",
      "   [ 0.3372549   0.62745098  0.21568627]\n",
      "   [ 0.80784314  0.14117647  0.58039216]\n",
      "   [ 0.79215686  1.          0.36862745]]\n",
      "\n",
      "  [[ 0.69803922  0.16862745  0.84705882]\n",
      "   [ 0.42745098  0.43137255  0.49803922]\n",
      "   [ 0.77254902  0.30196078  0.44313725]\n",
      "   ..., \n",
      "   [ 0.53333333  0.2627451   0.8       ]\n",
      "   [ 0.92941176  0.00784314  0.01568627]\n",
      "   [ 0.15686275  0.56862745  0.56470588]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.36470588  0.49019608  0.9372549 ]\n",
      "   [ 0.40392157  0.12156863  0.10196078]\n",
      "   [ 0.99215686  0.11764706  0.08235294]\n",
      "   ..., \n",
      "   [ 0.84313725  0.01568627  0.29803922]\n",
      "   [ 0.62352941  0.69803922  0.97647059]\n",
      "   [ 0.04313725  0.77647059  0.92941176]]\n",
      "\n",
      "  [[ 0.28235294  0.2745098   0.85882353]\n",
      "   [ 0.09019608  0.76862745  0.28235294]\n",
      "   [ 0.50196078  0.82352941  0.63921569]\n",
      "   ..., \n",
      "   [ 0.62745098  0.9254902   0.65882353]\n",
      "   [ 0.43529412  0.98431373  0.61176471]\n",
      "   [ 0.85490196  0.9372549   0.36078431]]\n",
      "\n",
      "  [[ 0.54901961  0.94901961  0.48627451]\n",
      "   [ 0.99215686  0.16862745  0.87843137]\n",
      "   [ 0.83137255  0.4627451   0.88627451]\n",
      "   ..., \n",
      "   [ 0.89019608  0.94901961  0.27058824]\n",
      "   [ 0.48627451  0.91372549  0.10980392]\n",
      "   [ 0.82352941  0.71372549  0.72941176]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313725  0.02352941  0.10588235]\n",
      "   [ 0.41960784  0.45098039  0.87843137]\n",
      "   [ 0.58039216  0.09019608  0.81960784]\n",
      "   ..., \n",
      "   [ 0.34901961  0.80784314  0.30588235]\n",
      "   [ 0.3372549   0.99215686  0.21960784]\n",
      "   [ 0.15294118  0.53333333  0.89411765]]\n",
      "\n",
      "  [[ 0.45098039  0.88627451  0.01176471]\n",
      "   [ 0.66666667  0.55686275  0.71764706]\n",
      "   [ 0.74117647  0.24313725  0.01960784]\n",
      "   ..., \n",
      "   [ 0.41568627  0.78823529  0.25098039]\n",
      "   [ 0.42352941  0.81568627  0.09803922]\n",
      "   [ 0.77254902  0.65490196  0.48627451]]\n",
      "\n",
      "  [[ 0.91372549  0.12156863  0.2627451 ]\n",
      "   [ 0.59607843  0.29803922  0.1372549 ]\n",
      "   [ 0.23529412  0.05882353  0.12941176]\n",
      "   ..., \n",
      "   [ 0.13333333  0.59607843  0.39607843]\n",
      "   [ 0.89411765  0.08235294  0.08235294]\n",
      "   [ 0.61176471  0.86666667  0.21960784]]]\n",
      "\n",
      "\n",
      " [[[ 0.00784314  0.12156863  0.96078431]\n",
      "   [ 0.88235294  0.          0.3372549 ]\n",
      "   [ 0.62352941  0.85882353  0.99607843]\n",
      "   ..., \n",
      "   [ 0.01568627  0.1372549   0.00784314]\n",
      "   [ 0.73333333  0.09803922  0.64313725]\n",
      "   [ 0.14901961  0.55686275  0.88235294]]\n",
      "\n",
      "  [[ 0.12156863  0.03529412  0.2627451 ]\n",
      "   [ 0.22745098  0.81960784  0.76078431]\n",
      "   [ 0.60784314  0.01176471  0.8       ]\n",
      "   ..., \n",
      "   [ 0.63921569  0.03529412  0.20784314]\n",
      "   [ 0.82352941  0.40784314  0.89803922]\n",
      "   [ 0.45098039  0.21568627  0.50980392]]\n",
      "\n",
      "  [[ 0.68235294  0.90980392  0.78823529]\n",
      "   [ 0.85098039  0.41960784  0.2       ]\n",
      "   [ 0.52941176  0.74117647  0.79607843]\n",
      "   ..., \n",
      "   [ 0.59607843  0.03529412  0.03529412]\n",
      "   [ 0.73333333  0.39215686  0.23137255]\n",
      "   [ 0.70588235  0.76078431  0.78823529]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.70980392  0.74117647  0.07843137]\n",
      "   [ 0.63921569  0.88235294  0.29019608]\n",
      "   [ 0.10588235  0.74901961  0.78039216]\n",
      "   ..., \n",
      "   [ 0.25490196  0.4627451   0.01568627]\n",
      "   [ 0.23137255  0.33333333  0.78039216]\n",
      "   [ 0.4745098   0.83921569  0.10588235]]\n",
      "\n",
      "  [[ 0.67843137  0.01176471  0.33333333]\n",
      "   [ 0.32156863  0.49411765  0.2745098 ]\n",
      "   [ 0.81176471  0.25098039  1.        ]\n",
      "   ..., \n",
      "   [ 0.18431373  0.25098039  0.83529412]\n",
      "   [ 0.65882353  0.62352941  0.87058824]\n",
      "   [ 0.88627451  0.74509804  0.5254902 ]]\n",
      "\n",
      "  [[ 0.37647059  0.53333333  0.15686275]\n",
      "   [ 0.50588235  0.48235294  0.52156863]\n",
      "   [ 0.90588235  0.0627451   0.57647059]\n",
      "   ..., \n",
      "   [ 0.70588235  0.69803922  0.3372549 ]\n",
      "   [ 0.43137255  0.49411765  0.65882353]\n",
      "   [ 0.4745098   0.80392157  0.84313725]]]\n",
      "\n",
      "\n",
      " [[[ 0.18039216  0.75294118  0.54901961]\n",
      "   [ 0.68235294  0.61176471  0.47058824]\n",
      "   [ 0.62745098  0.0627451   0.74901961]\n",
      "   ..., \n",
      "   [ 0.18823529  0.19215686  0.31372549]\n",
      "   [ 0.71372549  1.          0.02745098]\n",
      "   [ 0.49411765  0.5254902   0.89803922]]\n",
      "\n",
      "  [[ 0.05882353  0.25882353  0.6627451 ]\n",
      "   [ 0.38823529  0.16862745  0.92941176]\n",
      "   [ 0.54901961  0.75686275  0.48235294]\n",
      "   ..., \n",
      "   [ 0.47843137  0.99607843  0.40392157]\n",
      "   [ 0.90196078  0.04313725  0.65490196]\n",
      "   [ 0.67058824  0.90980392  0.3372549 ]]\n",
      "\n",
      "  [[ 0.15294118  0.54901961  0.54117647]\n",
      "   [ 0.07843137  0.36862745  0.05490196]\n",
      "   [ 0.95294118  0.58431373  0.75686275]\n",
      "   ..., \n",
      "   [ 0.49411765  0.41176471  0.9372549 ]\n",
      "   [ 0.11372549  0.96078431  0.02745098]\n",
      "   [ 0.17647059  0.43529412  0.17647059]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.8745098   0.55686275  0.4627451 ]\n",
      "   [ 0.45490196  0.06666667  0.90588235]\n",
      "   [ 0.39607843  0.16470588  0.59215686]\n",
      "   ..., \n",
      "   [ 0.58823529  0.10588235  0.4627451 ]\n",
      "   [ 0.28627451  0.03137255  0.21176471]\n",
      "   [ 0.76470588  0.66666667  0.01960784]]\n",
      "\n",
      "  [[ 0.57647059  0.98431373  0.62352941]\n",
      "   [ 0.25098039  0.48627451  0.85490196]\n",
      "   [ 0.09019608  0.03921569  0.16862745]\n",
      "   ..., \n",
      "   [ 0.1372549   0.85098039  0.0745098 ]\n",
      "   [ 0.25882353  0.03137255  0.28627451]\n",
      "   [ 0.70980392  0.54901961  0.14901961]]\n",
      "\n",
      "  [[ 0.05882353  0.6         0.05490196]\n",
      "   [ 0.58823529  0.34901961  0.78431373]\n",
      "   [ 0.09803922  0.50196078  0.27843137]\n",
      "   ..., \n",
      "   [ 0.10196078  0.69019608  0.65490196]\n",
      "   [ 0.89803922  0.01176471  0.99215686]\n",
      "   [ 0.16862745  0.16862745  0.11764706]]]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : x: ({Number of images} , {Height} , {Width} , {Layers(colors)})\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \"\"\"\n",
    "    #Method 1 : Using simplest normalization\n",
    "    normalized = x/255\n",
    "   \n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    #method 2 :\n",
    "    a = 0.0\n",
    "    b = 1.0\n",
    "    normalized = a + ( ( (x - np.min(x))*(b - a) )/( np.max(x) - np.min(x) ) )\n",
    "    \n",
    "    print(normalized)\n",
    "\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#encoder = None\n",
    "def one_hot_encode(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return np.eye(10)[x]\n",
    "    #create the encoder - hard way\n",
    "    \"\"\"\n",
    "    global encoder\n",
    "    if encoder is not None:\n",
    "        #if the given lable has less than 10 values, we do not initiate encoder creation again, we would transform\n",
    "        #the array to it's one-hot encoded form based on lables\n",
    "        return encoder.transform(x)\n",
    "    else:\n",
    "        encoder = preprocessing.LabelBinarizer()\n",
    "        #Fuind one-hot vactor values\n",
    "        encoder.fit(x)\n",
    "        #transform and return the one-hot encoded lables\n",
    "        return encoder.transform(x)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.23137255  0.24313725  0.24705882]\n",
      "   [ 0.16862745  0.18039216  0.17647059]\n",
      "   [ 0.19607843  0.18823529  0.16862745]\n",
      "   ..., \n",
      "   [ 0.61960784  0.51764706  0.42352941]\n",
      "   [ 0.59607843  0.49019608  0.4       ]\n",
      "   [ 0.58039216  0.48627451  0.40392157]]\n",
      "\n",
      "  [[ 0.0627451   0.07843137  0.07843137]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.07058824  0.03137255  0.        ]\n",
      "   ..., \n",
      "   [ 0.48235294  0.34509804  0.21568627]\n",
      "   [ 0.46666667  0.3254902   0.19607843]\n",
      "   [ 0.47843137  0.34117647  0.22352941]]\n",
      "\n",
      "  [[ 0.09803922  0.09411765  0.08235294]\n",
      "   [ 0.0627451   0.02745098  0.        ]\n",
      "   [ 0.19215686  0.10588235  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4627451   0.32941176  0.19607843]\n",
      "   [ 0.47058824  0.32941176  0.19607843]\n",
      "   [ 0.42745098  0.28627451  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81568627  0.66666667  0.37647059]\n",
      "   [ 0.78823529  0.6         0.13333333]\n",
      "   [ 0.77647059  0.63137255  0.10196078]\n",
      "   ..., \n",
      "   [ 0.62745098  0.52156863  0.2745098 ]\n",
      "   [ 0.21960784  0.12156863  0.02745098]\n",
      "   [ 0.20784314  0.13333333  0.07843137]]\n",
      "\n",
      "  [[ 0.70588235  0.54509804  0.37647059]\n",
      "   [ 0.67843137  0.48235294  0.16470588]\n",
      "   [ 0.72941176  0.56470588  0.11764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.58039216  0.36862745]\n",
      "   [ 0.38039216  0.24313725  0.13333333]\n",
      "   [ 0.3254902   0.20784314  0.13333333]]\n",
      "\n",
      "  [[ 0.69411765  0.56470588  0.45490196]\n",
      "   [ 0.65882353  0.50588235  0.36862745]\n",
      "   [ 0.70196078  0.55686275  0.34117647]\n",
      "   ..., \n",
      "   [ 0.84705882  0.72156863  0.54901961]\n",
      "   [ 0.59215686  0.4627451   0.32941176]\n",
      "   [ 0.48235294  0.36078431  0.28235294]]]\n",
      "\n",
      "\n",
      " [[[ 0.60392157  0.69411765  0.73333333]\n",
      "   [ 0.49411765  0.5372549   0.53333333]\n",
      "   [ 0.41176471  0.40784314  0.37254902]\n",
      "   ..., \n",
      "   [ 0.35686275  0.37254902  0.27843137]\n",
      "   [ 0.34117647  0.35294118  0.27843137]\n",
      "   [ 0.30980392  0.31764706  0.2745098 ]]\n",
      "\n",
      "  [[ 0.54901961  0.62745098  0.6627451 ]\n",
      "   [ 0.56862745  0.6         0.60392157]\n",
      "   [ 0.49019608  0.49019608  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.37647059  0.38823529  0.30588235]\n",
      "   [ 0.30196078  0.31372549  0.24313725]\n",
      "   [ 0.27843137  0.28627451  0.23921569]]\n",
      "\n",
      "  [[ 0.54901961  0.60784314  0.64313725]\n",
      "   [ 0.54509804  0.57254902  0.58431373]\n",
      "   [ 0.45098039  0.45098039  0.43921569]\n",
      "   ..., \n",
      "   [ 0.30980392  0.32156863  0.25098039]\n",
      "   [ 0.26666667  0.2745098   0.21568627]\n",
      "   [ 0.2627451   0.27058824  0.21568627]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.68627451  0.65490196  0.65098039]\n",
      "   [ 0.61176471  0.60392157  0.62745098]\n",
      "   [ 0.60392157  0.62745098  0.66666667]\n",
      "   ..., \n",
      "   [ 0.16470588  0.13333333  0.14117647]\n",
      "   [ 0.23921569  0.20784314  0.22352941]\n",
      "   [ 0.36470588  0.3254902   0.35686275]]\n",
      "\n",
      "  [[ 0.64705882  0.60392157  0.50196078]\n",
      "   [ 0.61176471  0.59607843  0.50980392]\n",
      "   [ 0.62352941  0.63137255  0.55686275]\n",
      "   ..., \n",
      "   [ 0.40392157  0.36470588  0.37647059]\n",
      "   [ 0.48235294  0.44705882  0.47058824]\n",
      "   [ 0.51372549  0.4745098   0.51372549]]\n",
      "\n",
      "  [[ 0.63921569  0.58039216  0.47058824]\n",
      "   [ 0.61960784  0.58039216  0.47843137]\n",
      "   [ 0.63921569  0.61176471  0.52156863]\n",
      "   ..., \n",
      "   [ 0.56078431  0.52156863  0.54509804]\n",
      "   [ 0.56078431  0.5254902   0.55686275]\n",
      "   [ 0.56078431  0.52156863  0.56470588]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   ..., \n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313725  0.47058824  0.43921569]\n",
      "   [ 0.43529412  0.4627451   0.43529412]\n",
      "   [ 0.41176471  0.43921569  0.41568627]\n",
      "   ..., \n",
      "   [ 0.28235294  0.31764706  0.31372549]\n",
      "   [ 0.28235294  0.31372549  0.30980392]\n",
      "   [ 0.28235294  0.31372549  0.30980392]]\n",
      "\n",
      "  [[ 0.43529412  0.4627451   0.43137255]\n",
      "   [ 0.40784314  0.43529412  0.40784314]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   ..., \n",
      "   [ 0.26666667  0.29411765  0.28627451]\n",
      "   [ 0.2745098   0.29803922  0.29411765]\n",
      "   [ 0.30588235  0.32941176  0.32156863]]\n",
      "\n",
      "  [[ 0.41568627  0.44313725  0.41176471]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   [ 0.37254902  0.4         0.36862745]\n",
      "   ..., \n",
      "   [ 0.30588235  0.33333333  0.3254902 ]\n",
      "   [ 0.30980392  0.33333333  0.3254902 ]\n",
      "   [ 0.31372549  0.3372549   0.32941176]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.27058824  0.27843137  0.20392157]\n",
      "   [ 0.24313725  0.24313725  0.19215686]\n",
      "   [ 0.22745098  0.22352941  0.18823529]\n",
      "   ..., \n",
      "   [ 0.4627451   0.49019608  0.31372549]\n",
      "   [ 0.4745098   0.49019608  0.28627451]\n",
      "   [ 0.48235294  0.50588235  0.29019608]]\n",
      "\n",
      "  [[ 0.2745098   0.27843137  0.19215686]\n",
      "   [ 0.23137255  0.22745098  0.18431373]\n",
      "   [ 0.2         0.19215686  0.16862745]\n",
      "   ..., \n",
      "   [ 0.48235294  0.48235294  0.3254902 ]\n",
      "   [ 0.45882353  0.47843137  0.29803922]\n",
      "   [ 0.41568627  0.43921569  0.25490196]]\n",
      "\n",
      "  [[ 0.28627451  0.28235294  0.18431373]\n",
      "   [ 0.25490196  0.24705882  0.17647059]\n",
      "   [ 0.20392157  0.19607843  0.17254902]\n",
      "   ..., \n",
      "   [ 0.47058824  0.46666667  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.27058824]\n",
      "   [ 0.42745098  0.43529412  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.76862745  0.82352941  0.37254902]\n",
      "   [ 0.79215686  0.82745098  0.42745098]\n",
      "   [ 0.76078431  0.78823529  0.41176471]\n",
      "   ..., \n",
      "   [ 0.77647059  0.84313725  0.35294118]\n",
      "   [ 0.81568627  0.85882353  0.42352941]\n",
      "   [ 0.83137255  0.88235294  0.43529412]]\n",
      "\n",
      "  [[ 0.74509804  0.80784314  0.36862745]\n",
      "   [ 0.74901961  0.80784314  0.37254902]\n",
      "   [ 0.74509804  0.80392157  0.36470588]\n",
      "   ..., \n",
      "   [ 0.77647059  0.83921569  0.36862745]\n",
      "   [ 0.76470588  0.81568627  0.37254902]\n",
      "   [ 0.80784314  0.8627451   0.41960784]]\n",
      "\n",
      "  [[ 0.71764706  0.76470588  0.36470588]\n",
      "   [ 0.72156863  0.77254902  0.37647059]\n",
      "   [ 0.71372549  0.75686275  0.36078431]\n",
      "   ..., \n",
      "   [ 0.75686275  0.81176471  0.35686275]\n",
      "   [ 0.74117647  0.8         0.34117647]\n",
      "   [ 0.77647059  0.82745098  0.39215686]]]\n",
      "\n",
      "\n",
      " [[[ 0.61960784  0.61960784  0.61960784]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61960784  0.62352941  0.60392157]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59215686]]\n",
      "\n",
      "  [[ 0.61568627  0.61568627  0.61568627]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.6       ]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59607843]]\n",
      "\n",
      "  [[ 0.61176471  0.61176471  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.22745098  0.26666667  0.26666667]\n",
      "   [ 0.24313725  0.28235294  0.29019608]\n",
      "   [ 0.21960784  0.25882353  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.24313725  0.2745098   0.30196078]\n",
      "   [ 0.18823529  0.22352941  0.23137255]\n",
      "   [ 0.22352941  0.25882353  0.26666667]]\n",
      "\n",
      "  [[ 0.20392157  0.24313725  0.23921569]\n",
      "   [ 0.23529412  0.27058824  0.27843137]\n",
      "   [ 0.16862745  0.20392157  0.20392157]\n",
      "   ..., \n",
      "   [ 0.25882353  0.29803922  0.30980392]\n",
      "   [ 0.21568627  0.25882353  0.2627451 ]\n",
      "   [ 0.23529412  0.2745098   0.29019608]]\n",
      "\n",
      "  [[ 0.19607843  0.22745098  0.23137255]\n",
      "   [ 0.2         0.21960784  0.21960784]\n",
      "   [ 0.20784314  0.23529412  0.23921569]\n",
      "   ..., \n",
      "   [ 0.18039216  0.21176471  0.21176471]\n",
      "   [ 0.22745098  0.25882353  0.26666667]\n",
      "   [ 0.23921569  0.28235294  0.29019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.76862745  0.67058824  0.51372549]\n",
      "   [ 0.76862745  0.6745098   0.51372549]\n",
      "   [ 0.75686275  0.65882353  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69411765  0.6         0.50980392]\n",
      "   [ 0.67058824  0.57647059  0.48627451]\n",
      "   [ 0.64705882  0.55294118  0.45882353]]\n",
      "\n",
      "  [[ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.76470588  0.6627451   0.50588235]\n",
      "   ..., \n",
      "   [ 0.69803922  0.60784314  0.51764706]\n",
      "   [ 0.6745098   0.58431373  0.49411765]\n",
      "   [ 0.65490196  0.56470588  0.47058824]]\n",
      "\n",
      "  [[ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.75294118  0.65490196  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69019608  0.60392157  0.51764706]\n",
      "   [ 0.66666667  0.58431373  0.49411765]\n",
      "   [ 0.64705882  0.56078431  0.4745098 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.7372549   0.61960784  0.40392157]\n",
      "   [ 0.7372549   0.62745098  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.4       ]\n",
      "   ..., \n",
      "   [ 0.8         0.74509804  0.69803922]\n",
      "   [ 0.78431373  0.74117647  0.70196078]\n",
      "   [ 0.76862745  0.71764706  0.68235294]]\n",
      "\n",
      "  [[ 0.74117647  0.62352941  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.40784314]\n",
      "   [ 0.74509804  0.63137255  0.4       ]\n",
      "   ..., \n",
      "   [ 0.80392157  0.74509804  0.68627451]\n",
      "   [ 0.78431373  0.73333333  0.69019608]\n",
      "   [ 0.77254902  0.71764706  0.67843137]]\n",
      "\n",
      "  [[ 0.72156863  0.60392157  0.39215686]\n",
      "   [ 0.72156863  0.61176471  0.39215686]\n",
      "   [ 0.72156863  0.60784314  0.38039216]\n",
      "   ..., \n",
      "   [ 0.78039216  0.71372549  0.64705882]\n",
      "   [ 0.76862745  0.70588235  0.65490196]\n",
      "   [ 0.75686275  0.69411765  0.65098039]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.1372549   0.09803922  0.10196078]\n",
      "   [ 0.10588235  0.08235294  0.08235294]\n",
      "   [ 0.09803922  0.07843137  0.0745098 ]\n",
      "   ..., \n",
      "   [ 0.51764706  0.50588235  0.50588235]\n",
      "   [ 0.52156863  0.4745098   0.45490196]\n",
      "   [ 0.49411765  0.45098039  0.44313725]]\n",
      "\n",
      "  [[ 0.24705882  0.21568627  0.19607843]\n",
      "   [ 0.1254902   0.10588235  0.08235294]\n",
      "   [ 0.06666667  0.05098039  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4         0.37254902  0.34509804]\n",
      "   [ 0.41176471  0.34901961  0.29803922]\n",
      "   [ 0.39215686  0.3372549   0.30196078]]\n",
      "\n",
      "  [[ 0.38823529  0.35686275  0.32941176]\n",
      "   [ 0.19215686  0.17647059  0.14509804]\n",
      "   [ 0.05882353  0.04705882  0.01960784]\n",
      "   ..., \n",
      "   [ 0.18039216  0.16862745  0.15294118]\n",
      "   [ 0.20392157  0.16078431  0.13333333]\n",
      "   [ 0.20392157  0.17254902  0.16078431]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.65098039  0.64705882  0.67058824]\n",
      "   [ 0.64313725  0.63921569  0.65098039]\n",
      "   [ 0.64313725  0.64313725  0.64705882]\n",
      "   ..., \n",
      "   [ 0.67843137  0.6745098   0.66666667]\n",
      "   [ 0.66666667  0.66666667  0.65882353]\n",
      "   [ 0.65490196  0.65490196  0.65490196]]\n",
      "\n",
      "  [[ 0.6627451   0.65882353  0.69019608]\n",
      "   [ 0.6627451   0.65882353  0.67843137]\n",
      "   [ 0.65882353  0.65882353  0.67058824]\n",
      "   ..., \n",
      "   [ 0.6745098   0.67058824  0.66666667]\n",
      "   [ 0.65882353  0.65490196  0.65490196]\n",
      "   [ 0.64705882  0.64705882  0.65098039]]\n",
      "\n",
      "  [[ 0.67843137  0.6745098   0.70196078]\n",
      "   [ 0.68627451  0.68235294  0.69803922]\n",
      "   [ 0.67843137  0.67843137  0.68627451]\n",
      "   ..., \n",
      "   [ 0.66666667  0.65882353  0.6627451 ]\n",
      "   [ 0.65882353  0.65490196  0.65882353]\n",
      "   [ 0.65098039  0.65098039  0.65882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07058824  0.05098039  0.03921569]\n",
      "   ..., \n",
      "   [ 0.07843137  0.0627451   0.0627451 ]\n",
      "   [ 0.08235294  0.0627451   0.05490196]\n",
      "   [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      "  [[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07058824  0.05098039  0.03921569]\n",
      "   ..., \n",
      "   [ 0.07843137  0.0627451   0.05882353]\n",
      "   [ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      "  [[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07058824  0.05098039  0.03921569]\n",
      "   ..., \n",
      "   [ 0.07843137  0.0627451   0.05490196]\n",
      "   [ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.25882353  0.21176471  0.16078431]\n",
      "   [ 0.31372549  0.2627451   0.20784314]\n",
      "   [ 0.18431373  0.1372549   0.0745098 ]\n",
      "   ..., \n",
      "   [ 0.5254902   0.5254902   0.39215686]\n",
      "   [ 0.43137255  0.44313725  0.30196078]\n",
      "   [ 0.38431373  0.4         0.25882353]]\n",
      "\n",
      "  [[ 0.23529412  0.18823529  0.12941176]\n",
      "   [ 0.21568627  0.16862745  0.10588235]\n",
      "   [ 0.19607843  0.14901961  0.08627451]\n",
      "   ..., \n",
      "   [ 0.48235294  0.49019608  0.3254902 ]\n",
      "   [ 0.30980392  0.31764706  0.16470588]\n",
      "   [ 0.28235294  0.29019608  0.14901961]]\n",
      "\n",
      "  [[ 0.25098039  0.21176471  0.14901961]\n",
      "   [ 0.21568627  0.17647059  0.11372549]\n",
      "   [ 0.18823529  0.14901961  0.08235294]\n",
      "   ..., \n",
      "   [ 0.60784314  0.61568627  0.43529412]\n",
      "   [ 0.53333333  0.5372549   0.38039216]\n",
      "   [ 0.34509804  0.34901961  0.2       ]]]\n",
      "\n",
      "\n",
      " [[[ 0.45490196  0.40392157  0.21960784]\n",
      "   [ 0.45098039  0.41176471  0.23137255]\n",
      "   [ 0.60784314  0.50196078  0.32156863]\n",
      "   ..., \n",
      "   [ 0.68627451  0.51764706  0.30196078]\n",
      "   [ 0.6627451   0.52156863  0.28235294]\n",
      "   [ 0.55686275  0.46666667  0.20784314]]\n",
      "\n",
      "  [[ 0.45490196  0.4         0.22745098]\n",
      "   [ 0.47843137  0.42352941  0.25490196]\n",
      "   [ 0.6         0.4745098   0.30980392]\n",
      "   ..., \n",
      "   [ 0.58823529  0.43529412  0.22352941]\n",
      "   [ 0.56862745  0.4745098   0.23529412]\n",
      "   [ 0.52156863  0.48235294  0.21176471]]\n",
      "\n",
      "  [[ 0.37254902  0.3372549   0.16078431]\n",
      "   [ 0.38431373  0.32941176  0.17254902]\n",
      "   [ 0.55294118  0.41568627  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.56862745  0.43921569  0.22745098]\n",
      "   [ 0.49411765  0.43529412  0.2       ]\n",
      "   [ 0.49803922  0.49019608  0.24313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.30196078  0.24705882  0.11372549]\n",
      "   [ 0.34509804  0.28235294  0.14509804]\n",
      "   [ 0.2745098   0.23137255  0.10588235]\n",
      "   ..., \n",
      "   [ 0.18823529  0.15294118  0.07843137]\n",
      "   [ 0.45490196  0.42352941  0.32941176]\n",
      "   [ 0.62352941  0.55686275  0.47843137]]\n",
      "\n",
      "  [[ 0.21568627  0.14509804  0.0627451 ]\n",
      "   [ 0.25490196  0.18039216  0.09411765]\n",
      "   [ 0.26666667  0.20784314  0.11764706]\n",
      "   ..., \n",
      "   [ 0.16470588  0.11764706  0.05098039]\n",
      "   [ 0.49411765  0.44705882  0.35294118]\n",
      "   [ 0.62745098  0.57647059  0.49019608]]\n",
      "\n",
      "  [[ 0.30588235  0.22352941  0.14509804]\n",
      "   [ 0.28235294  0.19607843  0.11764706]\n",
      "   [ 0.2627451   0.19607843  0.1254902 ]\n",
      "   ..., \n",
      "   [ 0.20392157  0.14509804  0.07058824]\n",
      "   [ 0.48627451  0.43137255  0.32941176]\n",
      "   [ 0.60784314  0.56470588  0.48627451]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.41568627  0.3372549   0.61176471]\n",
      "   [ 0.48235294  0.4         0.50588235]\n",
      "   [ 0.43529412  0.33333333  0.49019608]\n",
      "   ..., \n",
      "   [ 0.51372549  0.4627451   0.61568627]\n",
      "   [ 0.42745098  0.39215686  0.55686275]\n",
      "   [ 0.55294118  0.52941176  0.61960784]]\n",
      "\n",
      "  [[ 0.60784314  0.5254902   0.63529412]\n",
      "   [ 0.6745098   0.57254902  0.44313725]\n",
      "   [ 0.51372549  0.43529412  0.48627451]\n",
      "   ..., \n",
      "   [ 0.51764706  0.46666667  0.60784314]\n",
      "   [ 0.39215686  0.34901961  0.50980392]\n",
      "   [ 0.56862745  0.5372549   0.63529412]]\n",
      "\n",
      "  [[ 0.63529412  0.5254902   0.68235294]\n",
      "   [ 0.62745098  0.49803922  0.43137255]\n",
      "   [ 0.44313725  0.34509804  0.5254902 ]\n",
      "   ..., \n",
      "   [ 0.73333333  0.71372549  0.65490196]\n",
      "   [ 0.69411765  0.6745098   0.65882353]\n",
      "   [ 0.78823529  0.77254902  0.71764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.35686275  0.03921569  0.5254902 ]\n",
      "   [ 0.34509804  0.03529412  0.49803922]\n",
      "   [ 0.34509804  0.05098039  0.49803922]\n",
      "   ..., \n",
      "   [ 0.40784314  0.24705882  0.49411765]\n",
      "   [ 0.43529412  0.28235294  0.52156863]\n",
      "   [ 0.43921569  0.25490196  0.53333333]]\n",
      "\n",
      "  [[ 0.29803922  0.02352941  0.50196078]\n",
      "   [ 0.30588235  0.01568627  0.47058824]\n",
      "   [ 0.35686275  0.08235294  0.51764706]\n",
      "   ..., \n",
      "   [ 0.42745098  0.27058824  0.49803922]\n",
      "   [ 0.41960784  0.26666667  0.50980392]\n",
      "   [ 0.44705882  0.24705882  0.54509804]]\n",
      "\n",
      "  [[ 0.2627451   0.03137255  0.50196078]\n",
      "   [ 0.26666667  0.02352941  0.47058824]\n",
      "   [ 0.30980392  0.07058824  0.49803922]\n",
      "   ..., \n",
      "   [ 0.45882353  0.28627451  0.5254902 ]\n",
      "   [ 0.43137255  0.25098039  0.52156863]\n",
      "   [ 0.38823529  0.16470588  0.49019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.96862745  0.96470588  0.97254902]\n",
      "   [ 0.96078431  0.97647059  0.98823529]\n",
      "   [ 0.95686275  0.97254902  0.97647059]\n",
      "   ..., \n",
      "   [ 0.45882353  0.38039216  0.32941176]\n",
      "   [ 0.49803922  0.41960784  0.37647059]\n",
      "   [ 0.61960784  0.56470588  0.54117647]]\n",
      "\n",
      "  [[ 0.95294118  0.95686275  0.96862745]\n",
      "   [ 0.95294118  0.96862745  0.97647059]\n",
      "   [ 0.95294118  0.96078431  0.95686275]\n",
      "   ..., \n",
      "   [ 0.44313725  0.35686275  0.29803922]\n",
      "   [ 0.47843137  0.4         0.34509804]\n",
      "   [ 0.63137255  0.56862745  0.52941176]]\n",
      "\n",
      "  [[ 0.95686275  0.96078431  0.97647059]\n",
      "   [ 0.96078431  0.97647059  0.98039216]\n",
      "   [ 0.96862745  0.96862745  0.95686275]\n",
      "   ..., \n",
      "   [ 0.52941176  0.42745098  0.36470588]\n",
      "   [ 0.4745098   0.37647059  0.31764706]\n",
      "   [ 0.50588235  0.43137255  0.38823529]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.71372549  0.6627451   0.63529412]\n",
      "   [ 0.77647059  0.7254902   0.69803922]\n",
      "   [ 0.85490196  0.80392157  0.77647059]\n",
      "   ..., \n",
      "   [ 0.61568627  0.54901961  0.44313725]\n",
      "   [ 0.36862745  0.30588235  0.21960784]\n",
      "   [ 0.50980392  0.45882353  0.41176471]]\n",
      "\n",
      "  [[ 0.54509804  0.4627451   0.41960784]\n",
      "   [ 0.49411765  0.40392157  0.35686275]\n",
      "   [ 0.5254902   0.43529412  0.38431373]\n",
      "   ..., \n",
      "   [ 0.61960784  0.55686275  0.43921569]\n",
      "   [ 0.46666667  0.40784314  0.31372549]\n",
      "   [ 0.45490196  0.40784314  0.35294118]]\n",
      "\n",
      "  [[ 0.76862745  0.70196078  0.62745098]\n",
      "   [ 0.7254902   0.63921569  0.54509804]\n",
      "   [ 0.69019608  0.58823529  0.48235294]\n",
      "   ..., \n",
      "   [ 0.59607843  0.54901961  0.42352941]\n",
      "   [ 0.69411765  0.64313725  0.5372549 ]\n",
      "   [ 0.63529412  0.59607843  0.52156863]]]\n",
      "\n",
      "\n",
      " [[[ 0.74117647  0.89803922  0.94117647]\n",
      "   [ 0.76470588  0.90980392  0.94901961]\n",
      "   [ 0.79607843  0.93333333  0.96470588]\n",
      "   ..., \n",
      "   [ 0.65882353  0.75686275  0.81176471]\n",
      "   [ 0.65882353  0.74901961  0.79215686]\n",
      "   [ 0.65882353  0.7372549   0.78039216]]\n",
      "\n",
      "  [[ 0.79607843  0.9372549   0.96470588]\n",
      "   [ 0.83137255  0.96470588  0.98431373]\n",
      "   [ 0.82352941  0.94901961  0.96862745]\n",
      "   ..., \n",
      "   [ 0.58431373  0.67843137  0.76470588]\n",
      "   [ 0.59215686  0.6745098   0.76470588]\n",
      "   [ 0.60784314  0.67843137  0.77254902]]\n",
      "\n",
      "  [[ 0.83529412  0.96862745  0.98431373]\n",
      "   [ 0.81568627  0.94509804  0.96078431]\n",
      "   [ 0.82745098  0.94509804  0.95686275]\n",
      "   ..., \n",
      "   [ 0.58431373  0.6745098   0.76862745]\n",
      "   [ 0.57647059  0.65490196  0.76470588]\n",
      "   [ 0.56470588  0.63137255  0.75686275]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.31764706  0.35294118]\n",
      "   [ 0.21176471  0.32156863  0.47843137]\n",
      "   [ 0.15294118  0.40784314  0.62352941]\n",
      "   ..., \n",
      "   [ 0.18039216  0.15294118  0.18039216]\n",
      "   [ 0.19607843  0.16862745  0.2       ]\n",
      "   [ 0.27058824  0.24705882  0.25098039]]\n",
      "\n",
      "  [[ 0.25490196  0.2627451   0.28627451]\n",
      "   [ 0.16470588  0.22745098  0.35686275]\n",
      "   [ 0.17647059  0.36078431  0.50980392]\n",
      "   ..., \n",
      "   [ 0.14509804  0.08627451  0.17647059]\n",
      "   [ 0.17254902  0.10588235  0.18431373]\n",
      "   [ 0.22352941  0.17254902  0.2       ]]\n",
      "\n",
      "  [[ 0.20784314  0.22745098  0.24705882]\n",
      "   [ 0.15294118  0.2         0.25490196]\n",
      "   [ 0.24705882  0.30588235  0.36862745]\n",
      "   ..., \n",
      "   [ 0.18039216  0.10196078  0.19215686]\n",
      "   [ 0.23137255  0.14901961  0.18431373]\n",
      "   [ 0.29411765  0.23921569  0.2       ]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.10196078  0.09019608  0.1254902 ]\n",
      "   [ 0.06666667  0.05490196  0.09803922]\n",
      "   [ 0.05098039  0.03529412  0.09411765]\n",
      "   ..., \n",
      "   [ 0.05882353  0.05490196  0.10980392]\n",
      "   [ 0.09411765  0.09411765  0.14509804]\n",
      "   [ 0.08627451  0.08235294  0.13333333]]\n",
      "\n",
      "  [[ 0.07843137  0.06666667  0.10196078]\n",
      "   [ 0.05098039  0.03921569  0.08627451]\n",
      "   [ 0.05098039  0.03529412  0.09411765]\n",
      "   ..., \n",
      "   [ 0.0745098   0.06666667  0.1372549 ]\n",
      "   [ 0.08235294  0.07843137  0.1372549 ]\n",
      "   [ 0.11372549  0.11372549  0.15294118]]\n",
      "\n",
      "  [[ 0.05490196  0.04313725  0.07843137]\n",
      "   [ 0.05098039  0.03921569  0.08235294]\n",
      "   [ 0.05098039  0.03529412  0.09019608]\n",
      "   ..., \n",
      "   [ 0.06666667  0.0627451   0.1254902 ]\n",
      "   [ 0.09803922  0.09411765  0.14901961]\n",
      "   [ 0.12156863  0.12156863  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.35294118  0.42745098  0.5372549 ]\n",
      "   [ 0.13333333  0.25098039  0.37254902]\n",
      "   [ 0.10980392  0.21176471  0.35294118]\n",
      "   ..., \n",
      "   [ 0.09019608  0.07843137  0.14509804]\n",
      "   [ 0.0627451   0.05098039  0.11764706]\n",
      "   [ 0.03529412  0.02352941  0.09019608]]\n",
      "\n",
      "  [[ 0.30980392  0.41176471  0.55294118]\n",
      "   [ 0.22745098  0.37647059  0.54509804]\n",
      "   [ 0.1254902   0.26666667  0.43137255]\n",
      "   ..., \n",
      "   [ 0.05490196  0.04313725  0.10980392]\n",
      "   [ 0.0627451   0.05098039  0.11764706]\n",
      "   [ 0.03921569  0.02745098  0.09411765]]\n",
      "\n",
      "  [[ 0.50196078  0.61568627  0.76862745]\n",
      "   [ 0.22745098  0.36470588  0.58431373]\n",
      "   [ 0.09803922  0.23529412  0.41568627]\n",
      "   ..., \n",
      "   [ 0.05098039  0.03921569  0.10588235]\n",
      "   [ 0.04705882  0.03529412  0.10196078]\n",
      "   [ 0.05098039  0.03921569  0.10588235]]]\n",
      "\n",
      "\n",
      " [[[ 0.36862745  0.3372549   0.22745098]\n",
      "   [ 0.39607843  0.35686275  0.23921569]\n",
      "   [ 0.37254902  0.33333333  0.21176471]\n",
      "   ..., \n",
      "   [ 0.56862745  0.54117647  0.41568627]\n",
      "   [ 0.56862745  0.54901961  0.42352941]\n",
      "   [ 0.4745098   0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.34901961  0.32941176  0.21568627]\n",
      "   [ 0.38039216  0.34901961  0.23137255]\n",
      "   [ 0.39607843  0.35686275  0.23529412]\n",
      "   ..., \n",
      "   [ 0.57254902  0.5372549   0.41568627]\n",
      "   [ 0.57254902  0.54509804  0.41960784]\n",
      "   [ 0.47843137  0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.3372549   0.32941176  0.21176471]\n",
      "   [ 0.36862745  0.34509804  0.22352941]\n",
      "   [ 0.41960784  0.38431373  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.57254902  0.53333333  0.41568627]\n",
      "   [ 0.57647059  0.54117647  0.41960784]\n",
      "   [ 0.48235294  0.45490196  0.35294118]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.80392157  0.80392157  0.8       ]\n",
      "   [ 0.81568627  0.81568627  0.81176471]\n",
      "   [ 0.78823529  0.78823529  0.78431373]\n",
      "   ..., \n",
      "   [ 0.56862745  0.58431373  0.58431373]\n",
      "   [ 0.58431373  0.59607843  0.61568627]\n",
      "   [ 0.49019608  0.50196078  0.52941176]]\n",
      "\n",
      "  [[ 0.78823529  0.78823529  0.78823529]\n",
      "   [ 0.80392157  0.80392157  0.80392157]\n",
      "   [ 0.77647059  0.77647059  0.77647059]\n",
      "   ..., \n",
      "   [ 0.60392157  0.61960784  0.6627451 ]\n",
      "   [ 0.61960784  0.63529412  0.69411765]\n",
      "   [ 0.5254902   0.54117647  0.6       ]]\n",
      "\n",
      "  [[ 0.74509804  0.74509804  0.74117647]\n",
      "   [ 0.7372549   0.7372549   0.73333333]\n",
      "   [ 0.68627451  0.68627451  0.67843137]\n",
      "   ..., \n",
      "   [ 0.63529412  0.64313725  0.71372549]\n",
      "   [ 0.63921569  0.65098039  0.72156863]\n",
      "   [ 0.52941176  0.5372549   0.60784314]]]\n",
      "\n",
      "\n",
      " [[[ 0.71764706  0.72941176  0.69803922]\n",
      "   [ 0.61960784  0.65490196  0.59607843]\n",
      "   [ 0.65098039  0.6745098   0.62745098]\n",
      "   ..., \n",
      "   [ 0.55686275  0.57647059  0.56470588]\n",
      "   [ 0.5372549   0.57254902  0.55294118]\n",
      "   [ 0.56470588  0.58823529  0.57254902]]\n",
      "\n",
      "  [[ 0.49019608  0.5254902   0.47058824]\n",
      "   [ 0.38039216  0.43529412  0.35294118]\n",
      "   [ 0.39215686  0.43921569  0.37254902]\n",
      "   ..., \n",
      "   [ 0.24705882  0.29019608  0.26666667]\n",
      "   [ 0.23137255  0.29019608  0.2627451 ]\n",
      "   [ 0.2627451   0.30980392  0.28235294]]\n",
      "\n",
      "  [[ 0.41960784  0.4745098   0.40392157]\n",
      "   [ 0.33333333  0.40784314  0.30980392]\n",
      "   [ 0.34509804  0.40784314  0.3254902 ]\n",
      "   ..., \n",
      "   [ 0.24705882  0.30980392  0.27843137]\n",
      "   [ 0.20784314  0.28627451  0.25098039]\n",
      "   [ 0.23529412  0.30196078  0.27058824]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.42352941  0.45490196  0.40392157]\n",
      "   [ 0.38431373  0.41960784  0.34117647]\n",
      "   [ 0.46666667  0.50980392  0.41176471]\n",
      "   ..., \n",
      "   [ 0.44705882  0.49019608  0.44313725]\n",
      "   [ 0.40784314  0.45098039  0.40392157]\n",
      "   [ 0.35686275  0.39607843  0.35294118]]\n",
      "\n",
      "  [[ 0.78039216  0.80392157  0.77254902]\n",
      "   [ 0.77254902  0.79607843  0.74509804]\n",
      "   [ 0.81176471  0.83921569  0.76862745]\n",
      "   ..., \n",
      "   [ 0.79607843  0.81960784  0.79215686]\n",
      "   [ 0.78431373  0.80392157  0.78039216]\n",
      "   [ 0.74117647  0.76078431  0.7372549 ]]\n",
      "\n",
      "  [[ 0.98431373  1.          0.98823529]\n",
      "   [ 0.97254902  0.98823529  0.96470588]\n",
      "   [ 0.97647059  0.99215686  0.95686275]\n",
      "   ..., \n",
      "   [ 0.98039216  0.98431373  0.98039216]\n",
      "   [ 0.98039216  0.98039216  0.98039216]\n",
      "   [ 0.98039216  0.98431373  0.98039216]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.6745098   0.67843137  0.59607843]\n",
      "   [ 0.67058824  0.6745098   0.59607843]\n",
      "   [ 0.69803922  0.69803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.41960784  0.41176471  0.30196078]\n",
      "   [ 0.41568627  0.40392157  0.30980392]\n",
      "   [ 0.4         0.38823529  0.30588235]]\n",
      "\n",
      "  [[ 0.64705882  0.63921569  0.56078431]\n",
      "   [ 0.62352941  0.61568627  0.54117647]\n",
      "   [ 0.70588235  0.69803922  0.62352941]\n",
      "   ..., \n",
      "   [ 0.45882353  0.43921569  0.3254902 ]\n",
      "   [ 0.45882353  0.43921569  0.3372549 ]\n",
      "   [ 0.43529412  0.41568627  0.3254902 ]]\n",
      "\n",
      "  [[ 0.68235294  0.6627451   0.58823529]\n",
      "   [ 0.61176471  0.59215686  0.51764706]\n",
      "   [ 0.68235294  0.6627451   0.58823529]\n",
      "   ..., \n",
      "   [ 0.47058824  0.44705882  0.3254902 ]\n",
      "   [ 0.4745098   0.44705882  0.3372549 ]\n",
      "   [ 0.46666667  0.43921569  0.34509804]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.47843137  0.45882353  0.36862745]\n",
      "   [ 0.47058824  0.45098039  0.36470588]\n",
      "   [ 0.45490196  0.43921569  0.34901961]\n",
      "   ..., \n",
      "   [ 0.48627451  0.49803922  0.39215686]\n",
      "   [ 0.4745098   0.49411765  0.39215686]\n",
      "   [ 0.45882353  0.47843137  0.38039216]]\n",
      "\n",
      "  [[ 0.43529412  0.41176471  0.30588235]\n",
      "   [ 0.43921569  0.41960784  0.3254902 ]\n",
      "   [ 0.48235294  0.48627451  0.39607843]\n",
      "   ..., \n",
      "   [ 0.47843137  0.45490196  0.32941176]\n",
      "   [ 0.45882353  0.4627451   0.35294118]\n",
      "   [ 0.44313725  0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.43921569  0.41176471  0.30196078]\n",
      "   [ 0.45098039  0.42352941  0.32941176]\n",
      "   [ 0.4627451   0.47058824  0.37647059]\n",
      "   ..., \n",
      "   [ 0.48627451  0.45098039  0.31372549]\n",
      "   [ 0.43137255  0.43137255  0.31764706]\n",
      "   [ 0.4         0.41568627  0.30980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.37254902  0.34509804  0.2       ]\n",
      "   [ 0.36078431  0.34509804  0.19215686]\n",
      "   [ 0.35686275  0.34117647  0.19607843]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20392157  0.11764706]\n",
      "   [ 0.20392157  0.2         0.11764706]\n",
      "   [ 0.21176471  0.21176471  0.1254902 ]]\n",
      "\n",
      "  [[ 0.4         0.36862745  0.20784314]\n",
      "   [ 0.41568627  0.38039216  0.2       ]\n",
      "   [ 0.41960784  0.38431373  0.21176471]\n",
      "   ..., \n",
      "   [ 0.20392157  0.20392157  0.1254902 ]\n",
      "   [ 0.19215686  0.18823529  0.10588235]\n",
      "   [ 0.23137255  0.22745098  0.1254902 ]]\n",
      "\n",
      "  [[ 0.41960784  0.38431373  0.20392157]\n",
      "   [ 0.42745098  0.38823529  0.20392157]\n",
      "   [ 0.41176471  0.38039216  0.20392157]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20784314  0.1254902 ]\n",
      "   [ 0.21568627  0.20784314  0.11764706]\n",
      "   [ 0.24313725  0.23529412  0.1254902 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.37647059  0.36078431  0.23529412]\n",
      "   [ 0.24313725  0.23921569  0.15686275]\n",
      "   [ 0.20784314  0.21176471  0.14117647]\n",
      "   ..., \n",
      "   [ 0.19607843  0.19607843  0.12941176]\n",
      "   [ 0.21568627  0.21568627  0.14117647]\n",
      "   [ 0.23529412  0.22745098  0.15294118]]\n",
      "\n",
      "  [[ 0.31372549  0.29803922  0.2       ]\n",
      "   [ 0.23137255  0.22745098  0.15294118]\n",
      "   [ 0.19607843  0.2         0.12941176]\n",
      "   ..., \n",
      "   [ 0.21960784  0.20392157  0.13333333]\n",
      "   [ 0.25098039  0.23529412  0.15294118]\n",
      "   [ 0.25098039  0.23529412  0.14901961]]\n",
      "\n",
      "  [[ 0.18431373  0.18039216  0.1254902 ]\n",
      "   [ 0.15294118  0.15686275  0.10980392]\n",
      "   [ 0.15294118  0.15686275  0.09803922]\n",
      "   ..., \n",
      "   [ 0.30980392  0.29019608  0.17647059]\n",
      "   [ 0.25098039  0.23137255  0.14901961]\n",
      "   [ 0.23921569  0.22352941  0.14901961]]]\n",
      "\n",
      "\n",
      " [[[ 0.60784314  0.40392157  0.43137255]\n",
      "   [ 0.59607843  0.39215686  0.41960784]\n",
      "   [ 0.60784314  0.40392157  0.43137255]\n",
      "   ..., \n",
      "   [ 0.23137255  0.14901961  0.17254902]\n",
      "   [ 0.22352941  0.15686275  0.18039216]\n",
      "   [ 0.22352941  0.16078431  0.19607843]]\n",
      "\n",
      "  [[ 0.60784314  0.40784314  0.43529412]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   [ 0.6         0.4         0.42745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.18823529  0.20784314]\n",
      "   [ 0.21960784  0.15294118  0.17647059]\n",
      "   [ 0.21960784  0.16078431  0.19215686]]\n",
      "\n",
      "  [[ 0.61176471  0.41176471  0.43921569]\n",
      "   [ 0.58823529  0.38823529  0.41568627]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   ..., \n",
      "   [ 0.41176471  0.30980392  0.3254902 ]\n",
      "   [ 0.23137255  0.16470588  0.18823529]\n",
      "   [ 0.2         0.14117647  0.17254902]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.45490196  0.43921569  0.5254902 ]\n",
      "   [ 0.44313725  0.43529412  0.50980392]\n",
      "   [ 0.44313725  0.44313725  0.50196078]\n",
      "   ..., \n",
      "   [ 0.42745098  0.38039216  0.44705882]\n",
      "   [ 0.28235294  0.23921569  0.29803922]\n",
      "   [ 0.42352941  0.37647059  0.43529412]]\n",
      "\n",
      "  [[ 0.45490196  0.44313725  0.52156863]\n",
      "   [ 0.44705882  0.43529412  0.50980392]\n",
      "   [ 0.45098039  0.43921569  0.51372549]\n",
      "   ..., \n",
      "   [ 0.43137255  0.41176471  0.48627451]\n",
      "   [ 0.23137255  0.19607843  0.26666667]\n",
      "   [ 0.29411765  0.23921569  0.30980392]]\n",
      "\n",
      "  [[ 0.46666667  0.45490196  0.52941176]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   ..., \n",
      "   [ 0.47058824  0.45882353  0.5372549 ]\n",
      "   [ 0.39607843  0.37254902  0.44705882]\n",
      "   [ 0.24705882  0.19607843  0.26666667]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.69803922  0.69019608  0.74117647]\n",
      "   [ 0.69803922  0.69019608  0.74117647]\n",
      "   [ 0.69803922  0.69019608  0.74117647]\n",
      "   ..., \n",
      "   [ 0.66666667  0.65882353  0.70588235]\n",
      "   [ 0.65882353  0.65098039  0.69411765]\n",
      "   [ 0.64705882  0.63921569  0.68235294]]\n",
      "\n",
      "  [[ 0.70588235  0.69803922  0.74901961]\n",
      "   [ 0.70196078  0.69411765  0.74509804]\n",
      "   [ 0.70588235  0.69803922  0.74901961]\n",
      "   ..., \n",
      "   [ 0.67843137  0.67058824  0.71372549]\n",
      "   [ 0.67058824  0.6627451   0.70588235]\n",
      "   [ 0.65882353  0.65098039  0.69411765]]\n",
      "\n",
      "  [[ 0.69411765  0.68627451  0.7372549 ]\n",
      "   [ 0.69411765  0.68627451  0.7372549 ]\n",
      "   [ 0.69803922  0.69019608  0.74117647]\n",
      "   ..., \n",
      "   [ 0.67058824  0.6627451   0.70588235]\n",
      "   [ 0.6627451   0.65490196  0.69803922]\n",
      "   [ 0.65490196  0.64705882  0.69019608]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.43921569  0.41960784  0.41960784]\n",
      "   [ 0.44313725  0.42745098  0.42352941]\n",
      "   [ 0.44705882  0.43137255  0.43137255]\n",
      "   ..., \n",
      "   [ 0.39215686  0.38039216  0.36862745]\n",
      "   [ 0.38431373  0.36862745  0.36470588]\n",
      "   [ 0.39607843  0.37254902  0.37254902]]\n",
      "\n",
      "  [[ 0.43921569  0.4         0.39607843]\n",
      "   [ 0.43921569  0.40392157  0.4       ]\n",
      "   [ 0.44313725  0.40392157  0.40392157]\n",
      "   ..., \n",
      "   [ 0.4         0.37254902  0.36470588]\n",
      "   [ 0.4         0.36470588  0.35686275]\n",
      "   [ 0.4         0.36078431  0.35686275]]\n",
      "\n",
      "  [[ 0.40392157  0.37647059  0.36078431]\n",
      "   [ 0.39215686  0.36470588  0.35294118]\n",
      "   [ 0.40392157  0.37254902  0.36862745]\n",
      "   ..., \n",
      "   [ 0.36078431  0.32941176  0.31372549]\n",
      "   [ 0.36470588  0.3372549   0.31372549]\n",
      "   [ 0.35686275  0.32941176  0.30196078]]]\n",
      "\n",
      "\n",
      " [[[ 0.11372549  0.16862745  0.03921569]\n",
      "   [ 0.08627451  0.14117647  0.01568627]\n",
      "   [ 0.09803922  0.14509804  0.0627451 ]\n",
      "   ..., \n",
      "   [ 0.77254902  0.85882353  0.5372549 ]\n",
      "   [ 0.77647059  0.85882353  0.5372549 ]\n",
      "   [ 0.78039216  0.87058824  0.54901961]]\n",
      "\n",
      "  [[ 0.12156863  0.18039216  0.03529412]\n",
      "   [ 0.10588235  0.16078431  0.02352941]\n",
      "   [ 0.06666667  0.11372549  0.02352941]\n",
      "   ..., \n",
      "   [ 0.82352941  0.90980392  0.58039216]\n",
      "   [ 0.81960784  0.90588235  0.58039216]\n",
      "   [ 0.81960784  0.90588235  0.58039216]]\n",
      "\n",
      "  [[ 0.15686275  0.21568627  0.0627451 ]\n",
      "   [ 0.12156863  0.17647059  0.03137255]\n",
      "   [ 0.07843137  0.12941176  0.02745098]\n",
      "   ..., \n",
      "   [ 0.82352941  0.90980392  0.58823529]\n",
      "   [ 0.82352941  0.90980392  0.58431373]\n",
      "   [ 0.82352941  0.90980392  0.58431373]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.17647059  0.14901961  0.09019608]\n",
      "   [ 0.09411765  0.08235294  0.04313725]\n",
      "   [ 0.0627451   0.05490196  0.02745098]\n",
      "   ..., \n",
      "   [ 0.09803922  0.11372549  0.1254902 ]\n",
      "   [ 0.09411765  0.10980392  0.12156863]\n",
      "   [ 0.09411765  0.10980392  0.12156863]]\n",
      "\n",
      "  [[ 0.08235294  0.07058824  0.02745098]\n",
      "   [ 0.07058824  0.05098039  0.01176471]\n",
      "   [ 0.10588235  0.0627451   0.01960784]\n",
      "   ..., \n",
      "   [ 0.10196078  0.11764706  0.12941176]\n",
      "   [ 0.11372549  0.12941176  0.14117647]\n",
      "   [ 0.10980392  0.1254902   0.1372549 ]]\n",
      "\n",
      "  [[ 0.20784314  0.15686275  0.09019608]\n",
      "   [ 0.31764706  0.24313725  0.14901961]\n",
      "   [ 0.38039216  0.2745098   0.16862745]\n",
      "   ..., \n",
      "   [ 0.08627451  0.10196078  0.11372549]\n",
      "   [ 0.09411765  0.10980392  0.12156863]\n",
      "   [ 0.09019608  0.10588235  0.11764706]]]\n",
      "\n",
      "\n",
      " [[[ 0.14117647  0.25490196  0.4       ]\n",
      "   [ 0.12941176  0.21568627  0.42352941]\n",
      "   [ 0.08235294  0.18431373  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.10196078  0.1254902   0.15686275]\n",
      "   [ 0.10196078  0.12156863  0.12156863]\n",
      "   [ 0.11372549  0.11372549  0.12156863]]\n",
      "\n",
      "  [[ 0.21568627  0.41960784  0.47058824]\n",
      "   [ 0.18431373  0.36862745  0.42352941]\n",
      "   [ 0.05882353  0.24705882  0.44313725]\n",
      "   ..., \n",
      "   [ 0.08627451  0.2         0.41176471]\n",
      "   [ 0.09019608  0.19215686  0.39215686]\n",
      "   [ 0.08235294  0.18039216  0.38039216]]\n",
      "\n",
      "  [[ 0.32156863  0.45490196  0.44705882]\n",
      "   [ 0.36862745  0.49803922  0.4       ]\n",
      "   [ 0.30980392  0.45882353  0.42352941]\n",
      "   ..., \n",
      "   [ 0.18431373  0.3254902   0.6       ]\n",
      "   [ 0.18431373  0.3372549   0.61176471]\n",
      "   [ 0.17647059  0.33333333  0.6       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.62352941  0.62745098  0.58039216]\n",
      "   [ 0.63529412  0.62352941  0.58431373]\n",
      "   [ 0.65098039  0.62745098  0.59215686]\n",
      "   ..., \n",
      "   [ 0.73333333  0.70588235  0.69411765]\n",
      "   [ 0.7254902   0.69019608  0.68235294]\n",
      "   [ 0.71764706  0.68235294  0.6745098 ]]\n",
      "\n",
      "  [[ 0.65098039  0.6627451   0.62745098]\n",
      "   [ 0.66666667  0.6627451   0.63137255]\n",
      "   [ 0.67843137  0.6627451   0.63529412]\n",
      "   ..., \n",
      "   [ 0.71764706  0.69019608  0.68235294]\n",
      "   [ 0.70980392  0.67843137  0.67058824]\n",
      "   [ 0.70588235  0.6745098   0.67058824]]\n",
      "\n",
      "  [[ 0.65882353  0.68235294  0.65098039]\n",
      "   [ 0.67058824  0.67843137  0.65490196]\n",
      "   [ 0.68627451  0.67843137  0.65882353]\n",
      "   ..., \n",
      "   [ 0.71372549  0.69019608  0.67843137]\n",
      "   [ 0.70980392  0.6745098   0.66666667]\n",
      "   [ 0.70588235  0.6745098   0.66666667]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.27058824  0.34509804  0.44705882]\n",
      "   [ 0.35294118  0.4745098   0.58823529]\n",
      "   [ 0.35294118  0.49803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.00784314  0.01176471  0.07058824]\n",
      "   [ 0.00784314  0.00784314  0.0627451 ]\n",
      "   [ 0.00784314  0.00784314  0.05882353]]\n",
      "\n",
      "  [[ 0.11372549  0.15294118  0.25098039]\n",
      "   [ 0.05882353  0.10588235  0.20784314]\n",
      "   [ 0.05098039  0.09803922  0.2       ]\n",
      "   ..., \n",
      "   [ 0.00392157  0.          0.00784314]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.00392157  0.00784314]]\n",
      "\n",
      "  [[ 0.01568627  0.01176471  0.01960784]\n",
      "   [ 0.01568627  0.00392157  0.00784314]\n",
      "   [ 0.01176471  0.00392157  0.00784314]\n",
      "   ..., \n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.00784314]\n",
      "   [ 0.          0.          0.01176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.00392157  0.00392157  0.01960784]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.        ]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03137255  0.04313725]\n",
      "   [ 0.00784314  0.00784314  0.01568627]\n",
      "   [ 0.00392157  0.00392157  0.01568627]]\n",
      "\n",
      "  [[ 0.03137255  0.0627451   0.12941176]\n",
      "   [ 0.00392157  0.01568627  0.05098039]\n",
      "   [ 0.          0.00392157  0.01176471]\n",
      "   ..., \n",
      "   [ 0.02745098  0.03921569  0.09019608]\n",
      "   [ 0.01176471  0.01568627  0.03921569]\n",
      "   [ 0.00392157  0.00392157  0.01176471]]\n",
      "\n",
      "  [[ 0.14509804  0.25098039  0.42352941]\n",
      "   [ 0.09411765  0.16862745  0.30196078]\n",
      "   [ 0.03921569  0.0745098   0.15294118]\n",
      "   ..., \n",
      "   [ 0.07843137  0.11764706  0.23921569]\n",
      "   [ 0.05098039  0.07843137  0.16862745]\n",
      "   [ 0.02352941  0.03921569  0.09019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.7254902   0.79215686  0.81176471]\n",
      "   [ 0.67843137  0.77647059  0.80392157]\n",
      "   [ 0.69019608  0.81176471  0.85098039]\n",
      "   ..., \n",
      "   [ 0.14509804  0.17647059  0.24313725]\n",
      "   [ 0.10196078  0.1372549   0.2       ]\n",
      "   [ 0.12941176  0.19607843  0.27058824]]\n",
      "\n",
      "  [[ 0.42745098  0.47058824  0.46666667]\n",
      "   [ 0.4627451   0.52941176  0.52941176]\n",
      "   [ 0.4745098   0.56078431  0.56470588]\n",
      "   ..., \n",
      "   [ 0.16862745  0.19607843  0.23529412]\n",
      "   [ 0.12941176  0.13333333  0.16862745]\n",
      "   [ 0.15686275  0.18823529  0.22745098]]\n",
      "\n",
      "  [[ 0.20392157  0.22745098  0.21568627]\n",
      "   [ 0.22745098  0.26666667  0.25098039]\n",
      "   [ 0.22745098  0.28235294  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15686275  0.18431373  0.22745098]\n",
      "   [ 0.18431373  0.22745098  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.63529412  0.62745098  0.6745098 ]\n",
      "   [ 0.61960784  0.61176471  0.65490196]\n",
      "   [ 0.62352941  0.61568627  0.65882353]\n",
      "   ..., \n",
      "   [ 0.05882353  0.05098039  0.09019608]\n",
      "   [ 0.05490196  0.04705882  0.09019608]\n",
      "   [ 0.07058824  0.0627451   0.10588235]]\n",
      "\n",
      "  [[ 0.61176471  0.60392157  0.65882353]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   ..., \n",
      "   [ 0.05098039  0.04313725  0.08235294]\n",
      "   [ 0.05882353  0.05098039  0.09411765]\n",
      "   [ 0.1254902   0.11764706  0.16078431]]\n",
      "\n",
      "  [[ 0.58431373  0.57647059  0.63137255]\n",
      "   [ 0.56470588  0.55686275  0.61176471]\n",
      "   [ 0.57254902  0.56470588  0.61960784]\n",
      "   ..., \n",
      "   [ 0.0627451   0.05490196  0.09411765]\n",
      "   [ 0.20392157  0.19607843  0.23921569]\n",
      "   [ 0.38431373  0.37647059  0.41960784]]]\n",
      "\n",
      "\n",
      " [[[ 0.76470588  0.71764706  0.67058824]\n",
      "   [ 0.75686275  0.70980392  0.6627451 ]\n",
      "   [ 0.76078431  0.71372549  0.66666667]\n",
      "   ..., \n",
      "   [ 0.22352941  0.22352941  0.22352941]\n",
      "   [ 0.20392157  0.20392157  0.20392157]\n",
      "   [ 0.03137255  0.03137255  0.03137255]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.67843137]\n",
      "   [ 0.76470588  0.71764706  0.6745098 ]\n",
      "   [ 0.77254902  0.7254902   0.68235294]\n",
      "   ..., \n",
      "   [ 0.34117647  0.34117647  0.34117647]\n",
      "   [ 0.31372549  0.31372549  0.31372549]\n",
      "   [ 0.03921569  0.03921569  0.03921569]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.68627451]\n",
      "   [ 0.76862745  0.71764706  0.68235294]\n",
      "   [ 0.77647059  0.7254902   0.69411765]\n",
      "   ..., \n",
      "   [ 0.43137255  0.43137255  0.43137255]\n",
      "   [ 0.41568627  0.41568627  0.41568627]\n",
      "   [ 0.04705882  0.04705882  0.04705882]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.78039216  0.7254902   0.69411765]\n",
      "   [ 0.76862745  0.72156863  0.68627451]\n",
      "   [ 0.78039216  0.74117647  0.69803922]\n",
      "   ..., \n",
      "   [ 0.13333333  0.10980392  0.08235294]\n",
      "   [ 0.10980392  0.09019608  0.07058824]\n",
      "   [ 0.08627451  0.0745098   0.0627451 ]]\n",
      "\n",
      "  [[ 0.77254902  0.7254902   0.69019608]\n",
      "   [ 0.76470588  0.71764706  0.68235294]\n",
      "   [ 0.77254902  0.72941176  0.69411765]\n",
      "   ..., \n",
      "   [ 0.15294118  0.12156863  0.08235294]\n",
      "   [ 0.14509804  0.12156863  0.08627451]\n",
      "   [ 0.1372549   0.11372549  0.08627451]]\n",
      "\n",
      "  [[ 0.75686275  0.71764706  0.68235294]\n",
      "   [ 0.75686275  0.70588235  0.6745098 ]\n",
      "   [ 0.76470588  0.70980392  0.67843137]\n",
      "   ..., \n",
      "   [ 0.16470588  0.12941176  0.08235294]\n",
      "   [ 0.16862745  0.12941176  0.09019608]\n",
      "   [ 0.16078431  0.1254902   0.09411765]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.          1.          0.99607843]\n",
      "   [ 0.98823529  0.98823529  0.98823529]\n",
      "   [ 0.99215686  0.98823529  0.99607843]\n",
      "   ..., \n",
      "   [ 0.64705882  0.69411765  0.72156863]\n",
      "   [ 0.95294118  0.96470588  0.96862745]\n",
      "   [ 0.99607843  0.99215686  0.98823529]]\n",
      "\n",
      "  [[ 1.          1.          0.99607843]\n",
      "   [ 0.98823529  0.98823529  0.98823529]\n",
      "   [ 0.99607843  0.99607843  1.        ]\n",
      "   ..., \n",
      "   [ 0.50980392  0.56470588  0.63137255]\n",
      "   [ 0.88235294  0.90980392  0.9372549 ]\n",
      "   [ 0.99215686  1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.97254902  0.96862745  0.97647059]\n",
      "   ..., \n",
      "   [ 0.55294118  0.60784314  0.68627451]\n",
      "   [ 0.8627451   0.89019608  0.92156863]\n",
      "   [ 0.99215686  1.          1.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.91372549  0.91764706  0.91764706]\n",
      "   [ 0.84705882  0.84705882  0.84705882]\n",
      "   [ 0.94509804  0.94509804  0.94509804]\n",
      "   ..., \n",
      "   [ 0.03529412  0.04313725  0.04313725]\n",
      "   [ 0.07058824  0.0745098   0.0745098 ]\n",
      "   [ 0.6627451   0.67058824  0.66666667]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.08235294  0.09019608  0.08627451]\n",
      "   [ 0.44313725  0.45098039  0.44705882]\n",
      "   [ 0.92156863  0.92941176  0.9254902 ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.98431373  0.98431373  0.98431373]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.6745098   0.68235294  0.67843137]\n",
      "   [ 0.90196078  0.90980392  0.90588235]\n",
      "   [ 0.96862745  0.97254902  0.97254902]]]\n",
      "\n",
      "\n",
      " [[[ 0.49803922  0.56862745  0.65490196]\n",
      "   [ 0.49411765  0.56470588  0.65098039]\n",
      "   [ 0.49803922  0.56862745  0.65490196]\n",
      "   ..., \n",
      "   [ 0.49019608  0.55686275  0.62352941]\n",
      "   [ 0.49019608  0.55686275  0.62352941]\n",
      "   [ 0.48627451  0.55294118  0.61960784]]\n",
      "\n",
      "  [[ 0.49019608  0.56078431  0.64313725]\n",
      "   [ 0.49019608  0.56078431  0.63921569]\n",
      "   [ 0.49411765  0.56470588  0.64313725]\n",
      "   ..., \n",
      "   [ 0.49019608  0.55686275  0.61568627]\n",
      "   [ 0.48627451  0.55686275  0.61176471]\n",
      "   [ 0.48627451  0.55294118  0.61176471]]\n",
      "\n",
      "  [[ 0.49411765  0.56862745  0.63529412]\n",
      "   [ 0.48627451  0.56078431  0.62745098]\n",
      "   [ 0.49411765  0.56862745  0.63529412]\n",
      "   ..., \n",
      "   [ 0.48627451  0.56078431  0.61176471]\n",
      "   [ 0.48235294  0.55294118  0.60784314]\n",
      "   [ 0.48235294  0.55294118  0.60784314]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.32941176  0.40784314  0.4627451 ]\n",
      "   [ 0.33333333  0.40784314  0.4627451 ]\n",
      "   [ 0.34117647  0.41568627  0.47058824]\n",
      "   ..., \n",
      "   [ 0.25490196  0.3254902   0.37254902]\n",
      "   [ 0.30980392  0.38039216  0.42745098]\n",
      "   [ 0.34509804  0.41568627  0.4627451 ]]\n",
      "\n",
      "  [[ 0.3372549   0.41176471  0.47058824]\n",
      "   [ 0.3254902   0.4         0.45490196]\n",
      "   [ 0.3254902   0.4         0.45490196]\n",
      "   ..., \n",
      "   [ 0.28627451  0.35686275  0.40392157]\n",
      "   [ 0.3254902   0.39607843  0.44313725]\n",
      "   [ 0.34117647  0.41176471  0.45882353]]\n",
      "\n",
      "  [[ 0.33333333  0.40784314  0.4627451 ]\n",
      "   [ 0.33333333  0.40392157  0.45882353]\n",
      "   [ 0.3254902   0.4         0.45490196]\n",
      "   ..., \n",
      "   [ 0.28235294  0.35294118  0.4       ]\n",
      "   [ 0.30588235  0.37647059  0.42352941]\n",
      "   [ 0.32156863  0.39215686  0.43921569]]]\n",
      "\n",
      "\n",
      " [[[ 0.45490196  0.27843137  0.10196078]\n",
      "   [ 0.25098039  0.13333333  0.03921569]\n",
      "   [ 0.0745098   0.02352941  0.00784314]\n",
      "   ..., \n",
      "   [ 0.58039216  0.32941176  0.14901961]\n",
      "   [ 0.6627451   0.37647059  0.18039216]\n",
      "   [ 0.7372549   0.44705882  0.23137255]]\n",
      "\n",
      "  [[ 0.44705882  0.26666667  0.08627451]\n",
      "   [ 0.25098039  0.1372549   0.04313725]\n",
      "   [ 0.07058824  0.02352941  0.00784314]\n",
      "   ..., \n",
      "   [ 0.58431373  0.32941176  0.16862745]\n",
      "   [ 0.63921569  0.36862745  0.17647059]\n",
      "   [ 0.7254902   0.44705882  0.23137255]]\n",
      "\n",
      "  [[ 0.44705882  0.25882353  0.09019608]\n",
      "   [ 0.24313725  0.13333333  0.04313725]\n",
      "   [ 0.06666667  0.02352941  0.00784314]\n",
      "   ..., \n",
      "   [ 0.61568627  0.35294118  0.18431373]\n",
      "   [ 0.68627451  0.4         0.2       ]\n",
      "   [ 0.7254902   0.44705882  0.22745098]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.94509804  0.94117647  0.91764706]\n",
      "   [ 0.95294118  0.94901961  0.9254902 ]\n",
      "   [ 0.94509804  0.94509804  0.9254902 ]\n",
      "   ..., \n",
      "   [ 0.12156863  0.07058824  0.01568627]\n",
      "   [ 0.1372549   0.07843137  0.01960784]\n",
      "   [ 0.15294118  0.07843137  0.01960784]]\n",
      "\n",
      "  [[ 0.84313725  0.82745098  0.78823529]\n",
      "   [ 0.90196078  0.89019608  0.8627451 ]\n",
      "   [ 0.92941176  0.92156863  0.90196078]\n",
      "   ..., \n",
      "   [ 0.09019608  0.05098039  0.01176471]\n",
      "   [ 0.09803922  0.05098039  0.00784314]\n",
      "   [ 0.10196078  0.05098039  0.01176471]]\n",
      "\n",
      "  [[ 0.47058824  0.42352941  0.37254902]\n",
      "   [ 0.54117647  0.49803922  0.45098039]\n",
      "   [ 0.60784314  0.57254902  0.5254902 ]\n",
      "   ..., \n",
      "   [ 0.17254902  0.09803922  0.02745098]\n",
      "   [ 0.16078431  0.08627451  0.02352941]\n",
      "   [ 0.14901961  0.0745098   0.01960784]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.23921569  0.28627451  0.29803922]\n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15294118  0.19215686  0.25882353]\n",
      "   ..., \n",
      "   [ 0.27843137  0.35294118  0.31372549]\n",
      "   [ 0.24313725  0.30588235  0.29411765]\n",
      "   [ 0.17647059  0.22745098  0.23921569]]\n",
      "\n",
      "  [[ 0.24705882  0.29411765  0.30196078]\n",
      "   [ 0.17647059  0.22745098  0.2627451 ]\n",
      "   [ 0.1254902   0.16862745  0.22745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.34901961  0.32156863]\n",
      "   [ 0.27843137  0.32941176  0.31372549]\n",
      "   [ 0.19607843  0.23529412  0.24313725]]\n",
      "\n",
      "  [[ 0.24705882  0.31372549  0.30196078]\n",
      "   [ 0.22352941  0.29411765  0.29411765]\n",
      "   [ 0.24705882  0.30980392  0.31764706]\n",
      "   ..., \n",
      "   [ 0.32156863  0.37647059  0.35686275]\n",
      "   [ 0.29803922  0.34509804  0.32156863]\n",
      "   [ 0.20784314  0.24705882  0.24313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.51372549  0.52156863  0.4       ]\n",
      "   [ 0.61176471  0.60392157  0.4627451 ]\n",
      "   [ 0.63137255  0.61568627  0.4745098 ]\n",
      "   ..., \n",
      "   [ 0.91764706  0.8745098   0.67843137]\n",
      "   [ 0.8627451   0.8         0.61176471]\n",
      "   [ 0.70980392  0.65098039  0.49411765]]\n",
      "\n",
      "  [[ 0.41176471  0.42352941  0.3254902 ]\n",
      "   [ 0.41960784  0.42352941  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.32941176]\n",
      "   ..., \n",
      "   [ 0.92156863  0.86666667  0.6745098 ]\n",
      "   [ 0.89803922  0.81568627  0.63137255]\n",
      "   [ 0.74901961  0.66666667  0.51372549]]\n",
      "\n",
      "  [[ 0.2627451   0.29019608  0.23921569]\n",
      "   [ 0.30588235  0.3254902   0.26666667]\n",
      "   [ 0.43921569  0.45098039  0.35294118]\n",
      "   ..., \n",
      "   [ 0.89019608  0.80784314  0.58823529]\n",
      "   [ 0.85098039  0.75294118  0.54509804]\n",
      "   [ 0.72156863  0.62745098  0.45882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.02352941  0.05882353]\n",
      "   [ 0.07843137  0.08627451  0.09019608]\n",
      "   ..., \n",
      "   [ 0.23137255  0.27843137  0.21568627]\n",
      "   [ 0.22352941  0.2745098   0.21568627]\n",
      "   [ 0.20784314  0.26666667  0.23137255]]\n",
      "\n",
      "  [[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.03529412  0.05882353]\n",
      "   [ 0.09803922  0.1254902   0.10980392]\n",
      "   ..., \n",
      "   [ 0.21568627  0.23921569  0.18823529]\n",
      "   [ 0.25882353  0.29803922  0.22745098]\n",
      "   [ 0.18823529  0.24705882  0.21176471]]\n",
      "\n",
      "  [[ 0.04705882  0.02352941  0.0627451 ]\n",
      "   [ 0.04313725  0.03921569  0.05882353]\n",
      "   [ 0.14901961  0.18431373  0.14901961]\n",
      "   ..., \n",
      "   [ 0.18431373  0.20392157  0.16470588]\n",
      "   [ 0.22352941  0.25882353  0.19215686]\n",
      "   [ 0.20392157  0.25098039  0.2       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.69411765  0.6627451   0.71764706]\n",
      "   [ 0.70588235  0.6627451   0.72156863]\n",
      "   [ 0.72156863  0.6745098   0.74509804]\n",
      "   ..., \n",
      "   [ 0.6745098   0.62352941  0.68235294]\n",
      "   [ 0.66666667  0.61568627  0.67058824]\n",
      "   [ 0.64313725  0.58823529  0.64705882]]\n",
      "\n",
      "  [[ 0.62352941  0.61568627  0.69019608]\n",
      "   [ 0.63529412  0.61568627  0.69411765]\n",
      "   [ 0.65490196  0.63529412  0.71764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.69803922  0.78431373]\n",
      "   [ 0.70980392  0.68627451  0.77254902]\n",
      "   [ 0.69803922  0.67843137  0.76470588]]\n",
      "\n",
      "  [[ 0.61960784  0.61960784  0.72941176]\n",
      "   [ 0.62352941  0.62352941  0.73333333]\n",
      "   [ 0.63921569  0.63921569  0.74509804]\n",
      "   ..., \n",
      "   [ 0.70980392  0.70588235  0.82745098]\n",
      "   [ 0.70196078  0.69411765  0.81568627]\n",
      "   [ 0.68627451  0.68235294  0.80392157]]]\n",
      "\n",
      "\n",
      " [[[ 0.68627451  0.75686275  0.89803922]\n",
      "   [ 0.6745098   0.75294118  0.9254902 ]\n",
      "   [ 0.67058824  0.75686275  0.94117647]\n",
      "   ..., \n",
      "   [ 0.75686275  0.80784314  0.93333333]\n",
      "   [ 0.76862745  0.80784314  0.90588235]\n",
      "   [ 0.76078431  0.79607843  0.89019608]]\n",
      "\n",
      "  [[ 0.65490196  0.73333333  0.88627451]\n",
      "   [ 0.64313725  0.73333333  0.90196078]\n",
      "   [ 0.64313725  0.7372549   0.90980392]\n",
      "   ..., \n",
      "   [ 0.70196078  0.75686275  0.88235294]\n",
      "   [ 0.69803922  0.74901961  0.85098039]\n",
      "   [ 0.69019608  0.73333333  0.83137255]]\n",
      "\n",
      "  [[ 0.65490196  0.72156863  0.86666667]\n",
      "   [ 0.65490196  0.72941176  0.87058824]\n",
      "   [ 0.67058824  0.72941176  0.85098039]\n",
      "   ..., \n",
      "   [ 0.69019608  0.74901961  0.87843137]\n",
      "   [ 0.68235294  0.74117647  0.85098039]\n",
      "   [ 0.67058824  0.72156863  0.82745098]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.33333333  0.32941176  0.39607843]\n",
      "   [ 0.33333333  0.32156863  0.36470588]\n",
      "   [ 0.36078431  0.32941176  0.32156863]\n",
      "   ..., \n",
      "   [ 0.4745098   0.44313725  0.47058824]\n",
      "   [ 0.41960784  0.40392157  0.46666667]\n",
      "   [ 0.45882353  0.43921569  0.50588235]]\n",
      "\n",
      "  [[ 0.33333333  0.34117647  0.4       ]\n",
      "   [ 0.32941176  0.31764706  0.34901961]\n",
      "   [ 0.34117647  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.30196078  0.28235294  0.32941176]\n",
      "   [ 0.43137255  0.40392157  0.47058824]\n",
      "   [ 0.44705882  0.41960784  0.48627451]]\n",
      "\n",
      "  [[ 0.32156863  0.32941176  0.37647059]\n",
      "   [ 0.29411765  0.29019608  0.32156863]\n",
      "   [ 0.22352941  0.19607843  0.21568627]\n",
      "   ..., \n",
      "   [ 0.30196078  0.26666667  0.30588235]\n",
      "   [ 0.35686275  0.30588235  0.35294118]\n",
      "   [ 0.35686275  0.30588235  0.35294118]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.54901961  0.49019608  0.45098039]\n",
      "   [ 0.57254902  0.50980392  0.47843137]\n",
      "   [ 0.56078431  0.49803922  0.47843137]\n",
      "   ..., \n",
      "   [ 0.66666667  0.56862745  0.51372549]\n",
      "   [ 0.69019608  0.58823529  0.5254902 ]\n",
      "   [ 0.66666667  0.57647059  0.52156863]]\n",
      "\n",
      "  [[ 0.4745098   0.42352941  0.50588235]\n",
      "   [ 0.50980392  0.4627451   0.54509804]\n",
      "   [ 0.5254902   0.4745098   0.56078431]\n",
      "   ..., \n",
      "   [ 0.63921569  0.55294118  0.61568627]\n",
      "   [ 0.66666667  0.57254902  0.63137255]\n",
      "   [ 0.66666667  0.58039216  0.63137255]]\n",
      "\n",
      "  [[ 0.59607843  0.54509804  0.68235294]\n",
      "   [ 0.61568627  0.56862745  0.70196078]\n",
      "   [ 0.60784314  0.56078431  0.68627451]\n",
      "   ..., \n",
      "   [ 0.69411765  0.60392157  0.75686275]\n",
      "   [ 0.70980392  0.61176471  0.76078431]\n",
      "   [ 0.71764706  0.62745098  0.76078431]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.49019608  0.43137255  0.4       ]\n",
      "   [ 0.50588235  0.43921569  0.40392157]\n",
      "   [ 0.29803922  0.2627451   0.18431373]\n",
      "   ..., \n",
      "   [ 0.65882353  0.5372549   0.47058824]\n",
      "   [ 0.61960784  0.49411765  0.40392157]\n",
      "   [ 0.57254902  0.45490196  0.34117647]]\n",
      "\n",
      "  [[ 0.33333333  0.30196078  0.2745098 ]\n",
      "   [ 0.36862745  0.31764706  0.27843137]\n",
      "   [ 0.29019608  0.25490196  0.17647059]\n",
      "   ..., \n",
      "   [ 0.63529412  0.51764706  0.41568627]\n",
      "   [ 0.65098039  0.5254902   0.39215686]\n",
      "   [ 0.61960784  0.50196078  0.36078431]]\n",
      "\n",
      "  [[ 0.49019608  0.43921569  0.43529412]\n",
      "   [ 0.50980392  0.44313725  0.43529412]\n",
      "   [ 0.41176471  0.35686275  0.29411765]\n",
      "   ..., \n",
      "   [ 0.51764706  0.41568627  0.30588235]\n",
      "   [ 0.50980392  0.39607843  0.25098039]\n",
      "   [ 0.55686275  0.45098039  0.30588235]]]\n",
      "\n",
      "\n",
      " [[[ 0.39215686  0.42745098  0.32941176]\n",
      "   [ 0.47843137  0.49411765  0.42745098]\n",
      "   [ 0.34117647  0.34117647  0.29803922]\n",
      "   ..., \n",
      "   [ 0.29411765  0.30588235  0.27058824]\n",
      "   [ 0.2745098   0.28627451  0.25098039]\n",
      "   [ 0.2745098   0.28627451  0.25098039]]\n",
      "\n",
      "  [[ 0.3372549   0.38823529  0.27843137]\n",
      "   [ 0.29803922  0.32941176  0.25882353]\n",
      "   [ 0.23529412  0.25098039  0.21176471]\n",
      "   ..., \n",
      "   [ 0.30588235  0.31764706  0.28235294]\n",
      "   [ 0.29803922  0.30980392  0.2745098 ]\n",
      "   [ 0.32156863  0.33333333  0.29803922]]\n",
      "\n",
      "  [[ 0.32941176  0.39215686  0.28627451]\n",
      "   [ 0.3254902   0.37254902  0.29411765]\n",
      "   [ 0.30196078  0.3372549   0.28235294]\n",
      "   ..., \n",
      "   [ 0.29019608  0.30196078  0.26666667]\n",
      "   [ 0.28627451  0.29803922  0.2627451 ]\n",
      "   [ 0.3254902   0.3372549   0.30196078]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.25098039  0.30196078  0.30980392]\n",
      "   [ 0.47843137  0.52156863  0.56470588]\n",
      "   [ 0.5254902   0.56862745  0.61176471]\n",
      "   ..., \n",
      "   [ 0.41176471  0.48235294  0.47058824]\n",
      "   [ 0.32941176  0.40392157  0.35686275]\n",
      "   [ 0.23529412  0.34509804  0.24705882]]\n",
      "\n",
      "  [[ 0.17254902  0.2         0.21960784]\n",
      "   [ 0.30588235  0.32941176  0.36862745]\n",
      "   [ 0.37647059  0.39607843  0.43137255]\n",
      "   ..., \n",
      "   [ 0.57647059  0.64705882  0.69803922]\n",
      "   [ 0.49411765  0.56078431  0.58431373]\n",
      "   [ 0.36862745  0.45882353  0.44313725]]\n",
      "\n",
      "  [[ 0.14117647  0.1372549   0.15294118]\n",
      "   [ 0.23137255  0.22745098  0.25882353]\n",
      "   [ 0.32156863  0.31764706  0.33333333]\n",
      "   ..., \n",
      "   [ 0.5254902   0.6         0.62745098]\n",
      "   [ 0.54117647  0.59607843  0.61960784]\n",
      "   [ 0.50980392  0.58039216  0.58823529]]]\n",
      "\n",
      "\n",
      " [[[ 0.0745098   0.1254902   0.05882353]\n",
      "   [ 0.08235294  0.14901961  0.08235294]\n",
      "   [ 0.10588235  0.19215686  0.1254902 ]\n",
      "   ..., \n",
      "   [ 0.29411765  0.48627451  0.51372549]\n",
      "   [ 0.29803922  0.48627451  0.50980392]\n",
      "   [ 0.27843137  0.4627451   0.48627451]]\n",
      "\n",
      "  [[ 0.09019608  0.12156863  0.05490196]\n",
      "   [ 0.08235294  0.11764706  0.04705882]\n",
      "   [ 0.09019608  0.1372549   0.05490196]\n",
      "   ..., \n",
      "   [ 0.28235294  0.4627451   0.49411765]\n",
      "   [ 0.29411765  0.46666667  0.48627451]\n",
      "   [ 0.26666667  0.43529412  0.44705882]]\n",
      "\n",
      "  [[ 0.09411765  0.14509804  0.06666667]\n",
      "   [ 0.08627451  0.1372549   0.0627451 ]\n",
      "   [ 0.09411765  0.14117647  0.07058824]\n",
      "   ..., \n",
      "   [ 0.25098039  0.42745098  0.43921569]\n",
      "   [ 0.2627451   0.42745098  0.43529412]\n",
      "   [ 0.25098039  0.41176471  0.41176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.24313725  0.18039216  0.09019608]\n",
      "   [ 0.23529412  0.18039216  0.10588235]\n",
      "   [ 0.21568627  0.18823529  0.10980392]\n",
      "   ..., \n",
      "   [ 0.05098039  0.02352941  0.01568627]\n",
      "   [ 0.04705882  0.05490196  0.03137255]\n",
      "   [ 0.09803922  0.15686275  0.11764706]]\n",
      "\n",
      "  [[ 0.24705882  0.20784314  0.11764706]\n",
      "   [ 0.19215686  0.17647059  0.08627451]\n",
      "   [ 0.17647059  0.18039216  0.09019608]\n",
      "   ..., \n",
      "   [ 0.11372549  0.1372549   0.12156863]\n",
      "   [ 0.11764706  0.16470588  0.14509804]\n",
      "   [ 0.10588235  0.19607843  0.16862745]]\n",
      "\n",
      "  [[ 0.27058824  0.20392157  0.11372549]\n",
      "   [ 0.19215686  0.14901961  0.07843137]\n",
      "   [ 0.21176471  0.18039216  0.10588235]\n",
      "   ..., \n",
      "   [ 0.25882353  0.34509804  0.33333333]\n",
      "   [ 0.15686275  0.26666667  0.25098039]\n",
      "   [ 0.11372549  0.24313725  0.22745098]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.1372549   0.69803922  0.92156863]\n",
      "   [ 0.15686275  0.69019608  0.9372549 ]\n",
      "   [ 0.16470588  0.69019608  0.94509804]\n",
      "   ..., \n",
      "   [ 0.38823529  0.69411765  0.85882353]\n",
      "   [ 0.30980392  0.57647059  0.77254902]\n",
      "   [ 0.34901961  0.58039216  0.74117647]]\n",
      "\n",
      "  [[ 0.22352941  0.71372549  0.91764706]\n",
      "   [ 0.17254902  0.72156863  0.98039216]\n",
      "   [ 0.19607843  0.71764706  0.94117647]\n",
      "   ..., \n",
      "   [ 0.61176471  0.71372549  0.78431373]\n",
      "   [ 0.55294118  0.69411765  0.80784314]\n",
      "   [ 0.45490196  0.58431373  0.68627451]]\n",
      "\n",
      "  [[ 0.38431373  0.77254902  0.92941176]\n",
      "   [ 0.25098039  0.74117647  0.98823529]\n",
      "   [ 0.27058824  0.75294118  0.96078431]\n",
      "   ..., \n",
      "   [ 0.7372549   0.76470588  0.80784314]\n",
      "   [ 0.46666667  0.52941176  0.57647059]\n",
      "   [ 0.23921569  0.30980392  0.35294118]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.28627451  0.30980392  0.30196078]\n",
      "   [ 0.20784314  0.24705882  0.26666667]\n",
      "   [ 0.21176471  0.26666667  0.31372549]\n",
      "   ..., \n",
      "   [ 0.06666667  0.15686275  0.25098039]\n",
      "   [ 0.08235294  0.14117647  0.2       ]\n",
      "   [ 0.12941176  0.18823529  0.19215686]]\n",
      "\n",
      "  [[ 0.23921569  0.26666667  0.29411765]\n",
      "   [ 0.21568627  0.2745098   0.3372549 ]\n",
      "   [ 0.22352941  0.30980392  0.40392157]\n",
      "   ..., \n",
      "   [ 0.09411765  0.18823529  0.28235294]\n",
      "   [ 0.06666667  0.1372549   0.20784314]\n",
      "   [ 0.02745098  0.09019608  0.1254902 ]]\n",
      "\n",
      "  [[ 0.17254902  0.21960784  0.28627451]\n",
      "   [ 0.18039216  0.25882353  0.34509804]\n",
      "   [ 0.19215686  0.30196078  0.41176471]\n",
      "   ..., \n",
      "   [ 0.10588235  0.20392157  0.30196078]\n",
      "   [ 0.08235294  0.16862745  0.25882353]\n",
      "   [ 0.04705882  0.12156863  0.19607843]]]\n",
      "\n",
      "\n",
      " [[[ 0.74117647  0.82745098  0.94117647]\n",
      "   [ 0.72941176  0.81568627  0.9254902 ]\n",
      "   [ 0.7254902   0.81176471  0.92156863]\n",
      "   ..., \n",
      "   [ 0.68627451  0.76470588  0.87843137]\n",
      "   [ 0.6745098   0.76078431  0.87058824]\n",
      "   [ 0.6627451   0.76078431  0.8627451 ]]\n",
      "\n",
      "  [[ 0.76078431  0.82352941  0.9372549 ]\n",
      "   [ 0.74901961  0.81176471  0.9254902 ]\n",
      "   [ 0.74509804  0.80784314  0.92156863]\n",
      "   ..., \n",
      "   [ 0.67843137  0.75294118  0.8627451 ]\n",
      "   [ 0.67058824  0.74901961  0.85490196]\n",
      "   [ 0.65490196  0.74509804  0.84705882]]\n",
      "\n",
      "  [[ 0.81568627  0.85882353  0.95686275]\n",
      "   [ 0.80392157  0.84705882  0.94117647]\n",
      "   [ 0.8         0.84313725  0.9372549 ]\n",
      "   ..., \n",
      "   [ 0.68627451  0.74901961  0.85098039]\n",
      "   [ 0.6745098   0.74509804  0.84705882]\n",
      "   [ 0.6627451   0.74901961  0.84313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81176471  0.78039216  0.70980392]\n",
      "   [ 0.79607843  0.76470588  0.68627451]\n",
      "   [ 0.79607843  0.76862745  0.67843137]\n",
      "   ..., \n",
      "   [ 0.52941176  0.51764706  0.49803922]\n",
      "   [ 0.63529412  0.61960784  0.58823529]\n",
      "   [ 0.65882353  0.63921569  0.59215686]]\n",
      "\n",
      "  [[ 0.77647059  0.74509804  0.66666667]\n",
      "   [ 0.74117647  0.70980392  0.62352941]\n",
      "   [ 0.70588235  0.6745098   0.57647059]\n",
      "   ..., \n",
      "   [ 0.69803922  0.67058824  0.62745098]\n",
      "   [ 0.68627451  0.6627451   0.61176471]\n",
      "   [ 0.68627451  0.6627451   0.60392157]]\n",
      "\n",
      "  [[ 0.77647059  0.74117647  0.67843137]\n",
      "   [ 0.74117647  0.70980392  0.63529412]\n",
      "   [ 0.69803922  0.66666667  0.58431373]\n",
      "   ..., \n",
      "   [ 0.76470588  0.72156863  0.6627451 ]\n",
      "   [ 0.76862745  0.74117647  0.67058824]\n",
      "   [ 0.76470588  0.74509804  0.67058824]]]\n",
      "\n",
      "\n",
      " [[[ 0.89803922  0.89803922  0.9372549 ]\n",
      "   [ 0.9254902   0.92941176  0.96862745]\n",
      "   [ 0.91764706  0.9254902   0.96862745]\n",
      "   ..., \n",
      "   [ 0.85098039  0.85882353  0.91372549]\n",
      "   [ 0.86666667  0.8745098   0.91764706]\n",
      "   [ 0.87058824  0.8745098   0.91372549]]\n",
      "\n",
      "  [[ 0.87058824  0.86666667  0.89803922]\n",
      "   [ 0.9372549   0.9372549   0.97647059]\n",
      "   [ 0.91372549  0.91764706  0.96470588]\n",
      "   ..., \n",
      "   [ 0.8745098   0.8745098   0.9254902 ]\n",
      "   [ 0.89019608  0.89411765  0.93333333]\n",
      "   [ 0.82352941  0.82745098  0.8627451 ]]\n",
      "\n",
      "  [[ 0.83529412  0.80784314  0.82745098]\n",
      "   [ 0.91764706  0.90980392  0.9372549 ]\n",
      "   [ 0.90588235  0.91372549  0.95686275]\n",
      "   ..., \n",
      "   [ 0.8627451   0.8627451   0.90980392]\n",
      "   [ 0.8627451   0.85882353  0.90980392]\n",
      "   [ 0.79215686  0.79607843  0.84313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.58823529  0.56078431  0.52941176]\n",
      "   [ 0.54901961  0.52941176  0.49803922]\n",
      "   [ 0.51764706  0.49803922  0.47058824]\n",
      "   ..., \n",
      "   [ 0.87843137  0.87058824  0.85490196]\n",
      "   [ 0.90196078  0.89411765  0.88235294]\n",
      "   [ 0.94509804  0.94509804  0.93333333]]\n",
      "\n",
      "  [[ 0.5372549   0.51764706  0.49411765]\n",
      "   [ 0.50980392  0.49803922  0.47058824]\n",
      "   [ 0.49019608  0.4745098   0.45098039]\n",
      "   ..., \n",
      "   [ 0.70980392  0.70588235  0.69803922]\n",
      "   [ 0.79215686  0.78823529  0.77647059]\n",
      "   [ 0.83137255  0.82745098  0.81176471]]\n",
      "\n",
      "  [[ 0.47843137  0.46666667  0.44705882]\n",
      "   [ 0.4627451   0.45490196  0.43137255]\n",
      "   [ 0.47058824  0.45490196  0.43529412]\n",
      "   ..., \n",
      "   [ 0.70196078  0.69411765  0.67843137]\n",
      "   [ 0.64313725  0.64313725  0.63529412]\n",
      "   [ 0.63921569  0.63921569  0.63137255]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.61960784  0.43921569  0.19215686]\n",
      "   [ 0.62352941  0.43529412  0.18431373]\n",
      "   [ 0.64705882  0.45490196  0.2       ]\n",
      "   ..., \n",
      "   [ 0.5372549   0.37254902  0.14117647]\n",
      "   [ 0.49411765  0.35686275  0.14117647]\n",
      "   [ 0.45490196  0.33333333  0.12941176]]\n",
      "\n",
      "  [[ 0.59607843  0.43921569  0.2       ]\n",
      "   [ 0.59215686  0.43137255  0.15686275]\n",
      "   [ 0.62352941  0.44705882  0.17647059]\n",
      "   ..., \n",
      "   [ 0.53333333  0.37254902  0.12156863]\n",
      "   [ 0.49019608  0.35686275  0.1254902 ]\n",
      "   [ 0.46666667  0.34509804  0.13333333]]\n",
      "\n",
      "  [[ 0.59215686  0.43137255  0.18431373]\n",
      "   [ 0.59215686  0.42745098  0.12941176]\n",
      "   [ 0.61960784  0.43529412  0.14117647]\n",
      "   ..., \n",
      "   [ 0.54509804  0.38431373  0.13333333]\n",
      "   [ 0.50980392  0.37254902  0.13333333]\n",
      "   [ 0.47058824  0.34901961  0.12941176]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.26666667  0.48627451  0.69411765]\n",
      "   [ 0.16470588  0.39215686  0.58039216]\n",
      "   [ 0.12156863  0.34509804  0.5372549 ]\n",
      "   ..., \n",
      "   [ 0.14901961  0.38039216  0.57254902]\n",
      "   [ 0.05098039  0.25098039  0.42352941]\n",
      "   [ 0.15686275  0.33333333  0.49803922]]\n",
      "\n",
      "  [[ 0.23921569  0.45490196  0.65882353]\n",
      "   [ 0.19215686  0.4         0.58039216]\n",
      "   [ 0.1372549   0.33333333  0.51764706]\n",
      "   ..., \n",
      "   [ 0.10196078  0.32156863  0.50980392]\n",
      "   [ 0.11372549  0.32156863  0.49411765]\n",
      "   [ 0.07843137  0.25098039  0.41960784]]\n",
      "\n",
      "  [[ 0.21176471  0.41960784  0.62745098]\n",
      "   [ 0.21960784  0.41176471  0.58431373]\n",
      "   [ 0.17647059  0.34901961  0.51764706]\n",
      "   ..., \n",
      "   [ 0.09411765  0.30196078  0.48627451]\n",
      "   [ 0.13333333  0.32941176  0.50588235]\n",
      "   [ 0.08235294  0.2627451   0.43137255]]]\n",
      "\n",
      "\n",
      " [[[ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.90588235  0.90588235  0.90588235]\n",
      "   [ 0.90980392  0.90980392  0.90980392]\n",
      "   ..., \n",
      "   [ 0.91372549  0.91372549  0.91372549]\n",
      "   [ 0.91372549  0.91372549  0.91372549]\n",
      "   [ 0.90980392  0.90980392  0.90980392]]\n",
      "\n",
      "  [[ 0.93333333  0.93333333  0.93333333]\n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   ..., \n",
      "   [ 0.9254902   0.9254902   0.9254902 ]\n",
      "   [ 0.9254902   0.9254902   0.9254902 ]\n",
      "   [ 0.92156863  0.92156863  0.92156863]]\n",
      "\n",
      "  [[ 0.92941176  0.92941176  0.92941176]\n",
      "   [ 0.91764706  0.91764706  0.91764706]\n",
      "   [ 0.91764706  0.91764706  0.91764706]\n",
      "   ..., \n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.91764706  0.91764706  0.91764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.34117647  0.38823529  0.34901961]\n",
      "   [ 0.16862745  0.2         0.14509804]\n",
      "   [ 0.0745098   0.09019608  0.04313725]\n",
      "   ..., \n",
      "   [ 0.6627451   0.72156863  0.70196078]\n",
      "   [ 0.71372549  0.77254902  0.75686275]\n",
      "   [ 0.7372549   0.79215686  0.78823529]]\n",
      "\n",
      "  [[ 0.32156863  0.37647059  0.32156863]\n",
      "   [ 0.18039216  0.22352941  0.14117647]\n",
      "   [ 0.14117647  0.17254902  0.08627451]\n",
      "   ..., \n",
      "   [ 0.68235294  0.74117647  0.71764706]\n",
      "   [ 0.7254902   0.78431373  0.76862745]\n",
      "   [ 0.73333333  0.79215686  0.78431373]]\n",
      "\n",
      "  [[ 0.33333333  0.39607843  0.3254902 ]\n",
      "   [ 0.24313725  0.29411765  0.18823529]\n",
      "   [ 0.22745098  0.2627451   0.14901961]\n",
      "   ..., \n",
      "   [ 0.65882353  0.71764706  0.69803922]\n",
      "   [ 0.70588235  0.76470588  0.74901961]\n",
      "   [ 0.72941176  0.78431373  0.78039216]]]\n",
      "\n",
      "\n",
      " [[[ 0.61960784  0.74509804  0.87058824]\n",
      "   [ 0.61960784  0.73333333  0.85490196]\n",
      "   [ 0.54509804  0.65098039  0.76078431]\n",
      "   ..., \n",
      "   [ 0.89411765  0.90588235  0.91764706]\n",
      "   [ 0.92941176  0.9372549   0.95294118]\n",
      "   [ 0.93333333  0.94509804  0.96470588]]\n",
      "\n",
      "  [[ 0.66666667  0.78431373  0.89803922]\n",
      "   [ 0.6745098   0.78039216  0.88627451]\n",
      "   [ 0.59215686  0.69019608  0.78823529]\n",
      "   ..., \n",
      "   [ 0.90980392  0.90980392  0.9254902 ]\n",
      "   [ 0.96470588  0.96470588  0.98039216]\n",
      "   [ 0.96470588  0.96862745  0.98431373]]\n",
      "\n",
      "  [[ 0.68235294  0.78823529  0.88235294]\n",
      "   [ 0.69019608  0.78431373  0.87058824]\n",
      "   [ 0.61568627  0.70196078  0.78039216]\n",
      "   ..., \n",
      "   [ 0.90196078  0.89803922  0.90980392]\n",
      "   [ 0.98039216  0.97647059  0.98431373]\n",
      "   [ 0.96078431  0.95686275  0.96862745]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.12156863  0.15686275  0.17647059]\n",
      "   [ 0.11764706  0.15294118  0.17254902]\n",
      "   [ 0.10196078  0.1372549   0.15686275]\n",
      "   ..., \n",
      "   [ 0.14509804  0.15686275  0.18039216]\n",
      "   [ 0.03529412  0.05098039  0.05490196]\n",
      "   [ 0.01568627  0.02745098  0.01960784]]\n",
      "\n",
      "  [[ 0.09019608  0.13333333  0.15294118]\n",
      "   [ 0.10588235  0.14901961  0.16862745]\n",
      "   [ 0.09803922  0.14117647  0.16078431]\n",
      "   ..., \n",
      "   [ 0.0745098   0.07843137  0.09411765]\n",
      "   [ 0.01568627  0.02352941  0.01176471]\n",
      "   [ 0.01960784  0.02745098  0.01176471]]\n",
      "\n",
      "  [[ 0.10980392  0.16078431  0.18431373]\n",
      "   [ 0.11764706  0.16862745  0.19607843]\n",
      "   [ 0.1254902   0.17647059  0.20392157]\n",
      "   ..., \n",
      "   [ 0.01960784  0.02352941  0.03137255]\n",
      "   [ 0.01568627  0.01960784  0.01176471]\n",
      "   [ 0.02745098  0.03137255  0.02745098]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.0745098   0.05490196  0.04313725]\n",
      "   [ 0.05882353  0.05490196  0.04313725]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03529412  0.02745098]\n",
      "   [ 0.04705882  0.04313725  0.03529412]\n",
      "   [ 0.05098039  0.04705882  0.03921569]]\n",
      "\n",
      "  [[ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.07843137  0.0627451   0.05098039]\n",
      "   [ 0.07058824  0.06666667  0.04705882]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03529412  0.02745098]\n",
      "   [ 0.03921569  0.03529412  0.02745098]\n",
      "   [ 0.04705882  0.04313725  0.03529412]]\n",
      "\n",
      "  [[ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.08235294  0.06666667  0.04705882]\n",
      "   [ 0.07843137  0.07058824  0.04313725]\n",
      "   ..., \n",
      "   [ 0.04705882  0.04313725  0.03529412]\n",
      "   [ 0.04705882  0.04313725  0.03529412]\n",
      "   [ 0.05098039  0.04705882  0.03921569]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.12941176  0.09803922  0.05098039]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   ..., \n",
      "   [ 0.10980392  0.09803922  0.20392157]\n",
      "   [ 0.11372549  0.09803922  0.22745098]\n",
      "   [ 0.09019608  0.07843137  0.16470588]]\n",
      "\n",
      "  [[ 0.12941176  0.09803922  0.05490196]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   ..., \n",
      "   [ 0.10588235  0.09411765  0.20392157]\n",
      "   [ 0.10588235  0.09411765  0.21960784]\n",
      "   [ 0.09803922  0.08627451  0.18431373]]\n",
      "\n",
      "  [[ 0.12156863  0.09019608  0.04705882]\n",
      "   [ 0.1254902   0.09411765  0.05098039]\n",
      "   [ 0.12941176  0.09803922  0.05490196]\n",
      "   ..., \n",
      "   [ 0.09411765  0.09019608  0.19607843]\n",
      "   [ 0.10196078  0.09019608  0.20784314]\n",
      "   [ 0.09803922  0.07843137  0.18431373]]]\n",
      "\n",
      "\n",
      " [[[ 0.09803922  0.15686275  0.04705882]\n",
      "   [ 0.05882353  0.14117647  0.01176471]\n",
      "   [ 0.09019608  0.16078431  0.07058824]\n",
      "   ..., \n",
      "   [ 0.23921569  0.32156863  0.30588235]\n",
      "   [ 0.36078431  0.44313725  0.43921569]\n",
      "   [ 0.29411765  0.34901961  0.36078431]]\n",
      "\n",
      "  [[ 0.04705882  0.09803922  0.02352941]\n",
      "   [ 0.07843137  0.14509804  0.02745098]\n",
      "   [ 0.09411765  0.14117647  0.05882353]\n",
      "   ..., \n",
      "   [ 0.45098039  0.5254902   0.54117647]\n",
      "   [ 0.58431373  0.65882353  0.69411765]\n",
      "   [ 0.40784314  0.45882353  0.51372549]]\n",
      "\n",
      "  [[ 0.04705882  0.09803922  0.04313725]\n",
      "   [ 0.05882353  0.11372549  0.02352941]\n",
      "   [ 0.13333333  0.15686275  0.09411765]\n",
      "   ..., \n",
      "   [ 0.60392157  0.6745098   0.71372549]\n",
      "   [ 0.61568627  0.68627451  0.75294118]\n",
      "   [ 0.45490196  0.50588235  0.59215686]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.39215686  0.50588235  0.31764706]\n",
      "   [ 0.40392157  0.51764706  0.32941176]\n",
      "   [ 0.40784314  0.5254902   0.3372549 ]\n",
      "   ..., \n",
      "   [ 0.38039216  0.50196078  0.32941176]\n",
      "   [ 0.38431373  0.49411765  0.32941176]\n",
      "   [ 0.35686275  0.4745098   0.30980392]]\n",
      "\n",
      "  [[ 0.40392157  0.51764706  0.3254902 ]\n",
      "   [ 0.40784314  0.51372549  0.3254902 ]\n",
      "   [ 0.41960784  0.52941176  0.34117647]\n",
      "   ..., \n",
      "   [ 0.39607843  0.51764706  0.34117647]\n",
      "   [ 0.38823529  0.49803922  0.32941176]\n",
      "   [ 0.36078431  0.4745098   0.30980392]]\n",
      "\n",
      "  [[ 0.37254902  0.49411765  0.30588235]\n",
      "   [ 0.37254902  0.48235294  0.29803922]\n",
      "   [ 0.39607843  0.50196078  0.31764706]\n",
      "   ..., \n",
      "   [ 0.36470588  0.48627451  0.31372549]\n",
      "   [ 0.37254902  0.48235294  0.31764706]\n",
      "   [ 0.36078431  0.47058824  0.31372549]]]\n",
      "\n",
      "\n",
      " [[[ 0.28627451  0.30588235  0.29411765]\n",
      "   [ 0.38431373  0.40392157  0.44313725]\n",
      "   [ 0.38823529  0.41568627  0.44705882]\n",
      "   ..., \n",
      "   [ 0.52941176  0.58823529  0.59607843]\n",
      "   [ 0.52941176  0.58431373  0.60392157]\n",
      "   [ 0.79607843  0.84313725  0.8745098 ]]\n",
      "\n",
      "  [[ 0.27058824  0.28627451  0.2745098 ]\n",
      "   [ 0.32941176  0.34901961  0.38039216]\n",
      "   [ 0.26666667  0.29411765  0.31764706]\n",
      "   ..., \n",
      "   [ 0.33333333  0.37254902  0.34901961]\n",
      "   [ 0.27843137  0.32156863  0.31372549]\n",
      "   [ 0.47058824  0.52156863  0.52941176]]\n",
      "\n",
      "  [[ 0.27058824  0.28627451  0.2745098 ]\n",
      "   [ 0.35294118  0.37254902  0.39215686]\n",
      "   [ 0.24313725  0.27843137  0.29019608]\n",
      "   ..., \n",
      "   [ 0.29019608  0.31764706  0.2745098 ]\n",
      "   [ 0.20784314  0.24313725  0.21176471]\n",
      "   [ 0.24313725  0.29019608  0.27058824]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.48235294  0.50196078  0.37647059]\n",
      "   [ 0.51764706  0.51764706  0.4       ]\n",
      "   [ 0.50588235  0.50196078  0.39215686]\n",
      "   ..., \n",
      "   [ 0.42352941  0.41960784  0.34509804]\n",
      "   [ 0.24313725  0.23529412  0.21568627]\n",
      "   [ 0.10588235  0.10588235  0.10980392]]\n",
      "\n",
      "  [[ 0.45098039  0.4745098   0.35686275]\n",
      "   [ 0.48235294  0.48627451  0.37254902]\n",
      "   [ 0.50588235  0.49411765  0.38823529]\n",
      "   ..., \n",
      "   [ 0.45098039  0.45490196  0.36862745]\n",
      "   [ 0.25882353  0.25490196  0.23137255]\n",
      "   [ 0.10588235  0.10588235  0.10588235]]\n",
      "\n",
      "  [[ 0.45490196  0.47058824  0.35294118]\n",
      "   [ 0.4745098   0.47843137  0.36862745]\n",
      "   [ 0.50588235  0.50196078  0.39607843]\n",
      "   ..., \n",
      "   [ 0.45490196  0.45098039  0.36862745]\n",
      "   [ 0.26666667  0.25490196  0.22745098]\n",
      "   [ 0.10588235  0.10196078  0.10196078]]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,[None, image_shape[0], \n",
    "                                      image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,[None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Image Properties\n",
    "#     image_width = int(x_tensor.get_shape()[1])\n",
    "#     image_height = int(x_tensor.get_shape()[2])\n",
    "    color_channels =  int(x_tensor.get_shape()[3])\n",
    "    # Convolution filter\n",
    "    filter_size_width = int(conv_ksize[0])\n",
    "    filter_size_height = int(conv_ksize[1])\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=[filter_size_height, filter_size_width, \n",
    "                                              color_channels, conv_num_outputs]\n",
    "                                            ,\n",
    "                                             stddev=np.sqrt(2/x_tensor.shape[-1].value)\n",
    "                                            ))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight\n",
    "                              , strides=[1,int(conv_strides[0])\n",
    "                                         ,int(conv_strides[1]),1], padding='SAME')\n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,\n",
    "                                ksize=[1,int(pool_ksize[0]),\n",
    "                                       int(pool_ksize[1]),1],\n",
    "                                strides=[1,int(pool_strides[0]),\n",
    "                                         int(pool_strides[1]),1], padding='SAME')\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    result = tf.reshape(x_tensor, [-1,shape[1]*shape[2]*shape[3]])\n",
    "    return result\n",
    "    #Or\n",
    "    #return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=[shape[1],num_outputs]\n",
    "                                            ,\n",
    "                                             stddev=np.sqrt(2/x_tensor.shape[-1].value)\n",
    "                                            ))\n",
    "    bias = tf.Variable(tf.truncated_normal(stddev=np.sqrt(2/x_tensor.shape[-1].value), shape=[num_outputs]))\n",
    "\n",
    "\n",
    "    result = tf.nn.relu(tf.matmul(x_tensor, weight) + bias )\n",
    "    return result\n",
    "    \n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, \n",
    "                                             num_outputs=num_outputs,\n",
    "                                             activation_fn=tf.nn.relu)\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    shape = x_tensor.get_shape().as_list()\n",
    "    weight = tf.Variable(tf.truncated_normal(shape=[shape[1],num_outputs]\n",
    "                                            ,\n",
    "                                             stddev=np.sqrt(2/x_tensor.shape[-1].value)\n",
    "                                            ))\n",
    "    bias = tf.Variable(tf.truncated_normal(stddev=np.sqrt(2/x_tensor.shape[-1].value), shape=[num_outputs]))\n",
    "\n",
    "\n",
    "    result = tf.matmul(x_tensor, weight) + bias \n",
    "    return result\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor, \n",
    "                                                     num_outputs=num_outputs, \n",
    "                                                     activation_fn=None)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv1 = conv2d_maxpool(x, 32, (3,3), (1,1), (2,2), (2,2))\n",
    "    conv2 = conv2d_maxpool(conv1, 64, (3,3), (1,1), (2,2), (2,2))\n",
    "    conv3 = conv2d_maxpool(conv2,128,(3,3),(1,1),(2,2),(2,2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flat1 = flatten(conv2)\n",
    "    fc1 = fully_conn(flat1, 512)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    fc2 = fully_conn(fc1, 256)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    fc3 = fully_conn(fc2, 128)\n",
    "    fc3 = tf.nn.dropout(fc3, keep_prob)\n",
    "    out = output(fc3,10)\n",
    "    return out\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict = {x: feature_batch,y: label_batch, keep_prob: 1.0})\n",
    "    acc = session.run(accuracy,feed_dict = {x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print('Loss at {}'.format(loss), 'Validation Accuracy at {}'.format(acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 256\n",
    "batch_size = 64\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.2551984786987305 Validation Accuracy at 0.1753999888896942\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 2.2010035514831543 Validation Accuracy at 0.23099997639656067\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 2.088822603225708 Validation Accuracy at 0.301800012588501\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.9886350631713867 Validation Accuracy at 0.3479999899864197\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.8623933792114258 Validation Accuracy at 0.3619999885559082\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.7918140888214111 Validation Accuracy at 0.39319998025894165\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.6996122598648071 Validation Accuracy at 0.39419999718666077\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.6746292114257812 Validation Accuracy at 0.40039998292922974\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 1.6268928050994873 Validation Accuracy at 0.42799994349479675\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 1.4998846054077148 Validation Accuracy at 0.42580002546310425\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 1.4880011081695557 Validation Accuracy at 0.4225999712944031\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 1.4812076091766357 Validation Accuracy at 0.39819997549057007\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 1.4563707113265991 Validation Accuracy at 0.46059998869895935\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 1.2901756763458252 Validation Accuracy at 0.47179993987083435\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 1.2470401525497437 Validation Accuracy at 0.4761999547481537\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 1.139673113822937 Validation Accuracy at 0.481999933719635\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 1.03441321849823 Validation Accuracy at 0.49379995465278625\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 1.0714224576950073 Validation Accuracy at 0.48559993505477905\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 1.0748916864395142 Validation Accuracy at 0.4893999397754669\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 0.92625892162323 Validation Accuracy at 0.5049999952316284\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 0.9041836261749268 Validation Accuracy at 0.5053999423980713\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 0.8561834692955017 Validation Accuracy at 0.5229999423027039\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 0.8278966546058655 Validation Accuracy at 0.5117999315261841\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 0.7351773381233215 Validation Accuracy at 0.514799952507019\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 0.7182702422142029 Validation Accuracy at 0.5229999423027039\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 0.705412745475769 Validation Accuracy at 0.5175999402999878\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 0.6690433025360107 Validation Accuracy at 0.5257999300956726\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 0.6420630216598511 Validation Accuracy at 0.5327999591827393\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 0.6262489557266235 Validation Accuracy at 0.5189999341964722\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 0.5946495532989502 Validation Accuracy at 0.528999924659729\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 0.5358718633651733 Validation Accuracy at 0.5341999530792236\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 0.5354979038238525 Validation Accuracy at 0.5365999937057495\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 0.5072619915008545 Validation Accuracy at 0.5353999137878418\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 0.5140901207923889 Validation Accuracy at 0.5521999597549438\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 0.43958479166030884 Validation Accuracy at 0.538599967956543\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 0.4926643371582031 Validation Accuracy at 0.5485999584197998\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 0.44754689931869507 Validation Accuracy at 0.5445999503135681\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 0.42010220885276794 Validation Accuracy at 0.5541999340057373\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 0.38141539692878723 Validation Accuracy at 0.5471999645233154\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 0.3691219687461853 Validation Accuracy at 0.5469999313354492\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 0.33799105882644653 Validation Accuracy at 0.5549999475479126\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 0.3868039846420288 Validation Accuracy at 0.5511999130249023\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 0.36059293150901794 Validation Accuracy at 0.5573999285697937\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 0.3704029619693756 Validation Accuracy at 0.5539999604225159\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 0.3393271565437317 Validation Accuracy at 0.5479999780654907\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 0.3514902889728546 Validation Accuracy at 0.5479999780654907\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 0.31606096029281616 Validation Accuracy at 0.5687999725341797\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 0.33943289518356323 Validation Accuracy at 0.5567999482154846\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 0.41709104180336 Validation Accuracy at 0.5485999584197998\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 0.3136138617992401 Validation Accuracy at 0.5589998960494995\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 0.2578262984752655 Validation Accuracy at 0.5525999069213867\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 0.28455740213394165 Validation Accuracy at 0.5651999115943909\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 0.292020320892334 Validation Accuracy at 0.5593999624252319\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 0.28342893719673157 Validation Accuracy at 0.5495999455451965\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 0.2659941613674164 Validation Accuracy at 0.5539999008178711\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 0.2556012272834778 Validation Accuracy at 0.5531998872756958\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 0.30078208446502686 Validation Accuracy at 0.5535999536514282\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 0.3149096369743347 Validation Accuracy at 0.553399920463562\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 0.2843555510044098 Validation Accuracy at 0.5549999475479126\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 0.220479816198349 Validation Accuracy at 0.5561999678611755\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 0.2324983775615692 Validation Accuracy at 0.5565998554229736\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 0.2011202573776245 Validation Accuracy at 0.5563998818397522\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 0.17211443185806274 Validation Accuracy at 0.549799919128418\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 0.20064854621887207 Validation Accuracy at 0.554599940776825\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 0.19770868122577667 Validation Accuracy at 0.5559999346733093\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 0.20086802542209625 Validation Accuracy at 0.5683999061584473\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 0.22010210156440735 Validation Accuracy at 0.5625998973846436\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 0.17685392498970032 Validation Accuracy at 0.5663999319076538\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 0.1725868433713913 Validation Accuracy at 0.5593999028205872\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 0.16033735871315002 Validation Accuracy at 0.5635999441146851\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 0.18894240260124207 Validation Accuracy at 0.5557999610900879\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 0.21678519248962402 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 0.247245654463768 Validation Accuracy at 0.5535999536514282\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 0.14931201934814453 Validation Accuracy at 0.5643999576568604\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 0.15304429829120636 Validation Accuracy at 0.5651999115943909\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 0.13532981276512146 Validation Accuracy at 0.5609999299049377\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 0.13414005935192108 Validation Accuracy at 0.5611999034881592\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 0.1439794898033142 Validation Accuracy at 0.5679999589920044\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 0.131745383143425 Validation Accuracy at 0.5635999441146851\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 0.1266225278377533 Validation Accuracy at 0.5629999041557312\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 0.10798309743404388 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 0.11331538110971451 Validation Accuracy at 0.5654000043869019\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 0.13606002926826477 Validation Accuracy at 0.5607998967170715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 0.1113191545009613 Validation Accuracy at 0.5655999183654785\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 0.11322770267724991 Validation Accuracy at 0.5571998953819275\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 0.14267484843730927 Validation Accuracy at 0.564799964427948\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 0.14074848592281342 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 0.23514679074287415 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 0.27473294734954834 Validation Accuracy at 0.5625999569892883\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 0.1591469943523407 Validation Accuracy at 0.5657998919487\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 0.12948985397815704 Validation Accuracy at 0.5697999000549316\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 0.15143051743507385 Validation Accuracy at 0.5685999393463135\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 0.12419566512107849 Validation Accuracy at 0.575999915599823\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 0.11388528347015381 Validation Accuracy at 0.5689999461174011\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 0.08873574435710907 Validation Accuracy at 0.5677999258041382\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 0.06677117198705673 Validation Accuracy at 0.5669999122619629\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 0.09439770132303238 Validation Accuracy at 0.5655999779701233\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 0.08494631946086884 Validation Accuracy at 0.5685999393463135\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 0.07962382584810257 Validation Accuracy at 0.5707999467849731\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 0.07585077732801437 Validation Accuracy at 0.5687999129295349\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss at 0.06548057496547699 Validation Accuracy at 0.5683999061584473\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss at 0.09142087399959564 Validation Accuracy at 0.5615999102592468\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss at 0.08179181814193726 Validation Accuracy at 0.561799943447113\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss at 0.08490077406167984 Validation Accuracy at 0.5743999481201172\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss at 0.06448666751384735 Validation Accuracy at 0.5739999413490295\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss at 0.07584751397371292 Validation Accuracy at 0.5701999664306641\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss at 0.06344284117221832 Validation Accuracy at 0.559999942779541\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss at 0.08010388165712357 Validation Accuracy at 0.556399941444397\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss at 0.08622388541698456 Validation Accuracy at 0.5649999380111694\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss at 0.06215450167655945 Validation Accuracy at 0.5763999223709106\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss at 0.07392674684524536 Validation Accuracy at 0.5791999101638794\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss at 0.07236040383577347 Validation Accuracy at 0.5753999352455139\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss at 0.05800996720790863 Validation Accuracy at 0.5771999359130859\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss at 0.05920771509408951 Validation Accuracy at 0.5801999568939209\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss at 0.06598560512065887 Validation Accuracy at 0.5701999664306641\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss at 0.050933465361595154 Validation Accuracy at 0.5693999528884888\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss at 0.055174048990011215 Validation Accuracy at 0.5771999359130859\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss at 0.05971067398786545 Validation Accuracy at 0.5703999400138855\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss at 0.05356951057910919 Validation Accuracy at 0.5729999542236328\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss at 0.06936876475811005 Validation Accuracy at 0.5779999494552612\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss at 0.06987781822681427 Validation Accuracy at 0.5743998885154724\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss at 0.08248554915189743 Validation Accuracy at 0.5829998850822449\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss at 0.06396687030792236 Validation Accuracy at 0.5735999345779419\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss at 0.05221395567059517 Validation Accuracy at 0.5715999603271484\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss at 0.07222224026918411 Validation Accuracy at 0.5821999311447144\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss at 0.08038971573114395 Validation Accuracy at 0.5805999040603638\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss at 0.08440272510051727 Validation Accuracy at 0.5703999400138855\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss at 0.0801556184887886 Validation Accuracy at 0.5747999548912048\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss at 0.061212435364723206 Validation Accuracy at 0.5737999081611633\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss at 0.06310237944126129 Validation Accuracy at 0.5809999108314514\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss at 0.06499677896499634 Validation Accuracy at 0.5811998844146729\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss at 0.053668346256017685 Validation Accuracy at 0.5789998769760132\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss at 0.06587239354848862 Validation Accuracy at 0.5711999535560608\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss at 0.0635286197066307 Validation Accuracy at 0.5753999352455139\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss at 0.05029276758432388 Validation Accuracy at 0.5861998796463013\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss at 0.06604968011379242 Validation Accuracy at 0.5757998824119568\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss at 0.05458120256662369 Validation Accuracy at 0.5827999711036682\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss at 0.06146710366010666 Validation Accuracy at 0.5865999460220337\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss at 0.04396801069378853 Validation Accuracy at 0.5775999426841736\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss at 0.03940889239311218 Validation Accuracy at 0.5813999176025391\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss at 0.05748700350522995 Validation Accuracy at 0.5751999020576477\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss at 0.0502740740776062 Validation Accuracy at 0.5863999724388123\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss at 0.06391828507184982 Validation Accuracy at 0.5679998993873596\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss at 0.06657446920871735 Validation Accuracy at 0.5791999101638794\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss at 0.09306185692548752 Validation Accuracy at 0.5729999542236328\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss at 0.060767900198698044 Validation Accuracy at 0.573199987411499\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss at 0.11097384244203568 Validation Accuracy at 0.5753998756408691\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss at 0.05536768585443497 Validation Accuracy at 0.5791999101638794\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss at 0.05866232141852379 Validation Accuracy at 0.5823999643325806\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss at 0.04931492730975151 Validation Accuracy at 0.5853999257087708\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss at 0.04883403331041336 Validation Accuracy at 0.577799916267395\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss at 0.0614563524723053 Validation Accuracy at 0.5751999616622925\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss at 0.04805958643555641 Validation Accuracy at 0.5799999237060547\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss at 0.04161228984594345 Validation Accuracy at 0.5775999426841736\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss at 0.04871527850627899 Validation Accuracy at 0.5803999304771423\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss at 0.03587592765688896 Validation Accuracy at 0.5857999324798584\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss at 0.03920863941311836 Validation Accuracy at 0.5807998776435852\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss at 0.032949939370155334 Validation Accuracy at 0.5885999202728271\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss at 0.05234179645776749 Validation Accuracy at 0.5823999643325806\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss at 0.053592294454574585 Validation Accuracy at 0.5715999603271484\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss at 0.04369812458753586 Validation Accuracy at 0.5739999413490295\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss at 0.0390794575214386 Validation Accuracy at 0.5873998999595642\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss at 0.037366680800914764 Validation Accuracy at 0.5765998959541321\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss at 0.03415129333734512 Validation Accuracy at 0.5847999453544617\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss at 0.050951771438121796 Validation Accuracy at 0.5801999568939209\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss at 0.031209036707878113 Validation Accuracy at 0.5809999108314514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167, CIFAR-10 Batch 1:  Loss at 0.03293444588780403 Validation Accuracy at 0.579599916934967\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss at 0.03390280902385712 Validation Accuracy at 0.5845999121665955\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss at 0.030996907502412796 Validation Accuracy at 0.5807998776435852\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss at 0.030859531834721565 Validation Accuracy at 0.5871999263763428\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss at 0.03936021402478218 Validation Accuracy at 0.5787999033927917\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss at 0.03344789147377014 Validation Accuracy at 0.5891999006271362\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss at 0.04024448245763779 Validation Accuracy at 0.5757999420166016\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss at 0.03778747469186783 Validation Accuracy at 0.5861998796463013\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss at 0.0393165647983551 Validation Accuracy at 0.5885999202728271\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss at 0.04003126174211502 Validation Accuracy at 0.577799916267395\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss at 0.040054235607385635 Validation Accuracy at 0.5781999230384827\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss at 0.0348145067691803 Validation Accuracy at 0.5827999114990234\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss at 0.04541270062327385 Validation Accuracy at 0.5759998559951782\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss at 0.0321512371301651 Validation Accuracy at 0.5849999189376831\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss at 0.04006427526473999 Validation Accuracy at 0.5889999270439148\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss at 0.04030220955610275 Validation Accuracy at 0.5863999128341675\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss at 0.03519928827881813 Validation Accuracy at 0.5869998931884766\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss at 0.032610680907964706 Validation Accuracy at 0.5825998783111572\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss at 0.03603648394346237 Validation Accuracy at 0.5949999094009399\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss at 0.037696607410907745 Validation Accuracy at 0.5803999304771423\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss at 0.0377422571182251 Validation Accuracy at 0.5761998891830444\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss at 0.03432134538888931 Validation Accuracy at 0.5877999067306519\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss at 0.036116041243076324 Validation Accuracy at 0.5827999114990234\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss at 0.03358937054872513 Validation Accuracy at 0.5909999012947083\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss at 0.03173088654875755 Validation Accuracy at 0.5913999080657959\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss at 0.04728949815034866 Validation Accuracy at 0.5747999548912048\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss at 0.034508295357227325 Validation Accuracy at 0.5865998864173889\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss at 0.0499383807182312 Validation Accuracy at 0.5789998769760132\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss at 0.04437170922756195 Validation Accuracy at 0.5867999792098999\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss at 0.04495524615049362 Validation Accuracy at 0.578999936580658\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss at 0.04986757040023804 Validation Accuracy at 0.5851998925209045\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss at 0.03252411261200905 Validation Accuracy at 0.5797998905181885\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss at 0.013971528969705105 Validation Accuracy at 0.5935999155044556\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss at 0.013187272474169731 Validation Accuracy at 0.5805999040603638\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss at 0.0123202595859766 Validation Accuracy at 0.5811998844146729\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss at 0.011803749948740005 Validation Accuracy at 0.5841999650001526\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss at 0.012192532420158386 Validation Accuracy at 0.5803999304771423\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss at 0.011339203454554081 Validation Accuracy at 0.5875999331474304\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss at 0.02053254470229149 Validation Accuracy at 0.571199893951416\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss at 0.01314869336783886 Validation Accuracy at 0.5769999027252197\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss at 0.012480944395065308 Validation Accuracy at 0.5909999012947083\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss at 0.01113603264093399 Validation Accuracy at 0.5917999148368835\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss at 0.010050566866993904 Validation Accuracy at 0.5811999440193176\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss at 0.007707015611231327 Validation Accuracy at 0.5813999176025391\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss at 0.053544968366622925 Validation Accuracy at 0.5883999466896057\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss at 0.014158880338072777 Validation Accuracy at 0.5821999311447144\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss at 0.008885703980922699 Validation Accuracy at 0.590799868106842\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss at 0.010231767781078815 Validation Accuracy at 0.5805999040603638\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss at 0.007844323292374611 Validation Accuracy at 0.5833998918533325\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss at 0.007847482338547707 Validation Accuracy at 0.5934000015258789\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss at 0.008571955375373363 Validation Accuracy at 0.5915998816490173\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss at 0.008832987397909164 Validation Accuracy at 0.5959998965263367\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss at 0.012532215565443039 Validation Accuracy at 0.5953999161720276\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss at 0.009936661459505558 Validation Accuracy at 0.5911999344825745\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss at 0.007106136530637741 Validation Accuracy at 0.5955999493598938\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss at 0.00785812083631754 Validation Accuracy at 0.5799999237060547\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss at 0.008020700886845589 Validation Accuracy at 0.5791999101638794\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss at 0.015603206120431423 Validation Accuracy at 0.5749999284744263\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss at 0.008771220222115517 Validation Accuracy at 0.5833999514579773\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss at 0.008872699923813343 Validation Accuracy at 0.572399914264679\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss at 0.011535818688571453 Validation Accuracy at 0.5791999101638794\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss at 0.008944020606577396 Validation Accuracy at 0.5781999230384827\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss at 0.008168808184564114 Validation Accuracy at 0.5763999223709106\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss at 0.007933569140732288 Validation Accuracy at 0.58079993724823\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss at 0.007024367805570364 Validation Accuracy at 0.5771999359130859\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss at 0.007969806902110577 Validation Accuracy at 0.5815999507904053\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss at 0.008256040513515472 Validation Accuracy at 0.5917999744415283\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss at 0.010113653726875782 Validation Accuracy at 0.5887998938560486\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss at 0.00818575918674469 Validation Accuracy at 0.5855998992919922\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss at 0.00848322082310915 Validation Accuracy at 0.5847999453544617\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss at 0.008388109505176544 Validation Accuracy at 0.5933998823165894\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss at 0.008567601442337036 Validation Accuracy at 0.5873998999595642\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss at 0.011432877741754055 Validation Accuracy at 0.5871999263763428\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss at 0.008527172729372978 Validation Accuracy at 0.5839998722076416\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss at 0.018602512776851654 Validation Accuracy at 0.5943999290466309\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss at 0.010116063058376312 Validation Accuracy at 0.5935999155044556\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss at 0.007767208386212587 Validation Accuracy at 0.5881999731063843\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss at 0.009506617672741413 Validation Accuracy at 0.5857999920845032\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss at 0.007462635170668364 Validation Accuracy at 0.5875998735427856\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss at 0.0075540728867053986 Validation Accuracy at 0.5879998803138733\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss at 0.00746917026117444 Validation Accuracy at 0.5873998999595642\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss at 0.007311148103326559 Validation Accuracy at 0.5929999351501465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249, CIFAR-10 Batch 1:  Loss at 0.004011364188045263 Validation Accuracy at 0.5847999453544617\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss at 0.00802762620151043 Validation Accuracy at 0.5753999948501587\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss at 0.0033053988590836525 Validation Accuracy at 0.5843998789787292\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss at 0.0027741272933781147 Validation Accuracy at 0.5861998796463013\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss at 0.0008023445261642337 Validation Accuracy at 0.5889999270439148\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss at 0.002069673500955105 Validation Accuracy at 0.5757999420166016\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss at 0.002496412955224514 Validation Accuracy at 0.5881999135017395\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss at 0.0019186630379408598 Validation Accuracy at 0.5903998613357544\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.3002126216888428 Validation Accuracy at 0.13539999723434448\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss at 2.3094215393066406 Validation Accuracy at 0.11959999054670334\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss at 2.1427831649780273 Validation Accuracy at 0.1775999665260315\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss at 2.1322412490844727 Validation Accuracy at 0.18859998881816864\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss at 2.0708281993865967 Validation Accuracy at 0.20819997787475586\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 2.1799933910369873 Validation Accuracy at 0.21359997987747192\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss at 1.9637820720672607 Validation Accuracy at 0.2703999876976013\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss at 1.727853775024414 Validation Accuracy at 0.26739999651908875\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss at 1.8339636325836182 Validation Accuracy at 0.29839998483657837\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss at 1.8537496328353882 Validation Accuracy at 0.30399999022483826\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 1.977596640586853 Validation Accuracy at 0.3173999786376953\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss at 1.8193525075912476 Validation Accuracy at 0.31779998540878296\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss at 1.5930880308151245 Validation Accuracy at 0.3206000030040741\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss at 1.7311029434204102 Validation Accuracy at 0.3317999839782715\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss at 1.7310616970062256 Validation Accuracy at 0.3319999873638153\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.8536564111709595 Validation Accuracy at 0.3417999744415283\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss at 1.6523549556732178 Validation Accuracy at 0.33319997787475586\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss at 1.5669218301773071 Validation Accuracy at 0.3677999973297119\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss at 1.6860817670822144 Validation Accuracy at 0.32979997992515564\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss at 1.5971603393554688 Validation Accuracy at 0.3553999662399292\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.7334051132202148 Validation Accuracy at 0.3643999695777893\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss at 1.6266005039215088 Validation Accuracy at 0.3657999634742737\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss at 1.5165660381317139 Validation Accuracy at 0.3725999593734741\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss at 1.5132689476013184 Validation Accuracy at 0.3928000032901764\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss at 1.5227324962615967 Validation Accuracy at 0.387999951839447\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.6286834478378296 Validation Accuracy at 0.4017999470233917\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss at 1.5311566591262817 Validation Accuracy at 0.4001999497413635\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss at 1.4517436027526855 Validation Accuracy at 0.41419997811317444\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss at 1.456629753112793 Validation Accuracy at 0.4195999503135681\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss at 1.468090295791626 Validation Accuracy at 0.40939998626708984\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.5479545593261719 Validation Accuracy at 0.42640000581741333\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss at 1.4945051670074463 Validation Accuracy at 0.4357999563217163\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss at 1.3947328329086304 Validation Accuracy at 0.43599995970726013\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss at 1.3828644752502441 Validation Accuracy at 0.4421999454498291\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss at 1.4151767492294312 Validation Accuracy at 0.45559993386268616\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.456239104270935 Validation Accuracy at 0.46939995884895325\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss at 1.3722562789916992 Validation Accuracy at 0.4695999324321747\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss at 1.3320481777191162 Validation Accuracy at 0.49299997091293335\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss at 1.2399736642837524 Validation Accuracy at 0.5251999497413635\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss at 1.2879854440689087 Validation Accuracy at 0.52239990234375\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 1.3733935356140137 Validation Accuracy at 0.5173999667167664\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss at 1.2299154996871948 Validation Accuracy at 0.525999903678894\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss at 1.2472573518753052 Validation Accuracy at 0.5341999530792236\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss at 1.1288011074066162 Validation Accuracy at 0.5367999076843262\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss at 1.1920030117034912 Validation Accuracy at 0.5521999001502991\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 1.2509278059005737 Validation Accuracy at 0.5517999529838562\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss at 1.2467067241668701 Validation Accuracy at 0.5180000066757202\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss at 1.1881940364837646 Validation Accuracy at 0.5523998737335205\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss at 1.102144479751587 Validation Accuracy at 0.5691999197006226\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss at 1.1815102100372314 Validation Accuracy at 0.5761998891830444\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 1.2206535339355469 Validation Accuracy at 0.5833998918533325\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss at 1.0006465911865234 Validation Accuracy at 0.5779999494552612\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss at 0.9748713970184326 Validation Accuracy at 0.5755999088287354\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss at 1.0491065979003906 Validation Accuracy at 0.5895999073982239\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss at 1.0899994373321533 Validation Accuracy at 0.5925998687744141\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 1.0297836065292358 Validation Accuracy at 0.5943999290466309\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss at 0.9351292848587036 Validation Accuracy at 0.5931999087333679\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss at 0.9540463089942932 Validation Accuracy at 0.590999960899353\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss at 1.0426274538040161 Validation Accuracy at 0.5939999222755432\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss at 0.9598643183708191 Validation Accuracy at 0.6069998741149902\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 0.9699732065200806 Validation Accuracy at 0.6007999181747437\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss at 0.9914364814758301 Validation Accuracy at 0.596799910068512\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss at 0.8313144445419312 Validation Accuracy at 0.597399890422821\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss at 0.9709982872009277 Validation Accuracy at 0.6269998550415039\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss at 0.8973307609558105 Validation Accuracy at 0.6115999221801758\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 0.9454330205917358 Validation Accuracy at 0.6121999025344849\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss at 0.832122266292572 Validation Accuracy at 0.6139999628067017\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss at 0.7770311236381531 Validation Accuracy at 0.6175998449325562\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss at 0.9455510973930359 Validation Accuracy at 0.6297998428344727\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss at 0.8096482753753662 Validation Accuracy at 0.6263998746871948\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 0.8746050000190735 Validation Accuracy at 0.6167998909950256\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss at 0.7866561412811279 Validation Accuracy at 0.626599907875061\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss at 0.6457440853118896 Validation Accuracy at 0.6267998814582825\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss at 0.811342716217041 Validation Accuracy at 0.6333999037742615\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss at 0.7614846229553223 Validation Accuracy at 0.6355998516082764\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 0.81201171875 Validation Accuracy at 0.6433998942375183\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss at 0.7058532238006592 Validation Accuracy at 0.6345999240875244\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss at 0.625673770904541 Validation Accuracy at 0.6345999240875244\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss at 0.7638381719589233 Validation Accuracy at 0.6381999254226685\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss at 0.7953663468360901 Validation Accuracy at 0.636199951171875\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 0.7090311646461487 Validation Accuracy at 0.6425999402999878\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss at 0.7562819123268127 Validation Accuracy at 0.6351999044418335\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss at 0.5226052403450012 Validation Accuracy at 0.6393998861312866\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss at 0.6797343492507935 Validation Accuracy at 0.6457998752593994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, CIFAR-10 Batch 5:  Loss at 0.7463095784187317 Validation Accuracy at 0.6487998962402344\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 0.7076627016067505 Validation Accuracy at 0.6463998556137085\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss at 0.6893535852432251 Validation Accuracy at 0.6525998711585999\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss at 0.5256953239440918 Validation Accuracy at 0.6475998163223267\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss at 0.6560143232345581 Validation Accuracy at 0.6581999063491821\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss at 0.6614621877670288 Validation Accuracy at 0.6525998711585999\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 0.7155206203460693 Validation Accuracy at 0.649199903011322\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss at 0.6708095669746399 Validation Accuracy at 0.6497999429702759\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss at 0.43547388911247253 Validation Accuracy at 0.6623998880386353\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss at 0.6549088954925537 Validation Accuracy at 0.6479998826980591\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss at 0.6119098663330078 Validation Accuracy at 0.649199903011322\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 0.7023148536682129 Validation Accuracy at 0.6501998901367188\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss at 0.6707834005355835 Validation Accuracy at 0.6433998942375183\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss at 0.4477963447570801 Validation Accuracy at 0.6541999578475952\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss at 0.6492288708686829 Validation Accuracy at 0.6537998914718628\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss at 0.6131854057312012 Validation Accuracy at 0.6555998921394348\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 0.6491698026657104 Validation Accuracy at 0.6577998399734497\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss at 0.6602487564086914 Validation Accuracy at 0.6469999551773071\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss at 0.44297388195991516 Validation Accuracy at 0.6519998908042908\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss at 0.5826046466827393 Validation Accuracy at 0.6611998677253723\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss at 0.5826730728149414 Validation Accuracy at 0.6405999064445496\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 0.64825439453125 Validation Accuracy at 0.6537999510765076\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss at 0.6180858016014099 Validation Accuracy at 0.6595998406410217\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss at 0.4796144962310791 Validation Accuracy at 0.6627999544143677\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss at 0.5269732475280762 Validation Accuracy at 0.6669999361038208\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss at 0.5515187978744507 Validation Accuracy at 0.6593998074531555\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 0.6326675415039062 Validation Accuracy at 0.6579998731613159\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss at 0.5988221168518066 Validation Accuracy at 0.6627998948097229\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss at 0.39957261085510254 Validation Accuracy at 0.6517998576164246\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss at 0.47131916880607605 Validation Accuracy at 0.6731998324394226\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss at 0.5338904857635498 Validation Accuracy at 0.6621999144554138\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 0.6020877361297607 Validation Accuracy at 0.6647999286651611\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss at 0.6126402020454407 Validation Accuracy at 0.6587998867034912\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss at 0.3886355459690094 Validation Accuracy at 0.6619998216629028\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss at 0.4351956844329834 Validation Accuracy at 0.6595998406410217\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss at 0.48061761260032654 Validation Accuracy at 0.6587998867034912\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 0.583017885684967 Validation Accuracy at 0.6535998582839966\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss at 0.4978225529193878 Validation Accuracy at 0.6589999198913574\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss at 0.39859795570373535 Validation Accuracy at 0.653799831867218\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss at 0.4316888153553009 Validation Accuracy at 0.6593998670578003\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss at 0.4797714948654175 Validation Accuracy at 0.6547998785972595\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 0.5746838450431824 Validation Accuracy at 0.6609999537467957\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss at 0.47990211844444275 Validation Accuracy at 0.6661999225616455\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss at 0.2616152763366699 Validation Accuracy at 0.674799919128418\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss at 0.42562445998191833 Validation Accuracy at 0.672799825668335\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss at 0.3710091710090637 Validation Accuracy at 0.6563999056816101\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 0.5308968424797058 Validation Accuracy at 0.6635998487472534\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss at 0.5419908165931702 Validation Accuracy at 0.663399875164032\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss at 0.36541417241096497 Validation Accuracy at 0.6639998555183411\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss at 0.3802317678928375 Validation Accuracy at 0.6611998677253723\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss at 0.34928345680236816 Validation Accuracy at 0.6627998948097229\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 0.4795113205909729 Validation Accuracy at 0.6627998948097229\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss at 0.4368007183074951 Validation Accuracy at 0.663399875164032\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss at 0.3119105100631714 Validation Accuracy at 0.6659998893737793\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss at 0.3474941551685333 Validation Accuracy at 0.66159987449646\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss at 0.4542916417121887 Validation Accuracy at 0.6629998683929443\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 0.4458625316619873 Validation Accuracy at 0.6707999110221863\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss at 0.44818776845932007 Validation Accuracy at 0.6665998697280884\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss at 0.2647019028663635 Validation Accuracy at 0.6731998920440674\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss at 0.3445468246936798 Validation Accuracy at 0.6709998846054077\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss at 0.37997281551361084 Validation Accuracy at 0.6691998839378357\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 0.4706060290336609 Validation Accuracy at 0.6643998622894287\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss at 0.37363165616989136 Validation Accuracy at 0.672799825668335\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss at 0.278359591960907 Validation Accuracy at 0.6757999062538147\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss at 0.3369380235671997 Validation Accuracy at 0.6667999029159546\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss at 0.3268423080444336 Validation Accuracy at 0.6671999096870422\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 0.3836990296840668 Validation Accuracy at 0.6741998791694641\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss at 0.36245664954185486 Validation Accuracy at 0.6763999462127686\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss at 0.30457937717437744 Validation Accuracy at 0.6711999177932739\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss at 0.3129004240036011 Validation Accuracy at 0.6703999042510986\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss at 0.27706050872802734 Validation Accuracy at 0.678199827671051\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 0.4401262402534485 Validation Accuracy at 0.6649998426437378\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss at 0.38200366497039795 Validation Accuracy at 0.6709998846054077\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss at 0.21531060338020325 Validation Accuracy at 0.6701998710632324\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss at 0.28283369541168213 Validation Accuracy at 0.6761998534202576\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss at 0.25066378712654114 Validation Accuracy at 0.6675998568534851\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 0.3430057764053345 Validation Accuracy at 0.6647999286651611\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss at 0.3969781696796417 Validation Accuracy at 0.6571998596191406\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss at 0.2703104615211487 Validation Accuracy at 0.6751999258995056\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss at 0.31691259145736694 Validation Accuracy at 0.6697999238967896\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss at 0.2782908082008362 Validation Accuracy at 0.6713998913764954\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 0.3263911306858063 Validation Accuracy at 0.6661999225616455\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss at 0.35068124532699585 Validation Accuracy at 0.6767998933792114\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss at 0.20366816222667694 Validation Accuracy at 0.6801998615264893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, CIFAR-10 Batch 4:  Loss at 0.2768644094467163 Validation Accuracy at 0.6677998900413513\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss at 0.25696617364883423 Validation Accuracy at 0.6647998690605164\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 0.33883121609687805 Validation Accuracy at 0.6629998683929443\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss at 0.3159920871257782 Validation Accuracy at 0.6701998710632324\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss at 0.21811333298683167 Validation Accuracy at 0.6743998527526855\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss at 0.3021669387817383 Validation Accuracy at 0.6647999286651611\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss at 0.249687060713768 Validation Accuracy at 0.6725997924804688\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 0.2928048372268677 Validation Accuracy at 0.671799898147583\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss at 0.283382385969162 Validation Accuracy at 0.675399899482727\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss at 0.1774141937494278 Validation Accuracy at 0.6707998514175415\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss at 0.23757027089595795 Validation Accuracy at 0.6715998649597168\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss at 0.21016259491443634 Validation Accuracy at 0.678399920463562\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 0.22197529673576355 Validation Accuracy at 0.6737998723983765\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss at 0.3159751892089844 Validation Accuracy at 0.6777998208999634\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss at 0.14998051524162292 Validation Accuracy at 0.6797998547554016\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss at 0.30048835277557373 Validation Accuracy at 0.6675999164581299\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss at 0.20078183710575104 Validation Accuracy at 0.6751999855041504\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 0.24491673707962036 Validation Accuracy at 0.6709998846054077\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss at 0.2896602749824524 Validation Accuracy at 0.6677999496459961\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss at 0.17185497283935547 Validation Accuracy at 0.6771998405456543\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss at 0.23337648808956146 Validation Accuracy at 0.6779999136924744\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss at 0.18626898527145386 Validation Accuracy at 0.672799825668335\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 0.24916905164718628 Validation Accuracy at 0.6655998229980469\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss at 0.2538301944732666 Validation Accuracy at 0.6773998737335205\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss at 0.18981173634529114 Validation Accuracy at 0.671799898147583\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss at 0.25525808334350586 Validation Accuracy at 0.6747998595237732\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss at 0.2441166639328003 Validation Accuracy at 0.6729998588562012\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 0.2929021716117859 Validation Accuracy at 0.6725999712944031\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss at 0.2429817169904709 Validation Accuracy at 0.6703999042510986\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss at 0.13480244576931 Validation Accuracy at 0.671799898147583\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss at 0.1588241159915924 Validation Accuracy at 0.6787998676300049\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss at 0.23922908306121826 Validation Accuracy at 0.6629999279975891\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 0.21719099581241608 Validation Accuracy at 0.6741998791694641\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss at 0.2690044045448303 Validation Accuracy at 0.668199896812439\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss at 0.14465811848640442 Validation Accuracy at 0.6795998811721802\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss at 0.1950431913137436 Validation Accuracy at 0.6759998798370361\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss at 0.1929631531238556 Validation Accuracy at 0.6683998703956604\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 0.13921961188316345 Validation Accuracy at 0.6697998642921448\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss at 0.27078914642333984 Validation Accuracy at 0.6767998933792114\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss at 0.17357248067855835 Validation Accuracy at 0.6695998907089233\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss at 0.20952168107032776 Validation Accuracy at 0.6791998744010925\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss at 0.1772099733352661 Validation Accuracy at 0.6757998466491699\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 0.18249978125095367 Validation Accuracy at 0.6639999151229858\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss at 0.340139776468277 Validation Accuracy at 0.6649998426437378\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss at 0.15189826488494873 Validation Accuracy at 0.6789998412132263\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss at 0.22321072220802307 Validation Accuracy at 0.6605998873710632\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss at 0.13832896947860718 Validation Accuracy at 0.6739999055862427\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 0.20121584832668304 Validation Accuracy at 0.6755998730659485\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss at 0.26820263266563416 Validation Accuracy at 0.6667999029159546\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss at 0.14089903235435486 Validation Accuracy at 0.6761997938156128\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss at 0.16703180968761444 Validation Accuracy at 0.6763998866081238\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss at 0.1394854485988617 Validation Accuracy at 0.6749998331069946\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 0.19118261337280273 Validation Accuracy at 0.6701999306678772\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss at 0.27077817916870117 Validation Accuracy at 0.6645998954772949\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss at 0.12581582367420197 Validation Accuracy at 0.6755999326705933\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss at 0.19196943938732147 Validation Accuracy at 0.6743998527526855\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss at 0.0808180570602417 Validation Accuracy at 0.6829997897148132\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 0.2011997103691101 Validation Accuracy at 0.6747998595237732\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss at 0.29609814286231995 Validation Accuracy at 0.6751999258995056\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss at 0.17193548381328583 Validation Accuracy at 0.6765998005867004\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss at 0.1704774796962738 Validation Accuracy at 0.6689999103546143\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss at 0.10702881217002869 Validation Accuracy at 0.6721998453140259\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 0.12426833063364029 Validation Accuracy at 0.6777998805046082\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss at 0.26216188073158264 Validation Accuracy at 0.6675998568534851\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss at 0.11082616448402405 Validation Accuracy at 0.6743998527526855\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss at 0.16860975325107574 Validation Accuracy at 0.6685998439788818\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss at 0.11031162738800049 Validation Accuracy at 0.6737998127937317\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 0.14239785075187683 Validation Accuracy at 0.6777998805046082\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss at 0.2485966980457306 Validation Accuracy at 0.6755999326705933\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss at 0.10950297117233276 Validation Accuracy at 0.6689999103546143\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss at 0.14284127950668335 Validation Accuracy at 0.6707998514175415\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss at 0.12174223363399506 Validation Accuracy at 0.6661998629570007\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 0.12466645985841751 Validation Accuracy at 0.6699999570846558\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss at 0.2884596884250641 Validation Accuracy at 0.674599826335907\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss at 0.10253563523292542 Validation Accuracy at 0.6735998392105103\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss at 0.12042118608951569 Validation Accuracy at 0.6741999387741089\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss at 0.08293978869915009 Validation Accuracy at 0.6711998581886292\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 0.15997202694416046 Validation Accuracy at 0.6675998568534851\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss at 0.2368791699409485 Validation Accuracy at 0.6715998649597168\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss at 0.09877359122037888 Validation Accuracy at 0.6745998859405518\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss at 0.10620319098234177 Validation Accuracy at 0.6807999014854431\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss at 0.0975990742444992 Validation Accuracy at 0.6791998744010925\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 0.12240077555179596 Validation Accuracy at 0.6701998710632324\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss at 0.22831402719020844 Validation Accuracy at 0.6783998608589172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, CIFAR-10 Batch 3:  Loss at 0.12002722173929214 Validation Accuracy at 0.6721998453140259\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss at 0.11637385189533234 Validation Accuracy at 0.6779998540878296\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss at 0.0818629264831543 Validation Accuracy at 0.6751998662948608\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 0.07564295083284378 Validation Accuracy at 0.6731998920440674\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss at 0.2241014540195465 Validation Accuracy at 0.6719998717308044\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss at 0.0940650999546051 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss at 0.09450574964284897 Validation Accuracy at 0.6763998866081238\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss at 0.08249437808990479 Validation Accuracy at 0.6757998466491699\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 0.08285760134458542 Validation Accuracy at 0.6711999177932739\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss at 0.265787273645401 Validation Accuracy at 0.668199896812439\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss at 0.08112683147192001 Validation Accuracy at 0.6805998682975769\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss at 0.06447887420654297 Validation Accuracy at 0.6789998412132263\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss at 0.07972681522369385 Validation Accuracy at 0.6779998540878296\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 0.045102737843990326 Validation Accuracy at 0.6769998669624329\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss at 0.22639384865760803 Validation Accuracy at 0.6825999617576599\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss at 0.0971679762005806 Validation Accuracy at 0.684199869632721\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss at 0.07899150252342224 Validation Accuracy at 0.6861998438835144\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss at 0.10719707608222961 Validation Accuracy at 0.6779999136924744\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 0.10217542946338654 Validation Accuracy at 0.681199848651886\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss at 0.21585486829280853 Validation Accuracy at 0.6821998357772827\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss at 0.07512609660625458 Validation Accuracy at 0.6869997978210449\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss at 0.07026568055152893 Validation Accuracy at 0.6821998953819275\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss at 0.05585065111517906 Validation Accuracy at 0.6765998601913452\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 0.11512051522731781 Validation Accuracy at 0.6735998392105103\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss at 0.1883678436279297 Validation Accuracy at 0.6885999441146851\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss at 0.1070263683795929 Validation Accuracy at 0.6795998811721802\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss at 0.09267798811197281 Validation Accuracy at 0.6757998466491699\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss at 0.054359473288059235 Validation Accuracy at 0.6807998418807983\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 0.10704975575208664 Validation Accuracy at 0.6719998717308044\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss at 0.2445310801267624 Validation Accuracy at 0.6741998791694641\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss at 0.0886819064617157 Validation Accuracy at 0.6849998831748962\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss at 0.08579349517822266 Validation Accuracy at 0.6775998473167419\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss at 0.05663635581731796 Validation Accuracy at 0.676399827003479\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 0.09072503447532654 Validation Accuracy at 0.6689999103546143\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss at 0.18863588571548462 Validation Accuracy at 0.6693999171257019\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss at 0.09447146207094193 Validation Accuracy at 0.6795998215675354\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss at 0.08012048900127411 Validation Accuracy at 0.6741998791694641\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss at 0.04663178324699402 Validation Accuracy at 0.6823998689651489\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 0.11918847262859344 Validation Accuracy at 0.6767998933792114\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss at 0.22364716231822968 Validation Accuracy at 0.6819998025894165\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss at 0.08725769072771072 Validation Accuracy at 0.6833999156951904\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss at 0.06990371644496918 Validation Accuracy at 0.6815997958183289\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss at 0.0714574083685875 Validation Accuracy at 0.684199869632721\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 0.10988146811723709 Validation Accuracy at 0.6795998811721802\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss at 0.2071782499551773 Validation Accuracy at 0.6725998520851135\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss at 0.07064201682806015 Validation Accuracy at 0.6801998615264893\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss at 0.06738870590925217 Validation Accuracy at 0.6817998886108398\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss at 0.0655057355761528 Validation Accuracy at 0.6861999034881592\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 0.12364731729030609 Validation Accuracy at 0.6703999042510986\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss at 0.2246139794588089 Validation Accuracy at 0.6825998425483704\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss at 0.07306791841983795 Validation Accuracy at 0.690599799156189\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss at 0.07612816244363785 Validation Accuracy at 0.6803999543190002\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss at 0.06682193279266357 Validation Accuracy at 0.6831998229026794\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 0.07446970790624619 Validation Accuracy at 0.6839998960494995\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss at 0.21845370531082153 Validation Accuracy at 0.6831998825073242\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss at 0.06693791598081589 Validation Accuracy at 0.6813998818397522\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss at 0.09769931435585022 Validation Accuracy at 0.6751998662948608\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss at 0.03924939036369324 Validation Accuracy at 0.6827998757362366\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 0.1434294581413269 Validation Accuracy at 0.6791998744010925\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss at 0.20822645723819733 Validation Accuracy at 0.6829997897148132\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss at 0.1011497750878334 Validation Accuracy at 0.6783998608589172\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss at 0.07293615490198135 Validation Accuracy at 0.6883998513221741\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss at 0.04575328901410103 Validation Accuracy at 0.6775999069213867\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 0.08290628343820572 Validation Accuracy at 0.6725999116897583\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss at 0.21308645606040955 Validation Accuracy at 0.675399899482727\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss at 0.08296994864940643 Validation Accuracy at 0.684999942779541\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss at 0.07024326920509338 Validation Accuracy at 0.6853998303413391\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss at 0.03564290702342987 Validation Accuracy at 0.6807998418807983\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 0.11666381359100342 Validation Accuracy at 0.6821998357772827\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss at 0.23130065202713013 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss at 0.05079985409975052 Validation Accuracy at 0.6823998689651489\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss at 0.034173138439655304 Validation Accuracy at 0.6743998527526855\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss at 0.04778774082660675 Validation Accuracy at 0.6845998764038086\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 0.11458723247051239 Validation Accuracy at 0.6695998907089233\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss at 0.21962033212184906 Validation Accuracy at 0.6855999231338501\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss at 0.06578509509563446 Validation Accuracy at 0.6869999170303345\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss at 0.05401848256587982 Validation Accuracy at 0.6837998032569885\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss at 0.05495871603488922 Validation Accuracy at 0.6841998100280762\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 0.08567503839731216 Validation Accuracy at 0.6775997877120972\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss at 0.1960214227437973 Validation Accuracy at 0.6877998113632202\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss at 0.06456144899129868 Validation Accuracy at 0.6827998161315918\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss at 0.05784568935632706 Validation Accuracy at 0.6865999102592468\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss at 0.06616280227899551 Validation Accuracy at 0.6851998567581177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 0.10723760724067688 Validation Accuracy at 0.6809998750686646\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss at 0.13690105080604553 Validation Accuracy at 0.6823999285697937\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss at 0.0694722831249237 Validation Accuracy at 0.6827998757362366\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss at 0.04137227311730385 Validation Accuracy at 0.6765998601913452\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss at 0.04905541241168976 Validation Accuracy at 0.6815999150276184\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 0.10901496559381485 Validation Accuracy at 0.6733998656272888\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss at 0.12466796487569809 Validation Accuracy at 0.6821998357772827\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss at 0.07648016512393951 Validation Accuracy at 0.681199848651886\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss at 0.05290418863296509 Validation Accuracy at 0.6805998682975769\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss at 0.08502690494060516 Validation Accuracy at 0.6849997639656067\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 0.11117767542600632 Validation Accuracy at 0.6805998682975769\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss at 0.13222023844718933 Validation Accuracy at 0.6891999244689941\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss at 0.06475149840116501 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss at 0.04481550306081772 Validation Accuracy at 0.6891998052597046\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss at 0.07502204924821854 Validation Accuracy at 0.6857998967170715\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 0.04873058944940567 Validation Accuracy at 0.6791999340057373\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss at 0.1359662562608719 Validation Accuracy at 0.675399899482727\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss at 0.06439174711704254 Validation Accuracy at 0.6737998723983765\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss at 0.04492711275815964 Validation Accuracy at 0.6795998811721802\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss at 0.07772871851921082 Validation Accuracy at 0.6847999095916748\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 0.06792982667684555 Validation Accuracy at 0.681999921798706\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss at 0.14019694924354553 Validation Accuracy at 0.6809998750686646\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss at 0.07810167223215103 Validation Accuracy at 0.6839998364448547\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss at 0.07508620619773865 Validation Accuracy at 0.6749998331069946\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss at 0.05904646962881088 Validation Accuracy at 0.6905998587608337\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 0.05503076687455177 Validation Accuracy at 0.6863999366760254\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss at 0.2093926966190338 Validation Accuracy at 0.679399847984314\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss at 0.05674655735492706 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss at 0.04377415403723717 Validation Accuracy at 0.681199848651886\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss at 0.053031645715236664 Validation Accuracy at 0.6773998737335205\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 0.09903042018413544 Validation Accuracy at 0.678399920463562\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss at 0.13319909572601318 Validation Accuracy at 0.687799870967865\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss at 0.06858635693788528 Validation Accuracy at 0.678399920463562\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss at 0.043519455939531326 Validation Accuracy at 0.6867998838424683\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss at 0.06844660639762878 Validation Accuracy at 0.6789999604225159\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 0.07181791216135025 Validation Accuracy at 0.6831998825073242\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss at 0.14670027792453766 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss at 0.08879795670509338 Validation Accuracy at 0.6743997931480408\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss at 0.07344306260347366 Validation Accuracy at 0.6777998805046082\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss at 0.05661911517381668 Validation Accuracy at 0.6775999069213867\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 0.07043951004743576 Validation Accuracy at 0.6841999292373657\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss at 0.15520228445529938 Validation Accuracy at 0.686599850654602\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss at 0.06015843152999878 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss at 0.06450603157281876 Validation Accuracy at 0.6835998892784119\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss at 0.05263068899512291 Validation Accuracy at 0.6797998547554016\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 0.06742613762617111 Validation Accuracy at 0.6855999231338501\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss at 0.12123551964759827 Validation Accuracy at 0.6831998825073242\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss at 0.06969501078128815 Validation Accuracy at 0.6767998933792114\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss at 0.03682190179824829 Validation Accuracy at 0.6875998973846436\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss at 0.05167907476425171 Validation Accuracy at 0.6875998973846436\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 0.06936267018318176 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss at 0.21335254609584808 Validation Accuracy at 0.6817998290061951\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss at 0.07164206355810165 Validation Accuracy at 0.6891999244689941\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss at 0.03771470487117767 Validation Accuracy at 0.6859998106956482\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss at 0.05588270351290703 Validation Accuracy at 0.6913999319076538\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 0.034328609704971313 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss at 0.14653056859970093 Validation Accuracy at 0.6865999102592468\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss at 0.09315953403711319 Validation Accuracy at 0.6871998310089111\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss at 0.04129834845662117 Validation Accuracy at 0.6861999034881592\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss at 0.06779280304908752 Validation Accuracy at 0.6815999150276184\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 0.02682291716337204 Validation Accuracy at 0.6787998676300049\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss at 0.11854548752307892 Validation Accuracy at 0.6831998229026794\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss at 0.0666642040014267 Validation Accuracy at 0.6935998797416687\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss at 0.025427939370274544 Validation Accuracy at 0.6831998825073242\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss at 0.07256325334310532 Validation Accuracy at 0.6765998601913452\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 0.11715144664049149 Validation Accuracy at 0.6853998303413391\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss at 0.11166166514158249 Validation Accuracy at 0.680199921131134\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss at 0.05804147571325302 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss at 0.029511772096157074 Validation Accuracy at 0.6747998595237732\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss at 0.04632936045527458 Validation Accuracy at 0.6837999224662781\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 0.09834768623113632 Validation Accuracy at 0.6809998750686646\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss at 0.11878705769777298 Validation Accuracy at 0.6821998357772827\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss at 0.054236698895692825 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss at 0.02011912129819393 Validation Accuracy at 0.6717998385429382\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss at 0.052726179361343384 Validation Accuracy at 0.6845998764038086\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 0.09860071539878845 Validation Accuracy at 0.6741998791694641\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss at 0.12289275974035263 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss at 0.06518770009279251 Validation Accuracy at 0.678399920463562\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss at 0.014815480448305607 Validation Accuracy at 0.6817998886108398\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss at 0.06213243678212166 Validation Accuracy at 0.6761998534202576\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 0.037798285484313965 Validation Accuracy at 0.6775998473167419\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss at 0.1271982043981552 Validation Accuracy at 0.6823998689651489\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss at 0.061758629977703094 Validation Accuracy at 0.686599850654602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, CIFAR-10 Batch 4:  Loss at 0.021653486415743828 Validation Accuracy at 0.6815999150276184\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss at 0.03466176986694336 Validation Accuracy at 0.6799998879432678\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 0.0507698729634285 Validation Accuracy at 0.6767998933792114\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss at 0.12761101126670837 Validation Accuracy at 0.6807999014854431\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss at 0.060865290462970734 Validation Accuracy at 0.684199869632721\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss at 0.02167784422636032 Validation Accuracy at 0.6813998222351074\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss at 0.02431539073586464 Validation Accuracy at 0.6855998039245605\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 0.021584060043096542 Validation Accuracy at 0.6783998608589172\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss at 0.0943068116903305 Validation Accuracy at 0.6839998960494995\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss at 0.06711257249116898 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss at 0.021676093339920044 Validation Accuracy at 0.6753998398780823\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss at 0.0293582733720541 Validation Accuracy at 0.6823998689651489\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 0.0161003265529871 Validation Accuracy at 0.6843998432159424\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss at 0.12816452980041504 Validation Accuracy at 0.6821998953819275\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss at 0.048709698021411896 Validation Accuracy at 0.6869998574256897\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss at 0.022953620180487633 Validation Accuracy at 0.6763998866081238\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss at 0.03580266982316971 Validation Accuracy at 0.687799870967865\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 0.007491425145417452 Validation Accuracy at 0.6869997978210449\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss at 0.11800825595855713 Validation Accuracy at 0.679399847984314\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss at 0.09157010912895203 Validation Accuracy at 0.6823999285697937\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss at 0.029427720233798027 Validation Accuracy at 0.6769998669624329\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss at 0.03667224943637848 Validation Accuracy at 0.6825998425483704\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 0.01756824180483818 Validation Accuracy at 0.6767999529838562\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss at 0.14980147778987885 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss at 0.06368912011384964 Validation Accuracy at 0.6769999265670776\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss at 0.019921336323022842 Validation Accuracy at 0.6863998770713806\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss at 0.030034007504582405 Validation Accuracy at 0.6871998906135559\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 0.008240651339292526 Validation Accuracy at 0.6797998547554016\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss at 0.11114127188920975 Validation Accuracy at 0.6797999739646912\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss at 0.030962347984313965 Validation Accuracy at 0.6827998161315918\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss at 0.010695911943912506 Validation Accuracy at 0.6767998933792114\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss at 0.0283773485571146 Validation Accuracy at 0.6843998432159424\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 0.007091129198670387 Validation Accuracy at 0.6927998065948486\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss at 0.09826623648405075 Validation Accuracy at 0.6821998953819275\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss at 0.07428473979234695 Validation Accuracy at 0.6859999299049377\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss at 0.029534962028265 Validation Accuracy at 0.68479984998703\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss at 0.032684437930583954 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 0.011089658364653587 Validation Accuracy at 0.6867998242378235\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss at 0.10001546889543533 Validation Accuracy at 0.679599940776825\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss at 0.0394434854388237 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss at 0.013124505057930946 Validation Accuracy at 0.686599850654602\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss at 0.04771745204925537 Validation Accuracy at 0.6799998879432678\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 0.04280807077884674 Validation Accuracy at 0.6855998635292053\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss at 0.08544657379388809 Validation Accuracy at 0.684199869632721\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss at 0.06687921285629272 Validation Accuracy at 0.686599850654602\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss at 0.0364312008023262 Validation Accuracy at 0.6809999346733093\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss at 0.02677082270383835 Validation Accuracy at 0.6817998886108398\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 0.039223812520504 Validation Accuracy at 0.6781998872756958\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss at 0.09244342893362045 Validation Accuracy at 0.6785998344421387\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss at 0.037682320922613144 Validation Accuracy at 0.6771999001502991\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss at 0.03143579512834549 Validation Accuracy at 0.6849998235702515\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss at 0.017872700467705727 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 0.02868255041539669 Validation Accuracy at 0.6883997917175293\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss at 0.13485588133335114 Validation Accuracy at 0.6899999380111694\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss at 0.038089118897914886 Validation Accuracy at 0.6913998126983643\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss at 0.022281158715486526 Validation Accuracy at 0.6851998567581177\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss at 0.058412596583366394 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 0.06476003676652908 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss at 0.13497819006443024 Validation Accuracy at 0.6857998967170715\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss at 0.03195183351635933 Validation Accuracy at 0.687799870967865\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss at 0.0168246328830719 Validation Accuracy at 0.6821998357772827\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss at 0.016258370131254196 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 0.018630022183060646 Validation Accuracy at 0.6851999163627625\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss at 0.10330294072628021 Validation Accuracy at 0.6857998371124268\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss at 0.0388752818107605 Validation Accuracy at 0.6867998838424683\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss at 0.025091826915740967 Validation Accuracy at 0.6833998560905457\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss at 0.008388843387365341 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 0.015704141929745674 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss at 0.10415830463171005 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss at 0.07202444225549698 Validation Accuracy at 0.6871998906135559\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss at 0.014617915265262127 Validation Accuracy at 0.6815998554229736\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss at 0.021797575056552887 Validation Accuracy at 0.6885998249053955\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 0.029925765469670296 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss at 0.1407567709684372 Validation Accuracy at 0.6775999069213867\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss at 0.02949066087603569 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss at 0.00880482979118824 Validation Accuracy at 0.6849998235702515\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss at 0.016942620277404785 Validation Accuracy at 0.686599850654602\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 0.03104075789451599 Validation Accuracy at 0.6867998838424683\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss at 0.1062852218747139 Validation Accuracy at 0.686599850654602\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss at 0.025590166449546814 Validation Accuracy at 0.6857998967170715\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss at 0.027868660166859627 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss at 0.02057633362710476 Validation Accuracy at 0.6841999292373657\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss at 0.02623937278985977 Validation Accuracy at 0.6897998452186584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, CIFAR-10 Batch 2:  Loss at 0.10354313999414444 Validation Accuracy at 0.691399872303009\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss at 0.024511586874723434 Validation Accuracy at 0.6863998174667358\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss at 0.014477549120783806 Validation Accuracy at 0.6795998811721802\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss at 0.017837954685091972 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss at 0.014204474166035652 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss at 0.09366749972105026 Validation Accuracy at 0.6831998825073242\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss at 0.04345496743917465 Validation Accuracy at 0.6893998980522156\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss at 0.014597953297197819 Validation Accuracy at 0.6777998208999634\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss at 0.017025548964738846 Validation Accuracy at 0.685999870300293\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss at 0.014159712940454483 Validation Accuracy at 0.6881998181343079\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss at 0.11265712231397629 Validation Accuracy at 0.6891998052597046\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss at 0.026901066303253174 Validation Accuracy at 0.6883998513221741\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss at 0.021261505782604218 Validation Accuracy at 0.6857998371124268\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss at 0.03507960960268974 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss at 0.010751638561487198 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss at 0.1044614315032959 Validation Accuracy at 0.6927998065948486\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss at 0.03362677991390228 Validation Accuracy at 0.6899998784065247\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss at 0.021929144859313965 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss at 0.016830673441290855 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss at 0.015528065152466297 Validation Accuracy at 0.6835998892784119\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss at 0.09938641637563705 Validation Accuracy at 0.6919998526573181\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss at 0.05545606091618538 Validation Accuracy at 0.6897998452186584\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss at 0.021940000355243683 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss at 0.028931215405464172 Validation Accuracy at 0.6819998621940613\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss at 0.02165508270263672 Validation Accuracy at 0.6879999041557312\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss at 0.10487488657236099 Validation Accuracy at 0.6833999156951904\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss at 0.035573724657297134 Validation Accuracy at 0.692599892616272\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss at 0.0110098235309124 Validation Accuracy at 0.6829999089241028\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss at 0.014297847636044025 Validation Accuracy at 0.6819998621940613\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss at 0.025305040180683136 Validation Accuracy at 0.6865999102592468\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss at 0.10305321961641312 Validation Accuracy at 0.6809998750686646\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss at 0.05853498727083206 Validation Accuracy at 0.6823998689651489\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss at 0.010179217904806137 Validation Accuracy at 0.6921997666358948\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss at 0.008684837259352207 Validation Accuracy at 0.684199869632721\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss at 0.017317771911621094 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss at 0.10161254554986954 Validation Accuracy at 0.6801998615264893\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss at 0.027683250606060028 Validation Accuracy at 0.6869998574256897\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss at 0.008634502068161964 Validation Accuracy at 0.6809999346733093\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss at 0.009101947769522667 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss at 0.028566371649503708 Validation Accuracy at 0.6857998967170715\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss at 0.08024465292692184 Validation Accuracy at 0.694399893283844\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss at 0.032354600727558136 Validation Accuracy at 0.6883998513221741\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss at 0.01184549555182457 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss at 0.004329472780227661 Validation Accuracy at 0.6883999109268188\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss at 0.008514714427292347 Validation Accuracy at 0.6859999299049377\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss at 0.07588175684213638 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss at 0.016565866768360138 Validation Accuracy at 0.6893998980522156\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss at 0.014787858352065086 Validation Accuracy at 0.6839998960494995\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss at 0.011429733596742153 Validation Accuracy at 0.6809998750686646\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss at 0.014272999949753284 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss at 0.07514292001724243 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss at 0.021993448957800865 Validation Accuracy at 0.6893998980522156\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss at 0.013569749891757965 Validation Accuracy at 0.6811999082565308\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss at 0.013294391334056854 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss at 0.02328590117394924 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss at 0.08974383026361465 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss at 0.031287483870983124 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss at 0.009556867182254791 Validation Accuracy at 0.6817998290061951\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss at 0.012871088460087776 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss at 0.017733801156282425 Validation Accuracy at 0.6869998574256897\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss at 0.07426955550909042 Validation Accuracy at 0.6821998357772827\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss at 0.021938182413578033 Validation Accuracy at 0.6843998432159424\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss at 0.011068125255405903 Validation Accuracy at 0.6905998587608337\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss at 0.014175184071063995 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss at 0.03777336701750755 Validation Accuracy at 0.6905999183654785\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss at 0.06576124578714371 Validation Accuracy at 0.692599892616272\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss at 0.030857300385832787 Validation Accuracy at 0.6869998574256897\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss at 0.02077973447740078 Validation Accuracy at 0.6905998587608337\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss at 0.011530508287250996 Validation Accuracy at 0.6871998310089111\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss at 0.015295461751520634 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss at 0.07908936589956284 Validation Accuracy at 0.6927998065948486\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss at 0.017392218112945557 Validation Accuracy at 0.687799870967865\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss at 0.010930296964943409 Validation Accuracy at 0.6843999028205872\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss at 0.007248607464134693 Validation Accuracy at 0.6897997856140137\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss at 0.033597029745578766 Validation Accuracy at 0.6785998940467834\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss at 0.06407900154590607 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss at 0.03644412010908127 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss at 0.02722371742129326 Validation Accuracy at 0.678399920463562\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss at 0.005574266891926527 Validation Accuracy at 0.6937999129295349\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss at 0.019933944568037987 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss at 0.07453592866659164 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss at 0.010646088048815727 Validation Accuracy at 0.6919999122619629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117, CIFAR-10 Batch 4:  Loss at 0.0062144058756530285 Validation Accuracy at 0.6869997978210449\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss at 0.002561639528721571 Validation Accuracy at 0.6921999454498291\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss at 0.015923980623483658 Validation Accuracy at 0.6891998052597046\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss at 0.08597734570503235 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss at 0.03336692601442337 Validation Accuracy at 0.6905998587608337\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss at 0.004699581302702427 Validation Accuracy at 0.6851999163627625\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss at 0.022142168134450912 Validation Accuracy at 0.6899999380111694\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss at 0.012910221703350544 Validation Accuracy at 0.6919999122619629\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss at 0.07747200131416321 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss at 0.03496154397726059 Validation Accuracy at 0.6871998906135559\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss at 0.008040213026106358 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss at 0.009131400845944881 Validation Accuracy at 0.6885998249053955\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss at 0.012999466620385647 Validation Accuracy at 0.6859999299049377\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss at 0.06835152953863144 Validation Accuracy at 0.6919998526573181\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss at 0.027201427146792412 Validation Accuracy at 0.6893998980522156\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss at 0.005679081194102764 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss at 0.007033255882561207 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss at 0.011708317324519157 Validation Accuracy at 0.6919998526573181\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss at 0.08831474930047989 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss at 0.018479157239198685 Validation Accuracy at 0.6899998784065247\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss at 0.007969259284436703 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss at 0.01049486082047224 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss at 0.034242261201143265 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss at 0.08605567365884781 Validation Accuracy at 0.6895999312400818\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss at 0.015271897427737713 Validation Accuracy at 0.6903998255729675\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss at 0.013821825385093689 Validation Accuracy at 0.6897999048233032\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss at 0.0035875977482646704 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss at 0.041783880442380905 Validation Accuracy at 0.6825998425483704\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss at 0.08233454078435898 Validation Accuracy at 0.6913999319076538\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss at 0.021468618884682655 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss at 0.004337805788964033 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss at 0.002157822484150529 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss at 0.04259238764643669 Validation Accuracy at 0.6845998764038086\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss at 0.09283660352230072 Validation Accuracy at 0.6935998201370239\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss at 0.019373256713151932 Validation Accuracy at 0.6869999170303345\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss at 0.016767313703894615 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss at 0.003394442144781351 Validation Accuracy at 0.6951999068260193\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss at 0.012745744548738003 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss at 0.06997942924499512 Validation Accuracy at 0.6895999312400818\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss at 0.013976888731122017 Validation Accuracy at 0.6871998310089111\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss at 0.010278645902872086 Validation Accuracy at 0.6855998635292053\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss at 0.01037236861884594 Validation Accuracy at 0.6921998858451843\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss at 0.019749030470848083 Validation Accuracy at 0.6911998391151428\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss at 0.10369857400655746 Validation Accuracy at 0.687799870967865\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss at 0.04875707998871803 Validation Accuracy at 0.6857998967170715\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss at 0.011932541616261005 Validation Accuracy at 0.6869999170303345\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss at 0.0024053221568465233 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss at 0.012457586824893951 Validation Accuracy at 0.693199872970581\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss at 0.08940199017524719 Validation Accuracy at 0.6865999102592468\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss at 0.017863934859633446 Validation Accuracy at 0.6881998777389526\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss at 0.025240883231163025 Validation Accuracy at 0.6853999495506287\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss at 0.00618811696767807 Validation Accuracy at 0.6901998519897461\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss at 0.01798674277961254 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss at 0.07030696421861649 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss at 0.016858257353305817 Validation Accuracy at 0.693199872970581\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss at 0.013098610565066338 Validation Accuracy at 0.6985999345779419\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss at 0.0020229830406606197 Validation Accuracy at 0.6861998438835144\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss at 0.012047050520777702 Validation Accuracy at 0.6927998661994934\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss at 0.06950225681066513 Validation Accuracy at 0.6947999000549316\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss at 0.022806908935308456 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss at 0.006132747046649456 Validation Accuracy at 0.687799870967865\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss at 0.008339952677488327 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss at 0.005198854953050613 Validation Accuracy at 0.6905999183654785\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss at 0.05703296139836311 Validation Accuracy at 0.6885999441146851\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss at 0.01833047717809677 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss at 0.008926501497626305 Validation Accuracy at 0.6737999320030212\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss at 0.011317462660372257 Validation Accuracy at 0.6903998255729675\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss at 0.006036883220076561 Validation Accuracy at 0.6851999163627625\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss at 0.07437964528799057 Validation Accuracy at 0.6917998194694519\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss at 0.03568839654326439 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss at 0.00666344678029418 Validation Accuracy at 0.6879999041557312\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss at 0.003112046979367733 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss at 0.03659309446811676 Validation Accuracy at 0.6851998567581177\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss at 0.0708722397685051 Validation Accuracy at 0.7001999020576477\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss at 0.016351062804460526 Validation Accuracy at 0.6879999041557312\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss at 0.009766902774572372 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss at 0.006146569736301899 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss at 0.01818346604704857 Validation Accuracy at 0.6821998953819275\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss at 0.08943773806095123 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss at 0.02847638539969921 Validation Accuracy at 0.6941998600959778\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss at 0.008488197810947895 Validation Accuracy at 0.6873998641967773\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss at 0.009807624854147434 Validation Accuracy at 0.6997998356819153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134, CIFAR-10 Batch 1:  Loss at 0.009032871574163437 Validation Accuracy at 0.6839999556541443\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss at 0.05894008278846741 Validation Accuracy at 0.697399914264679\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss at 0.01837957091629505 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss at 0.021923314779996872 Validation Accuracy at 0.6851997971534729\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss at 0.005541050340980291 Validation Accuracy at 0.6913999319076538\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss at 0.0021672684233635664 Validation Accuracy at 0.6879999041557312\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss at 0.11221062391996384 Validation Accuracy at 0.6895998120307922\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss at 0.037906065583229065 Validation Accuracy at 0.6905999183654785\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss at 0.009036094881594181 Validation Accuracy at 0.6809999346733093\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss at 0.013853466138243675 Validation Accuracy at 0.6819998621940613\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss at 0.029774591326713562 Validation Accuracy at 0.6835998892784119\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss at 0.0962410718202591 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss at 0.020732175558805466 Validation Accuracy at 0.6939999461174011\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss at 0.0077862199395895 Validation Accuracy at 0.6863998770713806\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss at 0.005043688230216503 Validation Accuracy at 0.6895998120307922\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss at 0.0037552695721387863 Validation Accuracy at 0.6921998858451843\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss at 0.06318434327840805 Validation Accuracy at 0.691199779510498\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss at 0.018688762560486794 Validation Accuracy at 0.687799870967865\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss at 0.011519513092935085 Validation Accuracy at 0.6835998892784119\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss at 0.009167881682515144 Validation Accuracy at 0.6867998242378235\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss at 0.03203633800148964 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss at 0.0725162923336029 Validation Accuracy at 0.689599871635437\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss at 0.03807990998029709 Validation Accuracy at 0.6855999231338501\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss at 0.011057780124247074 Validation Accuracy at 0.6943999528884888\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss at 0.007074819412082434 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss at 0.006211347412317991 Validation Accuracy at 0.6973997950553894\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss at 0.04506274685263634 Validation Accuracy at 0.6969998478889465\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss at 0.02308683842420578 Validation Accuracy at 0.6891999244689941\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss at 0.02796107530593872 Validation Accuracy at 0.6941998600959778\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss at 0.013830884359776974 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss at 0.03048650361597538 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss at 0.06054398790001869 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss at 0.012114182114601135 Validation Accuracy at 0.682999849319458\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss at 0.018206411972641945 Validation Accuracy at 0.6713998317718506\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss at 0.00659377733245492 Validation Accuracy at 0.6957998275756836\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss at 0.00701529998332262 Validation Accuracy at 0.6883998513221741\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss at 0.04553399980068207 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss at 0.04457809776067734 Validation Accuracy at 0.6927999258041382\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss at 0.01507391594350338 Validation Accuracy at 0.6907998919487\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss at 0.009581562131643295 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss at 0.010022049769759178 Validation Accuracy at 0.6905999183654785\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss at 0.07084023207426071 Validation Accuracy at 0.6935998201370239\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss at 0.014483580365777016 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss at 0.002630641683936119 Validation Accuracy at 0.6939999461174011\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss at 0.004636669531464577 Validation Accuracy at 0.6905999183654785\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss at 0.0088896993547678 Validation Accuracy at 0.6941998600959778\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss at 0.05882497504353523 Validation Accuracy at 0.693199872970581\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss at 0.013136561028659344 Validation Accuracy at 0.689599871635437\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss at 0.006376749835908413 Validation Accuracy at 0.6909999251365662\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss at 0.007674560882151127 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss at 0.03851475566625595 Validation Accuracy at 0.6873999238014221\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss at 0.07558873295783997 Validation Accuracy at 0.692599892616272\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss at 0.023913152515888214 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss at 0.029553908854722977 Validation Accuracy at 0.6743999719619751\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss at 0.0032920492812991142 Validation Accuracy at 0.6905998587608337\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss at 0.01448238268494606 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss at 0.06446385383605957 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss at 0.03722306340932846 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss at 0.015215320512652397 Validation Accuracy at 0.6897998452186584\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss at 0.017045047134160995 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss at 0.006837205030024052 Validation Accuracy at 0.6893998384475708\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss at 0.059694062918424606 Validation Accuracy at 0.6987999081611633\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss at 0.023455016314983368 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss at 0.005064853932708502 Validation Accuracy at 0.6901998519897461\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss at 0.0031520272605121136 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss at 0.010734869167208672 Validation Accuracy at 0.7033998370170593\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss at 0.08567241579294205 Validation Accuracy at 0.6945999264717102\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss at 0.018409714102745056 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss at 0.005331937689334154 Validation Accuracy at 0.6969998478889465\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss at 0.022383378818631172 Validation Accuracy at 0.6889998316764832\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss at 0.02134975790977478 Validation Accuracy at 0.7007998824119568\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss at 0.0383453294634819 Validation Accuracy at 0.6941999197006226\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss at 0.0096717095002532 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss at 0.005071197170764208 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss at 0.022664874792099 Validation Accuracy at 0.6879999041557312\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss at 0.011094697751104832 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss at 0.06576256453990936 Validation Accuracy at 0.6855998635292053\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss at 0.029446594417095184 Validation Accuracy at 0.6991998553276062\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss at 0.014912275597453117 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss at 0.03260836750268936 Validation Accuracy at 0.6841999292373657\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss at 0.021162815392017365 Validation Accuracy at 0.6891999244689941\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss at 0.060105856508016586 Validation Accuracy at 0.6887998580932617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150, CIFAR-10 Batch 3:  Loss at 0.027760382741689682 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss at 0.011361902579665184 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss at 0.0014648166252300143 Validation Accuracy at 0.6899998784065247\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss at 0.010704286396503448 Validation Accuracy at 0.6799999475479126\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss at 0.06557635217905045 Validation Accuracy at 0.689599871635437\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss at 0.042858660221099854 Validation Accuracy at 0.6933998465538025\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss at 0.003954548388719559 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss at 0.009478054009377956 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss at 0.029016243293881416 Validation Accuracy at 0.6919998526573181\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss at 0.05722561478614807 Validation Accuracy at 0.6917999386787415\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss at 0.01815195195376873 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss at 0.007690358906984329 Validation Accuracy at 0.6855998635292053\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss at 0.0031950464472174644 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss at 0.002094220370054245 Validation Accuracy at 0.6977999210357666\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss at 0.04488964006304741 Validation Accuracy at 0.6983999013900757\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss at 0.015329910442233086 Validation Accuracy at 0.695999801158905\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss at 0.006139840930700302 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss at 0.0025017070583999157 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss at 0.01761108636856079 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss at 0.05782032012939453 Validation Accuracy at 0.6933999061584473\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss at 0.008032435551285744 Validation Accuracy at 0.6997998356819153\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss at 0.00575894583016634 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss at 0.008939310908317566 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss at 0.01570366695523262 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss at 0.04028334841132164 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss at 0.005394621752202511 Validation Accuracy at 0.6997998356819153\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss at 0.020782427862286568 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss at 0.0016938691260293126 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss at 0.0321834459900856 Validation Accuracy at 0.6975998282432556\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss at 0.022291813045740128 Validation Accuracy at 0.6927998661994934\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss at 0.020182324573397636 Validation Accuracy at 0.6927998065948486\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss at 0.011006603017449379 Validation Accuracy at 0.6885998845100403\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss at 0.020460762083530426 Validation Accuracy at 0.6891998052597046\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss at 0.009408436715602875 Validation Accuracy at 0.6969999074935913\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss at 0.0355730876326561 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss at 0.014014908112585545 Validation Accuracy at 0.6951999068260193\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss at 0.037916332483291626 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss at 0.009627819992601871 Validation Accuracy at 0.681999921798706\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss at 0.0076314350590109825 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss at 0.062481749802827835 Validation Accuracy at 0.6935998201370239\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss at 0.005250921938568354 Validation Accuracy at 0.697199821472168\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss at 0.012506471946835518 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss at 0.0014243999030441046 Validation Accuracy at 0.6965999007225037\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss at 0.01673293672502041 Validation Accuracy at 0.6899998784065247\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss at 0.03832394629716873 Validation Accuracy at 0.693199872970581\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss at 0.024948660284280777 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss at 0.006379999220371246 Validation Accuracy at 0.6823998689651489\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss at 0.009275964461266994 Validation Accuracy at 0.6961998343467712\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss at 0.03127269074320793 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss at 0.057244885712862015 Validation Accuracy at 0.697199821472168\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss at 0.015505827963352203 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss at 0.005080465693026781 Validation Accuracy at 0.6965999007225037\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss at 0.0015433145454153419 Validation Accuracy at 0.6889998912811279\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss at 0.018187718465924263 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss at 0.04606585204601288 Validation Accuracy at 0.700999915599823\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss at 0.005278122611343861 Validation Accuracy at 0.6919998526573181\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss at 0.007544496562331915 Validation Accuracy at 0.7035998106002808\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss at 0.0028063617646694183 Validation Accuracy at 0.6965999007225037\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss at 0.029499229043722153 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss at 0.03213662654161453 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss at 0.016031522303819656 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss at 0.0061586713418364525 Validation Accuracy at 0.6883999109268188\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss at 0.001215187949128449 Validation Accuracy at 0.694399893283844\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss at 0.008698735386133194 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss at 0.03211569786071777 Validation Accuracy at 0.6917999386787415\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss at 0.004032067023217678 Validation Accuracy at 0.697199821472168\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss at 0.002875249832868576 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss at 0.0008310444536618888 Validation Accuracy at 0.6969999074935913\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss at 0.039621613919734955 Validation Accuracy at 0.6861998438835144\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss at 0.05291586369276047 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss at 0.003312507178634405 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss at 0.018311426043510437 Validation Accuracy at 0.691399872303009\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss at 0.0005796060431748629 Validation Accuracy at 0.6967999339103699\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss at 0.007586801890283823 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss at 0.031999439001083374 Validation Accuracy at 0.6973997950553894\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss at 0.012604870833456516 Validation Accuracy at 0.6963998079299927\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss at 0.004796737339347601 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss at 0.0004518057394307107 Validation Accuracy at 0.6897998452186584\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss at 0.006727241910994053 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss at 0.039163123816251755 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss at 0.01835719123482704 Validation Accuracy at 0.6947998404502869\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss at 0.009623064659535885 Validation Accuracy at 0.6893999576568604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166, CIFAR-10 Batch 5:  Loss at 0.0066789803095161915 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss at 0.01372138038277626 Validation Accuracy at 0.6933999061584473\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss at 0.027955513447523117 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss at 0.007393715437501669 Validation Accuracy at 0.6855998635292053\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss at 0.002620734041556716 Validation Accuracy at 0.6921998858451843\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss at 0.0018481685547158122 Validation Accuracy at 0.686599850654602\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss at 0.04030053690075874 Validation Accuracy at 0.6827999353408813\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss at 0.030719613656401634 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss at 0.01978548988699913 Validation Accuracy at 0.6945999264717102\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss at 0.013655265793204308 Validation Accuracy at 0.6949998140335083\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss at 0.00900252815335989 Validation Accuracy at 0.6933999061584473\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss at 0.006922918371856213 Validation Accuracy at 0.6919997930526733\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss at 0.045840732753276825 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss at 0.013366665691137314 Validation Accuracy at 0.6933997869491577\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss at 0.008415632881224155 Validation Accuracy at 0.6887999176979065\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss at 0.002135353395715356 Validation Accuracy at 0.6897999048233032\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss at 0.02079363539814949 Validation Accuracy at 0.6921999454498291\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss at 0.02485417202115059 Validation Accuracy at 0.697199821472168\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss at 0.007738928776234388 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss at 0.010654108598828316 Validation Accuracy at 0.6981999278068542\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss at 0.0016087236581370234 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss at 0.011359036900103092 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss at 0.05308470129966736 Validation Accuracy at 0.6837998628616333\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss at 0.012256577610969543 Validation Accuracy at 0.7005999088287354\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss at 0.011414101347327232 Validation Accuracy at 0.6927998661994934\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss at 0.0021390789188444614 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss at 0.010666610673069954 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss at 0.024251867085695267 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss at 0.006260087713599205 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss at 0.006035170517861843 Validation Accuracy at 0.696199893951416\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss at 0.009099477902054787 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss at 0.012610532343387604 Validation Accuracy at 0.6943999528884888\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss at 0.04250074177980423 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss at 0.008519774302840233 Validation Accuracy at 0.6987999677658081\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss at 0.017415817826986313 Validation Accuracy at 0.6935999393463135\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss at 0.0034417780116200447 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss at 0.005270225927233696 Validation Accuracy at 0.6987998485565186\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss at 0.008235972374677658 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss at 0.011245532892644405 Validation Accuracy at 0.6993998289108276\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss at 0.010339628905057907 Validation Accuracy at 0.6969997882843018\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss at 0.009912670589983463 Validation Accuracy at 0.6853998303413391\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss at 0.005293705500662327 Validation Accuracy at 0.6885998249053955\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss at 0.02554653212428093 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss at 0.009381751529872417 Validation Accuracy at 0.6913998126983643\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss at 0.014747848734259605 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss at 0.0019281876739114523 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss at 0.001598833128809929 Validation Accuracy at 0.6949998140335083\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss at 0.05263547971844673 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss at 0.03601061925292015 Validation Accuracy at 0.6953999400138855\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss at 0.008910336531698704 Validation Accuracy at 0.6875998973846436\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss at 0.01212235726416111 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss at 0.002391410991549492 Validation Accuracy at 0.6983999013900757\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss at 0.02213151380419731 Validation Accuracy at 0.697999894618988\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss at 0.010130506008863449 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss at 0.004212474916130304 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss at 0.0036957385018467903 Validation Accuracy at 0.6967998147010803\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss at 0.015719959512352943 Validation Accuracy at 0.691399872303009\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss at 0.07012194395065308 Validation Accuracy at 0.6919999122619629\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss at 0.011945854872465134 Validation Accuracy at 0.6993998289108276\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss at 0.03706488013267517 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss at 0.003376163076609373 Validation Accuracy at 0.6877999305725098\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss at 0.015712521970272064 Validation Accuracy at 0.6777999401092529\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss at 0.030901428312063217 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss at 0.008885820396244526 Validation Accuracy at 0.691399872303009\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss at 0.0066911824978888035 Validation Accuracy at 0.694399893283844\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss at 0.0008907958399504423 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss at 0.006134890019893646 Validation Accuracy at 0.691399872303009\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss at 0.03345701843500137 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss at 0.012903914786875248 Validation Accuracy at 0.6979998350143433\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss at 0.010935358703136444 Validation Accuracy at 0.6857998371124268\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss at 0.000876337755471468 Validation Accuracy at 0.6907998919487\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss at 0.0077936891466379166 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss at 0.022592458873987198 Validation Accuracy at 0.6999998688697815\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss at 0.006598031613975763 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss at 0.005898530129343271 Validation Accuracy at 0.6855998635292053\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss at 0.008149479515850544 Validation Accuracy at 0.6947999000549316\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss at 0.0008795877220109105 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss at 0.05070819705724716 Validation Accuracy at 0.6899998784065247\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss at 0.004794527776539326 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss at 0.011823009699583054 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss at 0.009580021724104881 Validation Accuracy at 0.6887997984886169\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss at 0.0020479299128055573 Validation Accuracy at 0.6965999007225037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, CIFAR-10 Batch 2:  Loss at 0.01739179529249668 Validation Accuracy at 0.6967999339103699\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss at 0.009988650679588318 Validation Accuracy at 0.6987998485565186\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss at 0.006468100007623434 Validation Accuracy at 0.6869998574256897\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss at 0.0035172398202121258 Validation Accuracy at 0.6873998641967773\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss at 0.050107572227716446 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss at 0.01818801276385784 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss at 0.02065729722380638 Validation Accuracy at 0.6975998282432556\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss at 0.0040887086652219296 Validation Accuracy at 0.693199872970581\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss at 7.399254536721855e-05 Validation Accuracy at 0.6921998858451843\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss at 0.018525730818510056 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss at 0.02613978460431099 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss at 0.013221872970461845 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss at 0.011449766345322132 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss at 0.00086740602273494 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss at 0.0028920178301632404 Validation Accuracy at 0.6933999061584473\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss at 0.05670304596424103 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss at 0.010722304694354534 Validation Accuracy at 0.6993998289108276\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss at 0.006691767834126949 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss at 0.0028937484603375196 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss at 0.005072541069239378 Validation Accuracy at 0.6901998519897461\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss at 0.014875980094075203 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss at 0.008108386769890785 Validation Accuracy at 0.6983998417854309\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss at 0.004605292342603207 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss at 0.007008837070316076 Validation Accuracy at 0.693199872970581\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss at 0.004763201344758272 Validation Accuracy at 0.6893998384475708\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss at 0.01642780192196369 Validation Accuracy at 0.6897999048233032\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss at 0.013540725223720074 Validation Accuracy at 0.696199893951416\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss at 0.0022915953304618597 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss at 0.005207924637943506 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss at 0.006417729891836643 Validation Accuracy at 0.6987999677658081\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss at 0.03279005363583565 Validation Accuracy at 0.6891998052597046\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss at 0.02242676168680191 Validation Accuracy at 0.6861998438835144\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss at 0.008995895273983479 Validation Accuracy at 0.6893998980522156\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss at 0.0042005786672234535 Validation Accuracy at 0.694399893283844\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss at 0.011793343350291252 Validation Accuracy at 0.6921999454498291\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss at 0.014997432008385658 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss at 0.011799059808254242 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss at 0.003916874993592501 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss at 0.004193898756057024 Validation Accuracy at 0.7001999020576477\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss at 0.008179930970072746 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss at 0.016849935054779053 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss at 0.008171914145350456 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss at 0.009294264018535614 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss at 0.0014132325304672122 Validation Accuracy at 0.6963998675346375\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss at 0.012428744696080685 Validation Accuracy at 0.7015998959541321\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss at 0.07743330299854279 Validation Accuracy at 0.6917999386787415\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss at 0.003968174569308758 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss at 0.010156553238630295 Validation Accuracy at 0.6925998330116272\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss at 0.00472468975931406 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss at 0.006835286505520344 Validation Accuracy at 0.6951999068260193\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss at 0.04447463899850845 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss at 0.002352457959204912 Validation Accuracy at 0.7003998160362244\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss at 0.014977850019931793 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss at 0.002813883125782013 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss at 0.008272571489214897 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss at 0.0371352843940258 Validation Accuracy at 0.6897998452186584\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss at 0.0045719328336417675 Validation Accuracy at 0.6971998810768127\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss at 0.012232284061610699 Validation Accuracy at 0.6919997930526733\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss at 0.0010911159915849566 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss at 0.004746736027300358 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss at 0.023656977340579033 Validation Accuracy at 0.701999843120575\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss at 0.006549987010657787 Validation Accuracy at 0.7005998492240906\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss at 0.004944414831697941 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss at 0.0009534922428429127 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss at 0.0027336289640516043 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss at 0.02032419666647911 Validation Accuracy at 0.7011998295783997\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss at 0.0028578471392393112 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss at 0.0076945191249251366 Validation Accuracy at 0.6869999170303345\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss at 0.0007286901236511767 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss at 0.00864854734390974 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss at 0.026283813640475273 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss at 0.010133803822100163 Validation Accuracy at 0.7005998492240906\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss at 0.01492663286626339 Validation Accuracy at 0.692599892616272\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss at 0.00045506854075938463 Validation Accuracy at 0.6991998553276062\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss at 0.0019836644642055035 Validation Accuracy at 0.697799801826477\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss at 0.02872825600206852 Validation Accuracy at 0.6881998777389526\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss at 0.004130377899855375 Validation Accuracy at 0.6887999176979065\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss at 0.006415983662009239 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss at 0.001035516383126378 Validation Accuracy at 0.6959998607635498\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss at 0.013925880193710327 Validation Accuracy at 0.6963998079299927\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss at 0.0683007761836052 Validation Accuracy at 0.6773998737335205\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss at 0.006606255192309618 Validation Accuracy at 0.6903998255729675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199, CIFAR-10 Batch 4:  Loss at 0.009019292891025543 Validation Accuracy at 0.6853998303413391\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss at 0.00729157542809844 Validation Accuracy at 0.6875998973846436\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss at 0.012339890003204346 Validation Accuracy at 0.6901998519897461\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss at 0.033781420439481735 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss at 0.024848375469446182 Validation Accuracy at 0.6803998947143555\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss at 0.0046163927763700485 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss at 0.0003752725024241954 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss at 0.013330142013728619 Validation Accuracy at 0.6909998059272766\n",
      "Epoch 201, CIFAR-10 Batch 2:  Loss at 0.05103811249136925 Validation Accuracy at 0.6977998614311218\n",
      "Epoch 201, CIFAR-10 Batch 3:  Loss at 0.009413487277925014 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 201, CIFAR-10 Batch 4:  Loss at 0.003729052608832717 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 201, CIFAR-10 Batch 5:  Loss at 0.0014847879065200686 Validation Accuracy at 0.6957999467849731\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss at 0.012603914365172386 Validation Accuracy at 0.695999801158905\n",
      "Epoch 202, CIFAR-10 Batch 2:  Loss at 0.013183399103581905 Validation Accuracy at 0.6971998810768127\n",
      "Epoch 202, CIFAR-10 Batch 3:  Loss at 0.007672867737710476 Validation Accuracy at 0.6851998567581177\n",
      "Epoch 202, CIFAR-10 Batch 4:  Loss at 0.004729428794234991 Validation Accuracy at 0.690599799156189\n",
      "Epoch 202, CIFAR-10 Batch 5:  Loss at 0.0006729348679073155 Validation Accuracy at 0.6975998282432556\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss at 0.03994469717144966 Validation Accuracy at 0.6883999109268188\n",
      "Epoch 203, CIFAR-10 Batch 2:  Loss at 0.027986889705061913 Validation Accuracy at 0.6879999041557312\n",
      "Epoch 203, CIFAR-10 Batch 3:  Loss at 0.0040475293062627316 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 203, CIFAR-10 Batch 4:  Loss at 0.008427360095083714 Validation Accuracy at 0.7001999020576477\n",
      "Epoch 203, CIFAR-10 Batch 5:  Loss at 0.0027882943395525217 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss at 0.012081881985068321 Validation Accuracy at 0.6967998147010803\n",
      "Epoch 204, CIFAR-10 Batch 2:  Loss at 0.016369858756661415 Validation Accuracy at 0.6913998126983643\n",
      "Epoch 204, CIFAR-10 Batch 3:  Loss at 0.001850744360126555 Validation Accuracy at 0.693199872970581\n",
      "Epoch 204, CIFAR-10 Batch 4:  Loss at 0.00880403071641922 Validation Accuracy at 0.6957998275756836\n",
      "Epoch 204, CIFAR-10 Batch 5:  Loss at 0.01003204844892025 Validation Accuracy at 0.6885998249053955\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss at 0.0038935146294534206 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 205, CIFAR-10 Batch 2:  Loss at 0.025338945910334587 Validation Accuracy at 0.6995999217033386\n",
      "Epoch 205, CIFAR-10 Batch 3:  Loss at 0.0011305391089990735 Validation Accuracy at 0.699199914932251\n",
      "Epoch 205, CIFAR-10 Batch 4:  Loss at 0.009655061177909374 Validation Accuracy at 0.6905998587608337\n",
      "Epoch 205, CIFAR-10 Batch 5:  Loss at 0.0002641564351506531 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss at 0.021415986120700836 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 206, CIFAR-10 Batch 2:  Loss at 0.02939077466726303 Validation Accuracy at 0.6999999284744263\n",
      "Epoch 206, CIFAR-10 Batch 3:  Loss at 0.004287624265998602 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 206, CIFAR-10 Batch 4:  Loss at 0.009243706241250038 Validation Accuracy at 0.6927998065948486\n",
      "Epoch 206, CIFAR-10 Batch 5:  Loss at 0.0011889934539794922 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss at 0.01338337641209364 Validation Accuracy at 0.7013998031616211\n",
      "Epoch 207, CIFAR-10 Batch 2:  Loss at 0.04475673288106918 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 207, CIFAR-10 Batch 3:  Loss at 0.008462900295853615 Validation Accuracy at 0.697799801826477\n",
      "Epoch 207, CIFAR-10 Batch 4:  Loss at 0.01800333708524704 Validation Accuracy at 0.6897999048233032\n",
      "Epoch 207, CIFAR-10 Batch 5:  Loss at 0.001190155977383256 Validation Accuracy at 0.694399893283844\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss at 0.02394459955394268 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 208, CIFAR-10 Batch 2:  Loss at 0.030023228377103806 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 208, CIFAR-10 Batch 3:  Loss at 0.008863041177392006 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 208, CIFAR-10 Batch 4:  Loss at 0.010487779043614864 Validation Accuracy at 0.6891998648643494\n",
      "Epoch 208, CIFAR-10 Batch 5:  Loss at 0.0023614049423485994 Validation Accuracy at 0.6949998140335083\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss at 0.007747802417725325 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 209, CIFAR-10 Batch 2:  Loss at 0.03592067211866379 Validation Accuracy at 0.6971998810768127\n",
      "Epoch 209, CIFAR-10 Batch 3:  Loss at 0.002376439981162548 Validation Accuracy at 0.6913999319076538\n",
      "Epoch 209, CIFAR-10 Batch 4:  Loss at 0.0017493837513029575 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 209, CIFAR-10 Batch 5:  Loss at 0.0010127723217010498 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss at 0.03273056447505951 Validation Accuracy at 0.6871998310089111\n",
      "Epoch 210, CIFAR-10 Batch 2:  Loss at 0.019007395952939987 Validation Accuracy at 0.6867998838424683\n",
      "Epoch 210, CIFAR-10 Batch 3:  Loss at 0.010448087006807327 Validation Accuracy at 0.6849998235702515\n",
      "Epoch 210, CIFAR-10 Batch 4:  Loss at 0.005295744631439447 Validation Accuracy at 0.6817998886108398\n",
      "Epoch 210, CIFAR-10 Batch 5:  Loss at 0.0013805665075778961 Validation Accuracy at 0.7017998695373535\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss at 0.01675739698112011 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 211, CIFAR-10 Batch 2:  Loss at 0.01874857023358345 Validation Accuracy at 0.6979998350143433\n",
      "Epoch 211, CIFAR-10 Batch 3:  Loss at 0.008581366389989853 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 211, CIFAR-10 Batch 4:  Loss at 0.00616642227396369 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 211, CIFAR-10 Batch 5:  Loss at 0.0018807539017871022 Validation Accuracy at 0.692599892616272\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss at 0.010832048952579498 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 212, CIFAR-10 Batch 2:  Loss at 0.012296067550778389 Validation Accuracy at 0.6961998343467712\n",
      "Epoch 212, CIFAR-10 Batch 3:  Loss at 0.008033323101699352 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 212, CIFAR-10 Batch 4:  Loss at 0.0032943934202194214 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 212, CIFAR-10 Batch 5:  Loss at 0.003267333609983325 Validation Accuracy at 0.6945998668670654\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss at 0.007996386848390102 Validation Accuracy at 0.693199872970581\n",
      "Epoch 213, CIFAR-10 Batch 2:  Loss at 0.031085623428225517 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 213, CIFAR-10 Batch 3:  Loss at 0.005099192261695862 Validation Accuracy at 0.6981999278068542\n",
      "Epoch 213, CIFAR-10 Batch 4:  Loss at 0.0035978122614324093 Validation Accuracy at 0.6993998885154724\n",
      "Epoch 213, CIFAR-10 Batch 5:  Loss at 0.0011771542485803366 Validation Accuracy at 0.6947999000549316\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss at 0.00747320894151926 Validation Accuracy at 0.6965998411178589\n",
      "Epoch 214, CIFAR-10 Batch 2:  Loss at 0.013793589547276497 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 214, CIFAR-10 Batch 3:  Loss at 0.0029878984205424786 Validation Accuracy at 0.6927998661994934\n",
      "Epoch 214, CIFAR-10 Batch 4:  Loss at 0.00846676155924797 Validation Accuracy at 0.6939998865127563\n",
      "Epoch 214, CIFAR-10 Batch 5:  Loss at 0.006931948475539684 Validation Accuracy at 0.699199914932251\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss at 0.004230410326272249 Validation Accuracy at 0.697999894618988\n",
      "Epoch 215, CIFAR-10 Batch 2:  Loss at 0.017920320853590965 Validation Accuracy at 0.691399872303009\n",
      "Epoch 215, CIFAR-10 Batch 3:  Loss at 0.0028326348401606083 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 215, CIFAR-10 Batch 4:  Loss at 0.0027626967057585716 Validation Accuracy at 0.6989999413490295\n",
      "Epoch 215, CIFAR-10 Batch 5:  Loss at 0.00037037889705970883 Validation Accuracy at 0.7005999088287354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216, CIFAR-10 Batch 1:  Loss at 0.037812747061252594 Validation Accuracy at 0.6833999156951904\n",
      "Epoch 216, CIFAR-10 Batch 2:  Loss at 0.009051330387592316 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 216, CIFAR-10 Batch 3:  Loss at 0.019445251673460007 Validation Accuracy at 0.6923999190330505\n",
      "Epoch 216, CIFAR-10 Batch 4:  Loss at 0.005502142012119293 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 216, CIFAR-10 Batch 5:  Loss at 0.00021139717136975378 Validation Accuracy at 0.7087998390197754\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss at 0.025297053158283234 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 217, CIFAR-10 Batch 2:  Loss at 0.016662145033478737 Validation Accuracy at 0.6979998350143433\n",
      "Epoch 217, CIFAR-10 Batch 3:  Loss at 0.027587411925196648 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 217, CIFAR-10 Batch 4:  Loss at 0.018001606687903404 Validation Accuracy at 0.6847997903823853\n",
      "Epoch 217, CIFAR-10 Batch 5:  Loss at 0.0012175124138593674 Validation Accuracy at 0.6971998810768127\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss at 0.0045878151431679726 Validation Accuracy at 0.6981998085975647\n",
      "Epoch 218, CIFAR-10 Batch 2:  Loss at 0.01811739057302475 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 218, CIFAR-10 Batch 3:  Loss at 0.002196661429479718 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 218, CIFAR-10 Batch 4:  Loss at 0.008588769473135471 Validation Accuracy at 0.6901998519897461\n",
      "Epoch 218, CIFAR-10 Batch 5:  Loss at 0.004680979996919632 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss at 0.018346766009926796 Validation Accuracy at 0.6835998892784119\n",
      "Epoch 219, CIFAR-10 Batch 2:  Loss at 0.019802197813987732 Validation Accuracy at 0.6919998526573181\n",
      "Epoch 219, CIFAR-10 Batch 3:  Loss at 0.006202553864568472 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 219, CIFAR-10 Batch 4:  Loss at 0.0072454120963811874 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 219, CIFAR-10 Batch 5:  Loss at 0.0005170795484445989 Validation Accuracy at 0.701999843120575\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss at 0.0218079574406147 Validation Accuracy at 0.6917998790740967\n",
      "Epoch 220, CIFAR-10 Batch 2:  Loss at 0.031921904534101486 Validation Accuracy at 0.6883998513221741\n",
      "Epoch 220, CIFAR-10 Batch 3:  Loss at 0.005816182587295771 Validation Accuracy at 0.6963999271392822\n",
      "Epoch 220, CIFAR-10 Batch 4:  Loss at 0.003342125564813614 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 220, CIFAR-10 Batch 5:  Loss at 0.0014436570927500725 Validation Accuracy at 0.6979998350143433\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss at 0.004336594603955746 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 221, CIFAR-10 Batch 2:  Loss at 0.019483478739857674 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 221, CIFAR-10 Batch 3:  Loss at 0.002368448069319129 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 221, CIFAR-10 Batch 4:  Loss at 0.007488852832466364 Validation Accuracy at 0.6853998899459839\n",
      "Epoch 221, CIFAR-10 Batch 5:  Loss at 0.00021444211597554386 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss at 0.008946245536208153 Validation Accuracy at 0.6807998418807983\n",
      "Epoch 222, CIFAR-10 Batch 2:  Loss at 0.022568853572010994 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 222, CIFAR-10 Batch 3:  Loss at 0.0033964274916797876 Validation Accuracy at 0.6871998310089111\n",
      "Epoch 222, CIFAR-10 Batch 4:  Loss at 0.010228914208710194 Validation Accuracy at 0.7023999094963074\n",
      "Epoch 222, CIFAR-10 Batch 5:  Loss at 0.0007815822609700263 Validation Accuracy at 0.6929999589920044\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss at 0.01267506554722786 Validation Accuracy at 0.6849998235702515\n",
      "Epoch 223, CIFAR-10 Batch 2:  Loss at 0.03194613382220268 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 223, CIFAR-10 Batch 3:  Loss at 0.006442864891141653 Validation Accuracy at 0.6941999197006226\n",
      "Epoch 223, CIFAR-10 Batch 4:  Loss at 0.010370267555117607 Validation Accuracy at 0.690599799156189\n",
      "Epoch 223, CIFAR-10 Batch 5:  Loss at 0.0019292880315333605 Validation Accuracy at 0.6927998065948486\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss at 0.0034371193032711744 Validation Accuracy at 0.6897998452186584\n",
      "Epoch 224, CIFAR-10 Batch 2:  Loss at 0.009679373353719711 Validation Accuracy at 0.6941999197006226\n",
      "Epoch 224, CIFAR-10 Batch 3:  Loss at 0.0009924385230988264 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 224, CIFAR-10 Batch 4:  Loss at 0.02691517025232315 Validation Accuracy at 0.6839998364448547\n",
      "Epoch 224, CIFAR-10 Batch 5:  Loss at 0.003916552755981684 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss at 0.010976042598485947 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 225, CIFAR-10 Batch 2:  Loss at 0.017895901575684547 Validation Accuracy at 0.6979999542236328\n",
      "Epoch 225, CIFAR-10 Batch 3:  Loss at 0.007787549868226051 Validation Accuracy at 0.6947999000549316\n",
      "Epoch 225, CIFAR-10 Batch 4:  Loss at 0.007248874753713608 Validation Accuracy at 0.6921999454498291\n",
      "Epoch 225, CIFAR-10 Batch 5:  Loss at 0.004739785101264715 Validation Accuracy at 0.6923998594284058\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss at 0.008743437938392162 Validation Accuracy at 0.7005998492240906\n",
      "Epoch 226, CIFAR-10 Batch 2:  Loss at 0.004820003639906645 Validation Accuracy at 0.6991998553276062\n",
      "Epoch 226, CIFAR-10 Batch 3:  Loss at 0.004484578035771847 Validation Accuracy at 0.6991997957229614\n",
      "Epoch 226, CIFAR-10 Batch 4:  Loss at 0.011590681970119476 Validation Accuracy at 0.6865999698638916\n",
      "Epoch 226, CIFAR-10 Batch 5:  Loss at 0.00034768058685585856 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss at 0.038457393646240234 Validation Accuracy at 0.6811999082565308\n",
      "Epoch 227, CIFAR-10 Batch 2:  Loss at 0.010136280208826065 Validation Accuracy at 0.6889999508857727\n",
      "Epoch 227, CIFAR-10 Batch 3:  Loss at 0.0030081328004598618 Validation Accuracy at 0.6965999007225037\n",
      "Epoch 227, CIFAR-10 Batch 4:  Loss at 0.015189566649496555 Validation Accuracy at 0.6949998736381531\n",
      "Epoch 227, CIFAR-10 Batch 5:  Loss at 0.0008902446716092527 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss at 0.028766628354787827 Validation Accuracy at 0.6843998432159424\n",
      "Epoch 228, CIFAR-10 Batch 2:  Loss at 0.013335252180695534 Validation Accuracy at 0.6901998519897461\n",
      "Epoch 228, CIFAR-10 Batch 3:  Loss at 0.007656348403543234 Validation Accuracy at 0.6841999292373657\n",
      "Epoch 228, CIFAR-10 Batch 4:  Loss at 0.005080074537545443 Validation Accuracy at 0.6953998804092407\n",
      "Epoch 228, CIFAR-10 Batch 5:  Loss at 0.0010616474319249392 Validation Accuracy at 0.6973997950553894\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss at 0.00799402967095375 Validation Accuracy at 0.6989998817443848\n",
      "Epoch 229, CIFAR-10 Batch 2:  Loss at 0.01438942737877369 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 229, CIFAR-10 Batch 3:  Loss at 0.003820649581030011 Validation Accuracy at 0.6935998201370239\n",
      "Epoch 229, CIFAR-10 Batch 4:  Loss at 0.0037233924958854914 Validation Accuracy at 0.689599871635437\n",
      "Epoch 229, CIFAR-10 Batch 5:  Loss at 0.0003392208309378475 Validation Accuracy at 0.7025998830795288\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss at 0.03027261421084404 Validation Accuracy at 0.6879998445510864\n",
      "Epoch 230, CIFAR-10 Batch 2:  Loss at 0.02179647609591484 Validation Accuracy at 0.6907998919487\n",
      "Epoch 230, CIFAR-10 Batch 3:  Loss at 0.022577108815312386 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 230, CIFAR-10 Batch 4:  Loss at 0.009002447128295898 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 230, CIFAR-10 Batch 5:  Loss at 0.007070192135870457 Validation Accuracy at 0.6927998661994934\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss at 0.01429625041782856 Validation Accuracy at 0.699199914932251\n",
      "Epoch 231, CIFAR-10 Batch 2:  Loss at 0.007432963233441114 Validation Accuracy at 0.7083998918533325\n",
      "Epoch 231, CIFAR-10 Batch 3:  Loss at 0.010534344241023064 Validation Accuracy at 0.694399893283844\n",
      "Epoch 231, CIFAR-10 Batch 4:  Loss at 0.011314722709357738 Validation Accuracy at 0.6919999122619629\n",
      "Epoch 231, CIFAR-10 Batch 5:  Loss at 0.0018098268192261457 Validation Accuracy at 0.7003998756408691\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss at 0.0065024495124816895 Validation Accuracy at 0.699199914932251\n",
      "Epoch 232, CIFAR-10 Batch 2:  Loss at 0.019734397530555725 Validation Accuracy at 0.692599892616272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232, CIFAR-10 Batch 3:  Loss at 0.010870417580008507 Validation Accuracy at 0.6897998452186584\n",
      "Epoch 232, CIFAR-10 Batch 4:  Loss at 0.004059051163494587 Validation Accuracy at 0.69899982213974\n",
      "Epoch 232, CIFAR-10 Batch 5:  Loss at 0.0005118611152283847 Validation Accuracy at 0.7029998898506165\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss at 0.023578766733407974 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 233, CIFAR-10 Batch 2:  Loss at 0.029489876702427864 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 233, CIFAR-10 Batch 3:  Loss at 0.00396578898653388 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 233, CIFAR-10 Batch 4:  Loss at 0.001109415665268898 Validation Accuracy at 0.6983999013900757\n",
      "Epoch 233, CIFAR-10 Batch 5:  Loss at 0.007170849479734898 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss at 0.004612978082150221 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 234, CIFAR-10 Batch 2:  Loss at 0.010657737031579018 Validation Accuracy at 0.689599871635437\n",
      "Epoch 234, CIFAR-10 Batch 3:  Loss at 0.006615431047976017 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 234, CIFAR-10 Batch 4:  Loss at 0.009310705587267876 Validation Accuracy at 0.7015998363494873\n",
      "Epoch 234, CIFAR-10 Batch 5:  Loss at 0.0035352041013538837 Validation Accuracy at 0.7063998579978943\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss at 0.0061579504981637 Validation Accuracy at 0.6905999183654785\n",
      "Epoch 235, CIFAR-10 Batch 2:  Loss at 0.01728477142751217 Validation Accuracy at 0.6951999068260193\n",
      "Epoch 235, CIFAR-10 Batch 3:  Loss at 0.01312265545129776 Validation Accuracy at 0.6921998262405396\n",
      "Epoch 235, CIFAR-10 Batch 4:  Loss at 0.0026179736014455557 Validation Accuracy at 0.692599892616272\n",
      "Epoch 235, CIFAR-10 Batch 5:  Loss at 0.0001030757266562432 Validation Accuracy at 0.6983998417854309\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss at 0.017731711268424988 Validation Accuracy at 0.691399872303009\n",
      "Epoch 236, CIFAR-10 Batch 2:  Loss at 0.005687144584953785 Validation Accuracy at 0.6935999393463135\n",
      "Epoch 236, CIFAR-10 Batch 3:  Loss at 0.015159578062593937 Validation Accuracy at 0.694399893283844\n",
      "Epoch 236, CIFAR-10 Batch 4:  Loss at 0.006203430239111185 Validation Accuracy at 0.6985998153686523\n",
      "Epoch 236, CIFAR-10 Batch 5:  Loss at 0.002924902830272913 Validation Accuracy at 0.6929998993873596\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss at 0.0017284846398979425 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 237, CIFAR-10 Batch 2:  Loss at 0.009719645604491234 Validation Accuracy at 0.694399893283844\n",
      "Epoch 237, CIFAR-10 Batch 3:  Loss at 0.009992129169404507 Validation Accuracy at 0.6921998858451843\n",
      "Epoch 237, CIFAR-10 Batch 4:  Loss at 0.003244254272431135 Validation Accuracy at 0.697399914264679\n",
      "Epoch 237, CIFAR-10 Batch 5:  Loss at 0.002087026135995984 Validation Accuracy at 0.6903999447822571\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss at 0.012960309162735939 Validation Accuracy at 0.6933999061584473\n",
      "Epoch 238, CIFAR-10 Batch 2:  Loss at 0.006307365372776985 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 238, CIFAR-10 Batch 3:  Loss at 0.004057442303746939 Validation Accuracy at 0.6955998539924622\n",
      "Epoch 238, CIFAR-10 Batch 4:  Loss at 0.021044382825493813 Validation Accuracy at 0.6947998404502869\n",
      "Epoch 238, CIFAR-10 Batch 5:  Loss at 0.015590822324156761 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss at 0.0028354772366583347 Validation Accuracy at 0.6985998749732971\n",
      "Epoch 239, CIFAR-10 Batch 2:  Loss at 0.010652045719325542 Validation Accuracy at 0.6981998682022095\n",
      "Epoch 239, CIFAR-10 Batch 3:  Loss at 0.017939670011401176 Validation Accuracy at 0.6907998323440552\n",
      "Epoch 239, CIFAR-10 Batch 4:  Loss at 0.003437560983002186 Validation Accuracy at 0.696199893951416\n",
      "Epoch 239, CIFAR-10 Batch 5:  Loss at 0.004783219657838345 Validation Accuracy at 0.69899982213974\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss at 0.004833914339542389 Validation Accuracy at 0.697399914264679\n",
      "Epoch 240, CIFAR-10 Batch 2:  Loss at 0.01948242448270321 Validation Accuracy at 0.6975998878479004\n",
      "Epoch 240, CIFAR-10 Batch 3:  Loss at 0.01718989759683609 Validation Accuracy at 0.697799801826477\n",
      "Epoch 240, CIFAR-10 Batch 4:  Loss at 0.012550687417387962 Validation Accuracy at 0.6899998188018799\n",
      "Epoch 240, CIFAR-10 Batch 5:  Loss at 0.0007618188974447548 Validation Accuracy at 0.6991998553276062\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss at 0.015448424965143204 Validation Accuracy at 0.6973998546600342\n",
      "Epoch 241, CIFAR-10 Batch 2:  Loss at 0.028185443952679634 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 241, CIFAR-10 Batch 3:  Loss at 0.003243324812501669 Validation Accuracy at 0.6969998478889465\n",
      "Epoch 241, CIFAR-10 Batch 4:  Loss at 0.007415911182761192 Validation Accuracy at 0.6927999258041382\n",
      "Epoch 241, CIFAR-10 Batch 5:  Loss at 0.001696512452326715 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss at 0.0165933258831501 Validation Accuracy at 0.6915998458862305\n",
      "Epoch 242, CIFAR-10 Batch 2:  Loss at 0.0038031740114092827 Validation Accuracy at 0.6995999217033386\n",
      "Epoch 242, CIFAR-10 Batch 3:  Loss at 0.014018299989402294 Validation Accuracy at 0.6891999244689941\n",
      "Epoch 242, CIFAR-10 Batch 4:  Loss at 0.013759855180978775 Validation Accuracy at 0.6855998635292053\n",
      "Epoch 242, CIFAR-10 Batch 5:  Loss at 0.0028104218654334545 Validation Accuracy at 0.6781998872756958\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss at 0.016604704782366753 Validation Accuracy at 0.6829999089241028\n",
      "Epoch 243, CIFAR-10 Batch 2:  Loss at 0.006571286357939243 Validation Accuracy at 0.6875998973846436\n",
      "Epoch 243, CIFAR-10 Batch 3:  Loss at 0.003442370565608144 Validation Accuracy at 0.6957998871803284\n",
      "Epoch 243, CIFAR-10 Batch 4:  Loss at 0.005042728967964649 Validation Accuracy at 0.6875998973846436\n",
      "Epoch 243, CIFAR-10 Batch 5:  Loss at 0.0006317462539300323 Validation Accuracy at 0.6909998059272766\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss at 0.012202777899801731 Validation Accuracy at 0.6889998316764832\n",
      "Epoch 244, CIFAR-10 Batch 2:  Loss at 0.0032817432656884193 Validation Accuracy at 0.695399820804596\n",
      "Epoch 244, CIFAR-10 Batch 3:  Loss at 0.02560567483305931 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 244, CIFAR-10 Batch 4:  Loss at 0.0036011850461363792 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 244, CIFAR-10 Batch 5:  Loss at 0.0007155555067583919 Validation Accuracy at 0.6921998858451843\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss at 0.005724824499338865 Validation Accuracy at 0.6881998181343079\n",
      "Epoch 245, CIFAR-10 Batch 2:  Loss at 0.013418133370578289 Validation Accuracy at 0.697399914264679\n",
      "Epoch 245, CIFAR-10 Batch 3:  Loss at 0.003290634835138917 Validation Accuracy at 0.693199872970581\n",
      "Epoch 245, CIFAR-10 Batch 4:  Loss at 0.013804636895656586 Validation Accuracy at 0.6887998580932617\n",
      "Epoch 245, CIFAR-10 Batch 5:  Loss at 0.0008238098816946149 Validation Accuracy at 0.6951998472213745\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss at 0.02309206873178482 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 246, CIFAR-10 Batch 2:  Loss at 0.00787768978625536 Validation Accuracy at 0.6983998417854309\n",
      "Epoch 246, CIFAR-10 Batch 3:  Loss at 0.00424945866689086 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 246, CIFAR-10 Batch 4:  Loss at 0.00500679574906826 Validation Accuracy at 0.685999870300293\n",
      "Epoch 246, CIFAR-10 Batch 5:  Loss at 0.0003246923442929983 Validation Accuracy at 0.7001998424530029\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss at 0.014827686361968517 Validation Accuracy at 0.6915999054908752\n",
      "Epoch 247, CIFAR-10 Batch 2:  Loss at 0.029579104855656624 Validation Accuracy at 0.6979998350143433\n",
      "Epoch 247, CIFAR-10 Batch 3:  Loss at 0.0052222260273993015 Validation Accuracy at 0.6983998417854309\n",
      "Epoch 247, CIFAR-10 Batch 4:  Loss at 0.011440040543675423 Validation Accuracy at 0.678399920463562\n",
      "Epoch 247, CIFAR-10 Batch 5:  Loss at 0.00033473296207375824 Validation Accuracy at 0.6995998620986938\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss at 0.0031192449387162924 Validation Accuracy at 0.6933998465538025\n",
      "Epoch 248, CIFAR-10 Batch 2:  Loss at 0.019900640472769737 Validation Accuracy at 0.6997998952865601\n",
      "Epoch 248, CIFAR-10 Batch 3:  Loss at 0.0023327316157519817 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 248, CIFAR-10 Batch 4:  Loss at 0.0059553030878305435 Validation Accuracy at 0.6971998810768127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248, CIFAR-10 Batch 5:  Loss at 0.00046347230090759695 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss at 0.001535844407044351 Validation Accuracy at 0.6957999467849731\n",
      "Epoch 249, CIFAR-10 Batch 2:  Loss at 0.012035851366817951 Validation Accuracy at 0.6893998980522156\n",
      "Epoch 249, CIFAR-10 Batch 3:  Loss at 0.0070617180317640305 Validation Accuracy at 0.7031998634338379\n",
      "Epoch 249, CIFAR-10 Batch 4:  Loss at 0.002305151429027319 Validation Accuracy at 0.6977998614311218\n",
      "Epoch 249, CIFAR-10 Batch 5:  Loss at 0.00035420095082372427 Validation Accuracy at 0.6987998485565186\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss at 0.0045922864228487015 Validation Accuracy at 0.7005998492240906\n",
      "Epoch 250, CIFAR-10 Batch 2:  Loss at 0.024993330240249634 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 250, CIFAR-10 Batch 3:  Loss at 0.006587020121514797 Validation Accuracy at 0.6921999454498291\n",
      "Epoch 250, CIFAR-10 Batch 4:  Loss at 0.012597235850989819 Validation Accuracy at 0.6881998777389526\n",
      "Epoch 250, CIFAR-10 Batch 5:  Loss at 0.0041527701541781425 Validation Accuracy at 0.6935998797416687\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss at 0.050986405462026596 Validation Accuracy at 0.6927998661994934\n",
      "Epoch 251, CIFAR-10 Batch 2:  Loss at 0.034116413444280624 Validation Accuracy at 0.6945999264717102\n",
      "Epoch 251, CIFAR-10 Batch 3:  Loss at 0.002386450069025159 Validation Accuracy at 0.6949999332427979\n",
      "Epoch 251, CIFAR-10 Batch 4:  Loss at 0.004074440337717533 Validation Accuracy at 0.6897999048233032\n",
      "Epoch 251, CIFAR-10 Batch 5:  Loss at 0.0015076732961460948 Validation Accuracy at 0.6933999061584473\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss at 0.008141456171870232 Validation Accuracy at 0.6903998851776123\n",
      "Epoch 252, CIFAR-10 Batch 2:  Loss at 0.004986979998648167 Validation Accuracy at 0.6939998269081116\n",
      "Epoch 252, CIFAR-10 Batch 3:  Loss at 0.013704041950404644 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 252, CIFAR-10 Batch 4:  Loss at 0.0030957218259572983 Validation Accuracy at 0.6929998397827148\n",
      "Epoch 252, CIFAR-10 Batch 5:  Loss at 0.0008270041435025632 Validation Accuracy at 0.6951999068260193\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss at 0.01813668943941593 Validation Accuracy at 0.7009998559951782\n",
      "Epoch 253, CIFAR-10 Batch 2:  Loss at 0.011777348816394806 Validation Accuracy at 0.6941998600959778\n",
      "Epoch 253, CIFAR-10 Batch 3:  Loss at 0.0047227684408426285 Validation Accuracy at 0.6919997930526733\n",
      "Epoch 253, CIFAR-10 Batch 4:  Loss at 0.012962695211172104 Validation Accuracy at 0.6831998825073242\n",
      "Epoch 253, CIFAR-10 Batch 5:  Loss at 0.005852167494595051 Validation Accuracy at 0.6955999135971069\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss at 0.0043678912334144115 Validation Accuracy at 0.6943998336791992\n",
      "Epoch 254, CIFAR-10 Batch 2:  Loss at 0.02205142192542553 Validation Accuracy at 0.7013998627662659\n",
      "Epoch 254, CIFAR-10 Batch 3:  Loss at 0.004063853062689304 Validation Accuracy at 0.6937998533248901\n",
      "Epoch 254, CIFAR-10 Batch 4:  Loss at 0.01504010334610939 Validation Accuracy at 0.6911998987197876\n",
      "Epoch 254, CIFAR-10 Batch 5:  Loss at 0.01014186255633831 Validation Accuracy at 0.6841999292373657\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss at 0.0031228556763380766 Validation Accuracy at 0.6927999258041382\n",
      "Epoch 255, CIFAR-10 Batch 2:  Loss at 0.05005820840597153 Validation Accuracy at 0.6891998052597046\n",
      "Epoch 255, CIFAR-10 Batch 3:  Loss at 0.001369400997646153 Validation Accuracy at 0.689599871635437\n",
      "Epoch 255, CIFAR-10 Batch 4:  Loss at 0.0024751124437898397 Validation Accuracy at 0.6909998655319214\n",
      "Epoch 255, CIFAR-10 Batch 5:  Loss at 0.010892614722251892 Validation Accuracy at 0.6741998195648193\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss at 0.007047487888485193 Validation Accuracy at 0.6857998371124268\n",
      "Epoch 256, CIFAR-10 Batch 2:  Loss at 0.026708299294114113 Validation Accuracy at 0.6967998743057251\n",
      "Epoch 256, CIFAR-10 Batch 3:  Loss at 0.0011628407519310713 Validation Accuracy at 0.6971999406814575\n",
      "Epoch 256, CIFAR-10 Batch 4:  Loss at 0.013171994127333164 Validation Accuracy at 0.6883998513221741\n",
      "Epoch 256, CIFAR-10 Batch 5:  Loss at 0.0037989038974046707 Validation Accuracy at 0.6955997943878174\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7060111464968153\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuhg0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04828d1be0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
